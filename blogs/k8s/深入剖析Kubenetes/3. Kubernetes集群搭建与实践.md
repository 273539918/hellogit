# Kubernetes集群搭建与实践

## 10  Kubernetes一键部署利器： kubeadm

### 10.1 kubeadm的工作原理

kubeadm选择的集群部署方案：  把kubelet直接运行在宿主机上，然后使用容器部署其他的Kubernetes组件。

那么，为什么kubelet不与其他组件一样使用容器来部署呢？
Kubelet是Kubernetes项目用来操作Docker等容器运行时的核心组件。可是，除了跟容器运行时打交道外，kubelet在配置容器网络、管理容器数据卷时，都需要直接操作宿主机。如果kublet运行在一个容器里，那么直接操作宿主机就会变得非常麻烦。对于网络配置来说还好，kubelet容器可以通过不开启Network Namespace（即 Docker的host network模式）的方式，直接共享宿主机的网络栈。可是，要让kubelet隔着容器的Mount Namespace和文件系统，操作宿主机的文件系统，就非常困难了。比如，如果用户想要使用NFS做容器的持久化数据卷，那么kubelet就需要在容器进行绑定挂载前，在宿主机的指定目录上，先挂载NFS的远程目录。可是，由于现在kubelet是运行在容器里的，这就意味着它要做的这个"mount -F nfs"命令，被隔离在了一个单独的Mount Namespace中。即， kubelet做的挂载操作，不能被“传播”到宿主机上。

所以，使用kubeadm的第一步，是在机器上手动安装kubeadm，kubelet和kubectl 

### 10.1 使用kubeadm

安装kubeadm

```
$yum install kubeadm
```

#### 10.1.1 预检查

安装完成之后，就可以使用 kubeadm init 部署Master节点了。当执行kubeadm init指令后，kubeadm首先要做的，是一列的检查工作，以确定这台机器可以用来部署kubernetes。检查（Preflight Checks）包括：

- Linux内核的版本是否是3.10以上？
- Linux Cgroups模块是否可用？
- 机器的hostname是否标准？在Kubernetes项目中，机器的名字存储在Etcd中的API对象，都必须使用标准的DNS命名
- 用户安装的kubeadm和kubelet的版本是否匹配
- .......

#### 10.1.2 证书生成

在通过了Preflight Checks之后，kubeadm要为你做的，是生成Kubernetes对外提供服务所需的各种证书和对应的目录。

Kubernetes对外提供服务时，除非专门开启“不安全模式”，否则都要通过HTTPS才能访问kube-apiserver。这就需要为Kubernetes集群配置好证书文件。kubeadm为Kubernetes项目生成的证书文件都放在Master节点的/etc/kubernetes/pki目录下。在这个目录下，最主要的证书文件是ca.crt和对应的私钥ca.key。

此外，用户使用kubectl获取容器日志等streaming操作时，需要通过kube-apiserver向kubelet发起请求，这个连接也必须是安全的。kubeadm为这一步生成的是apiserver-kubelet-client.crt文件，对应的私钥是apiserver-kubelet-client.key。除此之外，Kubernetes集群中还有Aggregate APIServer等，也需要用到专门的证书，这里不再列举。需要指出的是，你可以选择不让kubeadm为你生成这些证书，而是拷贝现有的证书到如下证书的目录：

```
/etc/kubernetes/pki/ca.{crt,key}
```

这时，kubeadm就会跳过证书生成的步骤，把它完全交给用户处理。

#### 10.1.3 配置文件生成

证书生成后，kubeadm接下来会为其他组件生成访问kube-apiserver所需的配置文件。这些配置文件的路径是：/etc/kubernetes/xxx.conf: 

```
$ls /etc/kubernetes/ | grep conf
admin.conf
controller-manager.conf
kubelet.conf
scheduler.conf
```

这些文件里面记录的是，当前这个Master节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler，kubelet等），可以直接加在相应的文件，使用里面的信息与kube-apiserver建立安全连接。

接下来，kubeadm会为Master组件生成pod配置文件来启动Master上的组件。Kubernetes有三个Master组件，分别是kube-apiserver、kube-controller-manager、 kube-scheduler，它们都会被使用Pod的方式部署起来。你可能会有疑问，Kubernetes集群尚不存在，这些Pod是如何部署起来的？
这是因为在Kubernetes中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的Pod的YAML文件放在一个指定的目录里。这样，当这台机器上的kubelet启动时，它会自动检查这个目录，加在所有的Pod YAML文件，然后在这台机器上启动它们。从这一点也可以看出，kubelet在Kubernetes项目中的地位非常高，在设计上它就是一个安全独立的组件，而其他Master组件，则更像是辅助性的系统容器。在kubeadm中，Master组件的YAML文件会被生成在/etc/kubernetes/mainifests路径下。这一步完成后，kubeadm还会再生成一个Etcd的Pod YAML文件，用来通过同样的Static Pod的方式启动Etcd。所以，最后Master组件的Pod YAML文件如下所示：

```
$ ls /etc/kubernetes/manifests/
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
```

#### 10.1.4 Master容器拉起

Kubelet启动后，根据/etc/kubernetes/manifests/中的YAML文件拉起Master容器，kubeadm会通过检查localhost:6443/healthz这个Master组件的健康检查URL，等待Master组件完全运行起来

Master容器启动后，kubeadm就会为集群生成一个bootstrap token。在后面，只要持有这个token，任何一个安装了kubelet和kubead的节点，都可以通过kubeadm join加入到这个集群当中。这个token的值和使用方法，会在kubeadm init结束后被打印出来。如果token后续过期，可以使用kubeadm token create来创建一个新的

kubeadmin init执行完成后，返回的内容如下 :

```
$kubeadm init
...
Your Kubernetes control-plane has initialized successfully!
...
kubeadm join 11.88.191.33:6443 --token 3i8s05.dvp00wr5pv0i0tgz \
    --discovery-token-ca-cert-hash sha256:b56c2b425da42668e580b504335261144edece305c8f3d72975ce342c4771d58
...
```





在token生成之后，kubeadm会将ca.crt等Master节点的重要信息，通过ConfigMap的方式保存在Etcd中，供后续部署Node节点使用。这个ConfigMap的名字是cluster-info

 #### 10.1.5 安装插件

kubeadm init 的最后一步，就是安装默认插件。Kubernetes默认kube-proxy和DNS这两个插件是必须安装的。它们分别用来提供整个集群的服务发现和DNS功能。其实，这两个插件也只是两个容器镜像而已，所以kubeadm只要用Kubernetes客户端创建两个Pod就可以了

#### 10.1.6 Node节点拉起

Node节点拉起，是通过kubeadm join来完成。这个流程其实非常简单，kubeadm init生成bootstrap token之后，你就可以在任意一台安装了kubelet和kubeadm的机器上执行kubeadm join了。

```
kubeadm join 11.88.191.33:6443 --token 3i8s05.dvp00wr5pv0i0tgz \
    --discovery-token-ca-cert-hash sha256:b56c2b425da42668e580b504335261144edece305c8f3d72975ce342c4771d58
```

为什么需要这个token呢？因为任何一台机器想要成为Kubernetes集群中的一个节点，就必须在集群的kube-apiserver上注册。可是，要想跟api-server打交道，这台机器就必须要获取到相应的证书文件（CA文件）。可是，为了能够一键安装，我们就不能让用户去Master节点上手动拷贝这些文件。所以kubeadm至少需要发起一次“不安全模式”的访问到kube-apiserver，从而拿到保存在ConfigMap中的cluster-info（它保存了APIServer的授权信息）。而bootstrap token，扮演的就是这个过程中的安全验证的角色。

只要有了cluster-info里的kube-apiserver的地址、端口、证书、kubelet就可以以“安全模式”连接到apiserver上，这样一个新的节点就部署完成了。

可以在master节点查看刚刚加入的节点

```
$kubectl get nodes
NAME                   STATUS     ROLES    AGE    VERSION
bd011088191033.na610   NotReady   master   2m9s   v1.18.0
bd011088191061.na610   NotReady   <none>   40s    v1.18.0
```



##### 10.1.7 配置kubeadm的部署参数

前面介绍的kubeadm init和kubeadm join非常简单易用，可是我们如何定制我的集群组件参数呢？ 比如，我要指定kube-apiserver的启动参数，该如何操作？

推荐在使用kubeadm init部署Master节点时，使用下面这条指令：

```
$ kubeadm init --config kubeadm.yaml
```

这时，就可以给kubeadm提供一个YAML文件，它的内容如下所示

```
apiVersion: kubeadm.k8s.io/v1alpha2
kind: MasterConfiguration
kubernetesVersion: v1.11.0
api:
  advertiseAddress: 192.168.0.102
  bindPort: 6443
  ...
etcd:
  local:
    dataDir: /var/lib/etcd
    image: ""
imageRepository: k8s.gcr.io
kubeProxy:
  config:
    bindAddress: 0.0.0.0
    ...
kubeletConfiguration:
  baseConfig:
    address: 0.0.0.0
    ...
networking:
  dnsDomain: cluster.local
  podSubnet: ""
  serviceSubnet: 10.96.0.0/12
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  ...
```

这样，我要指定kube-apiserver的参数，就只要在文件里加上这样一段：

```
...
apiServerExtraArgs:
  advertise-address: 192.168.0.103
  anonymous-auth: false
  enable-admission-plugins: AlwaysPullImages,DefaultStorageClass
  audit-log-path: /home/johndoe/audit.log
```

然后，kubeadm就会使用上面这些信息替换/etc/kubernetes/mainifests/kube-apiserver.yaml里的command字段里的参数。你还可以修改kubelet和kube-proxy的配置，修改kubernetes使用的基础镜像的URL，指定自己的证书文件，指定特殊的容器运行时等等。

第一次搭建，可以通过如下命令，先初始化生成一个kubeadm.yaml

```
$kubeadm config print init-defaults > kubeadm.yaml
```

#### 10.1.8 kubeadm的欠缺

kubeadm能方便地搭建一个kubernetes的集群，但是不能用于生产环境，原因是目前kubeadm搭建起来的master节点是单点。kubeadm目前最欠缺的是，一键部署一个高可用的kubernetes集群，即：Etcd、Master组件都应该是多节点集群。这也正是kubeadm接下来发展的主要方向



## 10  从0到1 ： 搭建一个完整的Kubernetes集群

不太关注手工部署Kubernetes集群的同学，可以直接使用MiniKube在本地启动简单的Kubernetes集群进行学习，跳过这部分的学习。

### 10.1.9 部署Kubernetes的master节点

 我们编写一个给kubeadm用的YAML文件:

```
apiVersion: kubeadm.k8s.io/v1beta2
kind: MasterConfiguration
controllerManagerExtraArgs:
  horizontal-pod-autoscaler-use-rest-clients: "true"
  horizontal-pod-autoscaler-sync-period: "10s"
  node-monitor-grace-period: "10s"
apiServerExtraArgs:
  runtime-config: "api/all=true"
kubernetesVersion: "v1.18.0"
---
apiVersion: kubeadm.k8s.io/v1beta2
kind: InitConfiguration
```

然后，我们只需要执行一句指令:

```
kubeadm init --config kubeadm.yaml
```

就可以完成Kubernetes Master的部署了，部署完成后，kubeadm会生成一行指令：

```
kubeadm join 11.88.191.33:6443 --token 2wp3r6.9ypulb0q1nuwwg1q \
    --discovery-token-ca-cert-hash sha256:79d86aac867622f65208cd65eb4b49969a7ae4b48a456e0b76e4c09fd1cbe72e
```

这个kubeadm join命令，就是用来给这个Master节点添加更多工作节点的命令。此外，kubeadm还会提示我们第一次使用kubernetes集群所需要的配置命令：

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

需要这些配置命令的原因是：Kubernetes集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的Kubernetse集群的安全配置文件，保存到当前用户的.kube目录下，kubectl默认会使用这个目录下的授权信息访问Kubernetes集群。如果不这么做的话，我们每次都需要通过export KUBECONFIG环境变量的方式告诉kubectl这个安全配置文件的位置，如：

```
export KUBECONFIG=/etc/kubernetes/admin.conf
```

现在，我们就可以使用kubectl get命令来查看当前唯一一个节点的状态了：

```
$ kubectl get nodes
NAME                   STATUS     ROLES    AGE     VERSION
bd011088191033.na610   NotReady   master   6h42m   v1.18.0
```

可以看到，这个get指令输出的结果里，Master节点的状态是NotReady，这是为什么呢？我们可以用如下命令来查看详细信息:

```
$ kubectl describe node master

...

reason:d message:docker: network plugin is not ready: cni config uninitialized
...
```

可以看到NotReady的原因在于尚未部署任何网络插件。

另外，可以通过kubectl检查这个节点上各个系统Pod的状态，其中，kube-system是Kubernetes项目预留的系统Pod的工作空间（Namespace，注意它并不是Linux Namespace，它只是Kubernetes划分不同工作空间的单位）:

```
$ kubectl get pods -n kube-system
NAME                                           READY   STATUS    RESTARTS   AGE
coredns-66bff467f8-75td9                       0/1     Pending   0          6h42m
coredns-66bff467f8-lj7ch                       0/1     Pending   0          6h42m
etcd-bd011088191033.na610                      1/1     Running   0          6h42m
kube-apiserver-bd011088191033.na610            1/1     Running   0          6h42m
kube-controller-manager-bd011088191033.na610   1/1     Running   0          6h42m
kube-proxy-qhf7t                               1/1     Running   0          6h42m
kube-scheduler-bd011088191033.na610            1/1     Running   0          6h42m
```

可以看到，CoreDNS 等依赖于网络的Pod都处于Pending状态，即调度失败。这当然是符合预期的：因为这个Master节点的网络尚未就绪。部署网络插件非常简单，只需要执行一句kubelet apply指令，以Weave为例：

```
$ kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```

部署完成之后，我们可以通过kubectl get 重新检查pod的状态：

```
$kubectl get pods -n kube-system
NAME                                           READY   STATUS    RESTARTS   AGE
coredns-66bff467f8-75td9                       1/1     Running   0          7h58m
coredns-66bff467f8-lj7ch                       1/1     Running   0          7h58m
etcd-bd011088191033.na610                      1/1     Running   0          7h58m
kube-apiserver-bd011088191033.na610            1/1     Running   0          7h58m
kube-controller-manager-bd011088191033.na610   1/1     Running   0          7h58m
kube-proxy-qhf7t                               1/1     Running   0          7h58m
kube-scheduler-bd011088191033.na610            1/1     Running   0          7h58m
weave-net-cslz7                                2/2     Running   0          4m1s

```

可以看到，所有的系统Pod都成功启动了

### 10.1.10 部署Kubernetes的worker节点

Kubernetes的Worker节点跟Master节点几乎是相同的，它们运行着的都是一个kubelet组件。唯一的区别在于，在kubeadm init的过程中，kubelet启动后，Master节点上还会自动运行kube-apiserver、kube-scheduler、kube-controller-manager这三个系统Pod。部署Worker节点是最简单，只需要两步即可完成：

第一步：在所有Worker上安装docker、kubeadm、kubelet
第二步：执行部署Master节点时生成的kubeadm join指令：

```
$ kubeadm join 11.88.191.33:6443 --token 2wp3r6.9ypulb0q1nuwwg1q \
    --discovery-token-ca-cert-hash sha256:79d86aac867622f65208cd65eb4b49969a7ae4b48a456e0b76e4c09fd1cbe72e
```

若是worker节点加入之后，发现weave-net因拉取镜像失败，导致节点处于NotReady状态，可手动拉取镜像解决

```
$kubectl get nodes
NAME                   STATUS     ROLES    AGE    VERSION
bd011088191033.na610   Ready      master   10h    v1.18.0
bd011088191046.na610   NotReady   <none>   147m   v1.18.0
bd011088191061.na610   Ready      <none>   147m   v1.18.0

$#kubectl get pods --all-namespaces -o wide | grep bd011088191046.na610
kube-system   kube-proxy-4h4ft                               1/1     Running        0          153m   11.88.191.46   bd011088191046.na610   <none>           <none>
kube-system   weave-net-4z6ws                                0/2     ErrImagePull   0          153m   11.88.191.46   bd011088191046.na610   <none>           <none>

#kubectl describe pod weave-net-4z6ws --namespace=kube-system
...
 Failed to pull image "docker.io/weaveworks/weave-kube:2.6.2": rpc error: code = Unknown desc = error pulling image configuration: read tcp 11.88.191.46:52468->104.18.121.25:443: read: connection timed out
..
```

```
#在机器上手动拉取镜像
$docker pull weaveworks/weave-npc
```

若是docker pull还是超时，可以使用阿里云的镜像加速器

https://cr.console.aliyun.com/undefined/instances/mirrors



### 10.1.11 部署Dashboard可视化插件

在Kubernetes社区中，有一个很受欢迎的Dashboard项目，它可以给用户提供一个可视化的Web界面来查看当前集群的各种信息。它的部署非常简单:

```
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml
```

部署完成之后，我们就可以查看Dashboard对应Pod的状态了：

```
$ kubectl get pods -n kube-system
kubernetes-dashboard-6948bdb78-f67xk   1/1       Running   0          1m
```

1.7版本之后的Dashboard项目部署完成之后，默认只能通过Proxy的方式在本地访问。如果想要在集群外访问这个Dashboard的话，就需要用到ingress，后面会介绍到。

### 10.1.12 部署容器存储插件

介绍容器原理的时候提到过，很多时候我们需要使用数据卷（Volume）把外面宿主机上的目录或者文件挂载进容器的Mount Namespace中，从而达到容器和宿主机共享这些目录或者文件的目的。容器里的应用，也就可以在这些数据卷中新建和写入文件。

可是，如果你在某一台机器上启动一个容器，显然无法看到其他机器上的容器在它们数据卷里写入的文件。这是容器最典型的特征之一：无状态。

而容器的持久化存储，就是用来保存容器存储状态的重要手段：存储插件会在容器里挂载一个基于网络或者其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器上，或者以分布式的方式保存在多个基点上，而与当前宿主机没有任何绑定关系。这样，无论你在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容。这就是“持久化”的含义。我们选择的Kubernetes存储插件项目是： Rook。 使用如下命令部署：

```
$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/common.yaml

$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml

$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml
```

在部署完成后，你就可以看到Rook项目会将自己的Pod放置在由它自己管理的两个Namespace当中：

```
$ kubectl get pods -n rook-ceph-system
NAME                                  READY     STATUS    RESTARTS   AGE
rook-ceph-agent-7cv62                 1/1       Running   0          15s
rook-ceph-operator-78d498c68c-7fj72   1/1       Running   0          44s
rook-discover-2ctcv                   1/1       Running   0          15s

$ kubectl get pods -n rook-ceph
NAME                   READY     STATUS    RESTARTS   AGE
rook-ceph-mon0-kxnzh   1/1       Running   0          13s
rook-ceph-mon1-7dn2t   1/1       Running   0          2s
```

这样，一个基于Rook持久化存储集群就以容器的方式运行起来了，而接下来在Kubernetes项目上创建的所有Pod就能够通过Persistent Volume（PV）和Persistent Volume Claim（PVC）的方式，在容器里挂载数据卷了。而Rook项目，则会负责这些数据卷的生命周期管理、灾难备份等运维工作。

### 10.1.13 “云原生”

像Rook这样的项目，巧妙地依赖了Kubernetes提供的编排能力，合理的使用了很多诸如Operator、CRD等重要的扩展特性。这使得Rooke项目，成为了目前社区中基于Kubernetes API构建的最完善也是最成熟的容器存储插件。

其实在很多时候，大家说的所谓“云原生”，就是“Kubernetes原生”的意思。而像Rooke、Istio这样的项目，正是贯彻这个思路的典范。这个思想，也是开发和使用Kubernets的重要指导思想，即：基于Kubernetes开展工作时候，一定要优先考虑这两个问题：

1: 我的工作是不是可以容器化？
2: 我的工作是不是可以借助Kubernetes API和扩展机制来完成

而一旦这项工作能够基于Kubernetes实现容器化，就很有可能像上面的部署过程一样，大幅简化原本复杂的运维工作。对于时间宝贵的技术人员来说，这个变化的重要性是不言而喻的。






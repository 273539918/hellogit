# 容器技术概念入门篇

## 05 白话容器基础（一）：从进程说开去



### 05.1 容器只是一种特殊的进程

容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。对于Docker等大多数Linux容器来说，Cgroups技术是用来制造约束的主要手段，而Namesapce技术则是用来修改进程视图的主要方法。

创建一个容器

```shell
docker run -it busybox /bin/sh 
```

it参数告诉了Docker项目在启动容器后，需要给我们分配一个文本输入输出环境，也就是TTY，跟容器的标准输入相关联，这样我们就可以和这个Docker容器进行交互了。而/bin/sh 就是我们要在Docker容器里运行的程序。在该容器里，/bin/sh的PID是1，而实际在宿主机上它的PID是1000。这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号。这种技术，就是Linux里面的Namespace机制。

所以。Docker容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个容器所需要启动的一组Namespace参数。这样，容器就只能“看”到当前Namespace所限定的资源、文件、设备、状态或者配置。而对于宿主机以外其他不相关的程序，就完全看不到了

### 05.2 Namespace实现的基本原理

Namespace的使用方式也非常有意思：它其实只是Linux创建新进程的一个可选参数。Linux创建进程的方式如下：

```
int pid = clone(main_function,stack_size,SIGCHLD,NULL)
```

这个系统调用就会为我们创建一个新的进程，并且返回它的进程号Pid。而当我们用clone()系统调用时，指定CLONE_NEWPID参数时：

```
int pid = clone(main_function,stack_size,CLONE_NEWPID | SIGCHLD,NULL)
```

这时，新创建的进程将会“看到”一个全新的进程空间，在这个进程空间里，它的PID是1。之所以说“看到”，是因为这只是一个“障眼法”，它在宿主机真实的进程空间里，这个进程的PID还是真实的数值，比如100。

除了我们刚刚用到的PID Namespace，Linux操作系统还提供了Mount、UTS、IPC、Network和User这些Namespace，用来对各种不同的进程上下文进行“障眼法”操作。比如，Mount Namespace，用于让被隔离进程只看到当前Namespace里的挂载点信息；Network Namespace用于让被隔离进程看到当前Namespace里的网络设备和配置。 这，就是Linux容器最基本的实现原理了。

### 05.3 容器与虚拟机的区别

虚拟机的工作原理是通过硬件虚拟化功能，模拟出运行一个操作系统需要的各种硬件，比如CPU、内存、i/o设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，这样，用户的应用进程就可以运行在这个虚拟的机器中，它能看到的自然只有这个虚拟操作系统的文件和目录，以及这个机器里的虚拟设备。这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用。

容器与真实存在的虚拟机不同，在使用Docker的时候，并没有一个真正的“Docker容器”运行在宿主机里。Docker项目帮助用户启动的，还是原来的那些进程，只不过在创建这些进程时加上了各种各样的Namespace参数。



##05 白话容器基础（二）：隔离与限制

### 05.4 Namespace作为隔离手段与虚拟机相比的优劣势

机器性能损耗：与容器相比，虚拟机自身需要占用一部分资源，比如一个CentOS的KVM虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用100~200 MB内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘I/O的损耗非常大。而相比之下，容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的；而另一方面，使用Namespace作为隔离手段的容器并不需要单独的Guest OS，这就使得容器额外的资源占用几乎可以忽略不计

隔离程度：与虚拟机相比，容器的隔离机制不够彻底。因此容器只是运行在宿主机上的一种特殊进程，多个容器之间使用的还是同一个宿主机的操作系统。共享宿主机内核意味着，如果想要在windows宿主机上运行Linux容器，或者在低版本的Linux宿主机上运行高版本的Linux容器，都是行不通的。其次，在Linux内核中，有很多资源和对象是不能被Namespace化的，最典型的例子就是：系统时间

“敏捷”和“高性能”是容器相较于虚拟机最大的优势

### 05.5 对容器进行“限制”

Linux Cgroups的全程就是Linux Control Group。它主要的作用，就是限制一个进程组能够使用的资源上限、包括CPU、内存、磁盘、网络带宽等等。此外，Cgroup还能对进程进行优先级设置、审计、以及将进程挂起和恢复等操作。

```
$mount -t cgroup
cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd)
cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)
cgroup on /sys/fs/cgroup/net_cgroup type cgroup (rw,nosuid,nodev,noexec,relatime,net_cgroup)
cgroup on /sys/fs/cgroup/cpuset,cpu,cpuacct type cgroup
....
```

可以看到，/sys/fs/cgroup下面挂了许多诸如cpuset、cpu、memory这样的子目录，也叫做子系统。这些都是机器当前可以被Cgroups进行限制的资源种类。而在子系统对应的资源种类下，可以看到该类资源具体可以被限制的方法。以CPU子系统来说，可以看到:

```
$ ls /sys/fs/cgroup/cpu
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks
```

比如cfs_period和cfs_quota这样的关键词，可以用来限制进程长度为cfs_period的一段时间内，只能被分配到总量为cfs_quota的CPU时间。

### 05.6 模拟Docker项目创建容器的限制过程

我们在cpu子系统下面创建一个目录

```
root@ubuntu:/sys/fs/cgroup/cpu$ mkdir canghong_test_container
root@ubuntu:/sys/fs/cgroup/cpu$ ls canghong_test_container/
cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release
cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks

```

这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的container目录下，自动生成该子系统对应的资源限制文件。现在，我们在后台执行这样一条脚本:

```
$ while : ; do : ; done &
[1] 54403
```

显然，它执行了一个死循环，可以把计算机的CPU吃到100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号是PID是226。这样，我们就可以用top指令来确认一下CPU有没有被打满：

```
$ top
%Cpu(s):  1.1 us,  0.0 sy,  0.0 ni, 98.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
   PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 54403 admin     20   0  116364   1332      0 R 100.0  0.0   4:30.97 bash
```

注意：%CPU(S)和%CPU的区别：Cpu(s)表示的是 所有用户进程占用整个cpu的平均值，由于每个核心占用的百分比不同，所以按平均值来算比较有参考意义。而%CPU显示的是进程占用一个核的百分比，而不是整个cpu（12核）的百分比，有时候可能大于100，那是因为该进程启用了多线程占用了多个核心，所以有时候我们看该值得时候会超过100%，但不会超过总核数*100

而此时，我们可以通过查看container目录下文件，看到container控制组里的CPU quota还没有任何限制（即-1），CPU Perio 则是默认的100 ms（100000 us） ，即在100ms周期内，能无限制使用

```
$cat /sys/fs/cgroup/cpu/canghong_test_container/cpu.cfs_quota_us
-1
$cat /sys/fs/cgroup/cpu/canghong_test_container/cpu.cfs_period_us
100000
```

接下来，我们可以通过修改这些文件的内容来设置限制

```
$ echo 20000 > /sys/fs/cgroup/cpu/canghong_test_container/cpu.cfs_quota_us
```

它意味着在每个100ms的时间里，被该控制组限制的进程只能使用20ms的CPU时间，他就是说这个进程只能使用到20%的CPU带宽。接下来，我们把被限制的进程的PID写入container组里的tasks文件，上面的设置就会对该进程生效了：

```
$ echo 54403 > /sys/fs/cgroup/cpu/canghong_test_container/tasks 
```

我们可以用top指令查看一下:

```
$ top
%Cpu(s):  0.2 us,  0.0 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
 PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
214799 root      20   0  116352   1352      0 R  20.2  0.0   0:33.55 bash
```

可以看到，计算机的CPU使用率立即降到了20%
除CPU子系统外，Cgroups的每一个子系统都有其独有的资源限制能力。

### 05.7 回看容器的“限制”

Linux Cgroups的设计还是比较易用的，简单理解，它就是一个子系统目录加上一组资源限制文件的组合。而对于Docker等Linux容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的PID写到对应控制组的tasks文件中就可以了。而至于在这些控制组下面的资源文件里应该填上什么值，就靠用户执行docker run时的参数指定了，比如这样一条命令：

```
$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
```

在启动这个容器后，我们可以通过查看Cgroups文件系统下，CPU子系统中，“docker”这个控制组里的资源限制文件的内容来确认：

```
$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us 
100000
$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us 
20000
```

这意味着这个Docker容器，只能使用到20%的CPU带宽（在100ms周期内，能使用20ms）

### 05.8 容器需要解决的问题

问题1: 

一个正在运行的Docker容器，其实就是一个启用了多个Linux Namesapce的应用进程，而这个进程能够使用的资源量，则受Cgoups配置的限制。而容器的本质就是一个进程，用户的应用进程实际上就是容器里PID=1的进程，也是其他后续创建的所有进程的父进程。这意味着，在一个容器中，你没办法同时运行两个不同的应用，除非你能事先找到一个公共的PID=1的程序来充当两个不同应用的父进程，这也是为什么很多人会用systemd或者supervisord这样的软件来代替应用本身作为容器的启动进程。但是，容器的本身的设计是希望容器与应用能够同生命周期的，这个概念对后续的容器编排非常重要。否则，一旦出现类似于“容器是正常运行的，但是里面的应用早已经挂了”的情况，编排系统处理起来就非常麻烦了

问题2:

Cgroups对资源的限制能力也有很多不完善的地方，比如/proc文件系统的问题。比如当使用top命令查看进程的信息，比如CPU使用情况、内存占用率等，由于top也访问/proc文件来获得这些信息，因此在容器中执行top显示的是宿主机的CPU和内存数据，而不是当前容器的数据。造成这个问题的原因就是，/proc文件系统并不知道用户通过Cgroups给这个容器做了什么样的资源限制，即： /proc文件系统不了解Cgruops限制的存在

以上两个问题的解决，都会在后续的介绍中解决



## 05 白话容器基础（三）：深入理解容器镜像

容器里的进程看到的文件系统又是什么样子的呢？可能你立即就会想到，这一定是一个关于 Mount Namespace的问题： 容器里的应用进程，理应看到一份完全独立的文件系统。这样它就可以在自己的容器目录（比如/tmp）下进行操作，而完全不受宿主机以及其他容器的影响。
我们通过下面的一段小程序来验证真实情况：

```
#define _GNU_SOURCE
#include <sys/types.h>
#include <sys/wait.h>
#include <sys/mount.h>
#include <stdio.h>
#include <sched.h>
#include <signal.h>
#include <unistd.h>


/* 定义一个给 clone 用的栈，栈大小1M */
#define STACK_SIZE (1024 * 1024)
static char container_stack[STACK_SIZE];

char* const container_args[] = {
    "/bin/bash",
    NULL
};

int container_main(void* arg)
{
    printf("Container - inside the container!\n");
    sethostname("container",10); /* 设置hostname */
    /* 直接执行一个shell，以便我们观察这个进程空间里的资源是否被隔离了 */
    execv(container_args[0], container_args);
    printf("Something's wrong!\n");
    return 1;
}

int main()
{
    printf("Parent - start a container!\n");
    /* 调用clone函数，其中传出一个函数，还有一个栈空间的（为什么传尾指针，因为栈是反着的） */
    /* int container_pid = clone(container_main, container_stack+STACK_SIZE, SIGCHLD, NULL); */
    int container_pid = clone(container_main, container_stack+STACK_SIZE,
                         CLONE_NEWUTS | CLONE_NEWPID | CLONE_NEWNS | SIGCHLD, NULL); /*启用CLONE_NEWxx Namespace隔离 */
    /* 等待子进程结束 */
    waitpid(container_pid, NULL, 0);
    printf("Parent - container stopped!\n");
    return 0;
}

```

这段代码的功能非常简单：在main函数里，我们通过clone()系统调用创建了一个新的子进程container_main，并且声明要为它启用Mount Namespace（即：CLONE_NEWNS标志）。
编译这个程序：

```
$ [admin@xxxxxx gcc -o container_demo container_demo.c
$ [admin@xxxxxx /home/admin/canghong]
$sudo ./container_demo
Parent - start a container!
Container - inside the container!

[root@container /home/admin/canghong]
#
```

这样，我们就进入了这个“容器”当中。可是，如果在“容器”里执行一下ls指令的话，我们就会发现一个有趣的现象：/tmp 目录下的内容跟宿主机的内容是一样的。也就是说：即使开启了Mount Namesace，容器进程看到的文件系统也跟宿主机完全一样。原因是：Mount Namespace修改的，是容器进程对文件系统“挂载点”的认知，这也意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变，而在此之前，新创建的容器会直接继承宿主机的各个挂载点。

于是，我们在容器进程执行前可以添加一步重新挂在/tmp目录的操作:

```
int container_main(void* arg)
{
  printf("Container - inside the container!\n");
  // 如果你的机器的根目录的挂载类型是shared，那必须先重新挂载根目录
  // 如果MS_PRIVATE找不到，注意引入库  #include <sys/mount.h>
  // mount("", "/", NULL, MS_PRIVATE, "");
  mount("none", "/tmp", "tmpfs", 0, "");
  execv(container_args[0], container_args);
  printf("Something's wrong!\n");
  return 1;
}

```

mount("none","/tmp","tmpfs",0,"")语句告诉容器以tmpfs（内存盘）格式，重新挂载/tmp目录，修改后重新编译执行可以看到容器内的/tmp目录变成了一个空目录，这意味着重新挂载生效了，我们可以用mount -l 检查一下

```
$ mount -l | grep tmpfs
none on /tmp type tmpfs (rw,relatime)
```

注意，这个挂载点只有容器可见，在宿主机上是不存在的。在宿主机上查看：

```
# 在宿主机上
$ mount -l | grep tmpfs
```

这就是Mount Namespace跟其他Namespace的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。

### 05.9 容器的根文件系统（rootfs）

不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录"/"。而由于Mount Namespace的存在，这个挂载对于宿主机不可见，所以容器进程就可以在里面随便折腾了。在Linux操作系统里，有一个名为chroot的命令，可以改变进程的根目录到你指定的位置。
首先，创建一个tes目录和几个lib文件夹：

```
$ mkdir -p $HOME/test
$ mkdir -p $HOME/test/{bin,lib64,lib}
```

然后，把bash命令拷贝到test目录对应的bin路径下:

```
$ cp -v /bin/{bash,ls} $HOME/test/bin
```

接下来，把bash命令需要的所有so文件，也拷贝到test目录对应的lib路径下。

```
$ T=$HOME/test
$ list="$(ldd /bin/ls | egrep -o '/lib.*\.[0-9]')"
$ for i in $list; do cp -v "$i" "${T}${i}"; done
```

最后，执行chroot命令，告诉操作系统，我们将使用$HOME/test目录作为/bin/bash进程的根目录:

```
$ chroot $HOME/test /bin/bash
```

此时，如果你执行".bin/ls /"，就会看到，它返回的都是$HOME/test目录下面的内容，而不是宿主机的内容。
实际上，Mount Namespace正是基于对chroot的不断改良才被发明出来的，它也是Linux操作系统里的第一个Namesapce。
当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如Ubuntu16.04 的ISO。这样，在容器启动后，我们在容器里通过执行"ls /"查看根目录下的内容，就是Ubuntu16.04的所有目录和文件。
而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫做：rootfs（根文件系统）
所以，一个最常见的rootfs，或者说容器镜像，会包括如下所示的一些目录和文件，比如 /bin，/etc，/proc 等等：

```
$ ls /
bin dev etc home lib lib64 mnt opt proc root run sbin sys tmp usr var
```

而你进入容器之后执行的/bin/bash，就是/bin目录下的可执行文件，与宿主机的/bin/bash完全不同。
因此，对于Docker项目来说，它最核心的原理实际就是为待创建的用户进程：
1、启用Linux Namespace配置
2、设置指定的Cgroups参数
3、切换进程的根目录（Change Root）

### 05.10 容器的一致性

需要明确的是，rootfs只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在Linux操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定的内核镜像。正是由于rootfs的存在，容器才有了一个被反复宣传至今的重要特性：一致性。

对大多数开发者而言，他们对应用依赖的理解，一直局限在编程语言层面。比如，Golang的Godeps.json。但实际上一个一直以来很容易被忽略的事实是，对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。由于rootfs里打包的不只是应用，而是整个操作系统的文件和目录，这也意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟。

### 05.11 Docker引入的层（layer）： 增量rootfs

docker支持多种graphDriver(联合文件系统)，包括vfs，deviceMapper，overlay，overlay2，aufs等。可以通过如下命令查看docker默认使用的联合文件系统

```
$docker info | grep "Storage Driver"
Storage Driver: overlay2
```



#### 05.11.1 overlay2联合文件系统

```
$ docker image inspect ubuntu:latest
...
"Data": {
                "LowerDir": "/var/lib/docker/overlay2/b27b9e55ac2d1325823ac8dad02f031e959a8687a6da69154c841366a915f413/diff:/var/lib/docker/overlay2/8acc5d7bda8f6f79bf4fe8f3e152049f5e4b15e8d67e4003887473c81d90fe6b/diff:/var/lib/docker/overlay2/3f127af7afa0fb45b8cf9c12ebe9ecfae16a242c4466ea18178ed09cb3f0d4eb/diff",
                "MergedDir": "/var/lib/docker/overlay2/9c746e67e12eb8c183c0da500b94c2fa40dffdeddda974629bcab0851ede2629/merged",
                "UpperDir": "/var/lib/docker/overlay2/9c746e67e12eb8c183c0da500b94c2fa40dffdeddda974629bcab0851ede2629/diff",
                "WorkDir": "/var/lib/docker/overlay2/9c746e67e12eb8c183c0da500b94c2fa40dffdeddda974629bcab0851ede2629/work"
            }
...
```

下层目录（lowerdir）：镜像层，只读
上层目录（upperdir）：容器层，可读写
统一目录：merged，联合挂载目录

容器读写是如何使用overlay2 ？

读：读文件的时候，文件不在upperdir则从lowerdir读，如果upperdir和lowerdir存在相同名称的文件，则读取upperdir中的文件。
写： 在第一次写某个文件时，该文件只存在lowerdir中，则从lowerdir里面copy_up到upperdir层，不管文件多大，copy完再写，之后的操作就只修改upperdir层中文件的副本
删除：删除或者重命名镜像层的文件都只是在容器层生成whiteout文件标志。



#### 05.11.2 aufs 联合文件系统

Docker在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量rootfs。通过一个例子来说明，现在我们启动过一个容器，比如：

```
$ docker login -u user -p password
$ docker run -d ubuntu:latest sleep 3600
```

上述代码会从Docker Hub上拉取一个Ubuntu镜像到本地，查看镜像的内容：

```
$ docker image inspect ubuntu:latest
...
     "RootFS": {
      "Type": "layers",
      "Layers": [
        "sha256:f49017d4d5ce9c0f544c...",
        "sha256:8f2b771487e9d6354080...",
        "sha256:ccd4d61916aaa2159429...",
        "sha256:c01d74f99de40e097c73...",
        "sha256:268a067217b5fe78e000..."
      ]
    }
```

可以看到，这个Ubuntu镜像，实际上由五个层组成。这五个层就是五个增量rootfs，每一层都是Ubuntu操作系统文件与目录的一部分；而在使用镜像时，Docker会把这些增量联合挂载在一个统一的挂载点上。这个挂载点就是/var/lib/docker/aufs/mnt,  比如：

```
/var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e
```

不出意外，这个目录里面正是一个完整的Ubuntu操作系统：

```
$ ls /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e
bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var
```

那么，前面提到的五个镜像层，又是如何被联合挂载成这样一个完整的Ubuntu文件系统的呢？这个信息记录在AuFS的系统目录 /sys/fs/aufs下面，首先通过AuFS的挂载信息，找到对应目录的AuFS的内部ID（也叫si）:

```
$ cat /proc/mounts| grep aufs
none /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fc... aufs rw,relatime,si=972c6d361e6b32ba,dio,dirperm1 0 0
```

通过这个si，可以在/sys/fs/aufs下面查看被联合挂载在一起的各个层的信息:

```
$ cat /sys/fs/aufs/si_972c6d361e6b32ba/br[0-9]*
/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...=rw
/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...-init=ro+wh
/var/lib/docker/aufs/diff/32e8e20064858c0f2...=ro+wh
/var/lib/docker/aufs/diff/2b8858809bce62e62...=ro+wh
/var/lib/docker/aufs/diff/20707dce8efc0d267...=ro+wh
/var/lib/docker/aufs/diff/72b0744e06247c7d0...=ro+wh
/var/lib/docker/aufs/diff/a524a729adadedb90...=ro+w
```

从这些信息里，我们可以看到，镜像的层都放置在/var/lib/docker/aufs/diff 目录下，然后被联合挂载在/var/lib/docker/aufs/mnt里面。

从这个结构可以看出来，这个容器的rootfs由三部分组成：
1: 只读层（ro+wh）: 对应的正式ubuntu:latest镜像的五层
3: 可读写层（rw）:  用来存放你修改rootfs后产生的增量，无论是增、删、改、查都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用docker commit和push指令，保存这个被修改过的可读写层，并上传到Docker Hub上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这就是增量rootfs的好处 
2: init层（ro+wh）: init层是docker项目单独生成的一个内部层，专门用来存放/etc/hosts、/etc/resolv.conf等信息，这些文件本来属于只读的Ubuntu镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如hostname，所以就需要在可读写层对它们进行修改，但是我们又不希望执行docker commit时，把这些信息连通可读写层一起提交掉。因此以一个单独的层挂载了出来

最终，这7个层被联合挂载到了/var/lib/docker/aufs/mnt目录下



## 05 白话容器基础（四）：重新认识Docker容器

### 05.12 使用Docker部署应用

首先准备一个简单的python应用，使用Flask框架启动一个Web服务器：

```
from flask import Flask
import socket
import os

app = Flask(__name__)

@app.route('/')
def hello():
    html = "<h3>Hello {name}!</h3>" \
           "<b>Hostname:</b> {hostname}<br/>"           
    return html.format(name=os.getenv("NAME", "world"), hostname=socket.gethostname())
    
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=80)
```

填写依赖文件

```
echo "Flask" > requirements.txt
```

编写Dockerfile文件

```
# 使用官方提供的Python开发镜像作为基础镜像
FROM python:2.7-slim

# 将工作目录切换为/app
WORKDIR /app

# 将当前目录下的所有内容复制到/app下
ADD . /app

# 使用pip命令安装这个应用所需要的依赖
RUN pip install --trusted-host pypi.python.org -r requirements.txt

# 允许外界访问容器的80端口
EXPOSE 80

# 设置环境变量
ENV NAME World

# 设置容器进程为：python flask_demo.py，即：这个Python应用的启动命令
CMD ["python", "flask_demo.py"]
```

编写完成之后，可以看到当前目录下有3个文件

```
$ls
Dockerfile  flask_demo.py  requirements.txt
```

使用docker build来制作这个镜像

```
$docker build -t hellodocker .
```

docker build会自动加载当前目录下的Dockerfile文件，然后按顺序执行。上述的过程实际上可以等同于Docker使用基础镜像启动了一个容器，然后在容器中依次执行Dockerfile中的原语。接下来，可以查看build的结果

```
docker image ls
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
hellodocker         latest              e7fffe50542f        55 seconds ago      158MB
```

通过docker run命令启动容器：

```
docker run -p 4000:80 hellodocker
```

在这一句命令中，镜像名hellodocker后面什么都没写，因为在Dockerfile中已经指定了CMD。否则，就得把进程的启动命令加在后面：

```
docker run -p 4000:80 hellodocker python flask_demo.py
```

容器启动后，可以使用docker ps命令查看到docker进程，同时我们已经通过 -p 4000:80告诉了Docker，把容器内的80端口映射到宿主机的4000端口上，这样做的目的是，只要访问宿主机的4000端口，就可以看到容器里应用返回的结果

```
$ curl http://localhost:4000
<h3>Hello World!</h3><b>Hostname:</b> 4ddf4638572d<br/>
```

否则，就需要使用docker inspect命令查看容器的IP地址，然后访问"http://<容器IP地址>:80"才可以看到容器内应用的返回。

```
# 7efe207668f1是容器的container id
$docker inspect 7efe207668f1
...
"Networks": {
                "bridge": {
                    "IPAMConfig": null,
                    "Links": null,
                    "Aliases": null,
                    "NetworkID": "c7fc615145d04ddada89ebe7628b46a5da0cc3eecb89515069d274eb260996bb",
                    "EndpointID": "c98d1c4e5c348fd9700fc457c0e254d5955ed73b3eff9508a2c583455a516e8e",
                    "Gateway": "172.17.0.1",
                    "IPAddress": "172.17.0.2",
                    "IPPrefixLen": 16,
                    "IPv6Gateway": "",
                    "GlobalIPv6Address": "",
                    "GlobalIPv6PrefixLen": 0,
                    "MacAddress": "02:42:ac:11:00:02",
                    "DriverOpts": null
                }
...
```

拿到容器ip可以，可以直接访问

```
curl http://172.17.0.2:80
```



### 05.13 镜像上传到DockerHub

想要将镜像上传到DockerHub分享给更多的人，需要注册一个Docker Hub的账号： https://hub.docker.com/，然后在机器上使用docker login命令登陆。接下来使用docker tag命令给容器镜像起一个完成的名字，v1是给镜像分配的版本号

```
$docker tag hellodocker canghong/hellodocker:v1
```

然后使用docker push

```
$docker push canghong/hellodocker:v1
```

此外，还可以使用docker commit指令，把一个正在运行的容器，直接提交为一个镜像。一般来说，需要这么操作的原因是：这个容器运行起来后，我又在里面做了一些操作，并且要把操作结果保存到镜像里，比如： 

```
#7efe207668f1是通过ps查询出来的container id
$ docker exec -it 7efe207668f1 /bin/sh
# 在容器内部新建了一个文件
root@4ddf4638572d:/app# touch test.txt
root@4ddf4638572d:/app# exit

#将这个新建的文件提交到镜像中保存
$ docker commit 7efe207668f1 canghong/hellodocker:v2

# 推送新版本到Docker Hub上
$docker push canghong/hellodocker:v2
```

### 05.14 Docker exec 原理解析

上述我们使用了docker exec命令进入到了容器当中。在了解了Linux Namespace的隔离机制后，应该会很自然地想到一个问题： docker exec是怎么做到进入容器里的呢？

实际上，Linux Namespace创建的隔离空间虽然看不见摸不着，但一个进程的Namespace信息在宿主机上确确实实存在的，并且是以一个文件的方式存在。比如，通过如下指令，可以看到当前正在运行的Docker容器国旗的进程号（PID）是25686:

```
$ docker inspect --format '{{ .State.Pid }}'  7efe207668f1
44999
```

这时，你可以通过查看宿主机的proc文件，看到这个进程的所有Namespace对应的文件:

```
$ ls -l /proc/44999/ns
total 0
lrwxrwxrwx 1 root root 0 Mar 28 14:33 cgroup -> cgroup:[4026531835]
lrwxrwxrwx 1 root root 0 Mar 27 22:14 ipc -> ipc:[4026534461]
lrwxrwxrwx 1 root root 0 Mar 27 22:14 mnt -> mnt:[4026534459]
lrwxrwxrwx 1 root root 0 Mar 27 22:14 net -> net:[4026534464]
lrwxrwxrwx 1 root root 0 Mar 27 22:14 pid -> pid:[4026534462]
lrwxrwxrwx 1 root root 0 Mar 27 22:14 user -> user:[4026531837]
lrwxrwxrwx 1 root root 0 Mar 27 22:14 uts -> uts:[4026534460]
```

可以看到，一个进程的每种Linux Namespace都在它对应的/proc/[进程号]/ns 下有一个对应的虚拟文件，并且链接到一个真实的Namespace文件上。有了这样一个可以“hold”住所有Linux Namespace的文件，我们就可以对Namespace做一些很有意义的事情了，比如：加入到一个已经存在的Namespace当中。

这也意味着： 一个进程，可以选择加入到某个进程已有的Namespace当中，从而达到“进入”这个进程所在容器的目的，这正是docker exec的实现原理。

而这个操作所依赖的，是一个叫名叫setns()的Linux系统调用。它的调用方法，可以用如下的一小段程序说明：

```
#define _GNU_SOURCE
#include <fcntl.h>
#include <sched.h>
#include <unistd.h>
#include <stdlib.h>
#include <stdio.h>

#define errExit(msg) do { perror(msg); exit(EXIT_FAILURE);} while (0)

int main(int argc, char *argv[]) {
    int fd;
    
    fd = open(argv[1], O_RDONLY);
    if (setns(fd, 0) == -1) {
        errExit("setns");
    }
    execvp(argv[2], &argv[2]); 
    errExit("execvp");
}
```

这段代码的功能非常简单：它一共接收两个参数，第一个参数是argv[1]，即当前进程要加入Namespace文件的路径，比如/proc/25686/ns/net；而第二个参数，则是你要在这个Namespace里运行的进程，比如/bin/bash。
这段代码的核心操作，是通过open()系统调用打开了指定的Namespace文件，并把这个文件的描述符fd交给setns()使用。在setns()执行后，当前进程就加入了这个文件对应的Linux Namepsace当中了。

```
$ gcc -o set_ns set_ns.c 
$ ./set_ns /proc/44999/ns/net /bin/bash 
$ ifconfig
eth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:02  
          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:12 errors:0 dropped:0 overruns:0 frame:0
          TX packets:10 errors:0 dropped:0 overruns:0 carrier:0
     collisions:0 txqueuelen:0 
          RX bytes:976 (976.0 B)  TX bytes:796 (796.0 B)

lo        Link encap:Local Loopback  
          inet addr:127.0.0.1  Mask:255.0.0.0
          inet6 addr: ::1/128 Scope:Host
          UP LOOPBACK RUNNING  MTU:65536  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
    collisions:0 txqueuelen:1000 
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
```

正如上所示，当我们执行ifconfig命令查看网络上设备时，我们会发现能看到的网卡“改变”了。实际上，在setns()之后我们看到的网卡，正是在前面启动Docker容器里的网卡。也就是说，新创建的这个/bin/bash进程，由于也叫加入了该容器进程（PID=25686）的Network Namespace，它看到的网络设备与这个容器里是一样的，即：/bin/bash进程的网络设备试图，也被修改了。
在宿主机上，可以用ps指令找到这个set_ns程序执行的/bin/bash进程，其真实的PID是:

```
# 在宿主机上
ps aux | grep /bin/bash
root     157738  0.0  0.0 19944  3612 pts/0    S    14:15   0:00 /bin/bash
```

这时，如果按照前面介绍过的方法，查看一下这个PID的进程的Namespace，会发现：

```
$ ls -l /proc/157738/ns/net
lrwxrwxrwx 1 root root 0 Mar 28 14:44 /proc/157739/ns/net -> net:[4026534464]
$ ls -l  /proc/44999/ns/net
lrwxrwxrwx 1 root root 0 Mar 27 22:14 /proc/44999/ns/net -> net:[4026534464]

```

两个进程只想的Network Namespace完全一样。这说明这两个进程共享了一个名叫 net:[4026534464]的network Namespace。

此外，Docker还专门提供了一个参数，可以让你启动一个容器并“加入”到另一个容器的Network Namespace里，这个参数就是 -net，比如：

```
$ docker run -it --net container:7efe207668f1 busybox ifconfig
```

这样，我们新启动的这个容器，就会直接加入到7efe207668f1的容器，也就是前面创建的Python应用容器的Network Namespace中。所以，这里ifconfig返回的网卡信息，跟前面那个小程序返回的结果一摸一样。而如果我指定-net=host，这意味着这个容器不会为进程启用Network Namespace。这意味着，这个容器拆除了Network Namespace的“隔离墙”，所以，它会和宿主机上的其他普通进程一样，直接共享宿主机的网络栈。这就为容器直接操作和使用宿主机网络提供了一个渠道。

 ### 05.15 Docker Volume

前面介绍了使用rootfs机制和Mount Namespace，构建出了一个同宿主机完全隔离开的文件系统环境。这时候，我们就需要考虑两个问题：
1:  容器里进程创建的文件，怎么才能让宿主机获取到？
2：宿主机上的文件和目录，怎么才能让容器里的进程访问到？
这正是Docker Volume要解决的问题： Volume机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。如下命令，Docker就直接把宿主机的/home 目录挂载到容器的 /test目录上

````
$docker run -v /home:/test ...
````

这个原理也比较简单： 在容器进程被创建之后，尽管开启了Mount Namespace，但是在它执行chroot之前，容器进程一直可以看到宿主机上的整个文件系统。所以，我们只需要在rootfs准备好之后，在执行chroot之前，把volume指定的宿主机目录，挂载到指定的容器目录在宿主机上对应的目录上，这个Volume的挂载工作就可以完成了。

注意：上面提到的“容器进程”，是Docker创建的一个容器初始化进程（dockerinit），而不是应用进程（ENTRYPOINT+CMD）。dockerinit会负责完成根目录的准备、挂载设备和目录、配置hostname等一系列需要在容器内进行的初始化操作。最后，它通过execv()系统调用，让应用进程取代自己，成为容器里的PID=1的进程。

此外，容器Volume里的信息，并不会被docker commit提交掉。原因是使用 -v /home:/test创建volume时，docker只会在其可读写层创建一个空目录/test，然后挂载到/home目录。因此，对/test这个volume所有的实际操作都发生在宿主机的对应目录/home，在宿主机看来，容器中可读写层的/test始终是空的。因此，容器Volume里的信息，并不会被docker commit提交掉；但这个挂载点目录/test本身，则会出现在新的镜像中










# 深入理解StatefulSet

## 18 深入理解StatefulSet（一）： 拓扑状态

###  18.01 为什么要有StatefulSet ?

Deployment实际上并不足以覆盖所有的应用编排问题，因为对于Deployment而言，一个应用的所有Pod是完全一样的。所以，它们互相之间没有顺序，也无所谓运行在哪台宿主机上。需要的时候，Deployment就可以通过Pod模版创建新的Pod；不需要的时候，Deployment就可以“杀掉”任意一个Pod。

但是，并不是所有的应用都可以满足这样的需求，对于分布式应用，它的多个实例之间，往往有依赖关系，比如：主从关系、主备关系。还有数据存储类应用，它的多个实例，往往都会在本地磁盘上保存一份数据。而这些实例一旦被杀掉，即便重建出来，实例与数据之间的对应关系也已经失去，从而导致应用失败。

容器技术诞生后，大家很快发现，它用来封装“无状态应用”（Stateless Application），尤其是Web服务，非常好用。但是，一旦你想要用容器运行“有状态应用”，其困难程度就会直线上升。而且，这个问题解决起来，单纯依靠容器技术本身已无能为力，这也导致了很长一段时间内，“有状态应用”几乎成了容器技术圈子里的“忌讳”，大家一听到这个词，就纷纷摇头。

不过，Kubernetes项目还是成为了“第一个吃螃蟹的人”。

得益于“控制器模式”的设计思想，Kubernetes项目很早就在Deployment的基础上，扩展出了对“有状态应用”的初步支持。这个编排功能，就是: StatefulSet

### 18.02 StatefulSet核心功能

StatefulSet的设计其实非常容容易理解。它把真实世界里的应用状态，抽象为了两种情况：

1：拓扑状态。这种情况意味着，应用的多个实例之间不是完全对等的关系。这些应用实例子，必须按照某些顺序启动，比如应用的主节点A要先于从节点B启动。而如果你把A和B两个Pod删除掉，它们再次被创建出来时也必须严格按照这个顺序才行。并且，新创建出来的Pod，必须和原来Pod的网络标识一样，这样原来的访问者才能使用同样的方法，访问到这个新的Pod
2：存储状态。这种情况意味着，应用的多个实例分别绑定了不同的存储数据。对于这些应用实例来说，Pod A第一次读取到的数据，和隔了十分钟之后再次读取到的数据，应该是同一份，哪怕在此期间Pod A被重新创建过。这种情况最典型的例子，就是一个数据库应用的多个存储实例

所以，StatefulSet的核心功能，就是通过某种方式记录这些状态，然后在Pod被重新创建时，能够为新Pod恢复这些状态。

### 18.03 StatefulSet如何保证“拓扑状态”的稳定性

#### 18.03.01 Service是如何被访问的？

Kubernetes中，Service被访问的方式主要是以下两种方式：
1：以Service的VIP（Virtual IP，即：虚拟IP）方式。比如：当访问10.0.23.1这个Service的IP地址时，它会把请求转发到该Service所代理的某一个Pod上。
2：以Service的DNS方式。比如，访问"my-svc.my-namespace.svc.cluster.local"这条DNS记录，就可以访问到名叫my-svc的Service所代理的某一个Pod。而在这种Service DNS的方式下，具体还可以分为两种处理方法：第一种处理方法
2.1： Normal Service，这种情况下，你访问"my-svc.my-namespace.svc.cluster.local"解析到的，正是my-svc这个Service的VIP，后面的流程就跟VIP方式的一致了。
2.2：Headless Service。这种情况下，你访问"my-svc.my-namespace.svc.cluster.local"解析到的，直接就是my-svc代理的某一个Pod的IP地址。可以看到，这里的区别在于，Headless Service不需要分配一个VIP，而是可以直接以DNS记录的方式解析出被代理Pod的IP地址。

#### 18.03.02 Headless Service的定义

下面是一个标准的Headless Service对应的YAML文件

```
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
```

可以看到，所谓的Headless Service，其实仍然是一个标准Service的YAML文件。只不过，它的clusterIP字段的值是：None。即：这个Service，没有一个VIP作为“头”，这就是Headless的含义。

当你按照这样的方式创建了一个Headless Service之后，它所代理的所有Pod的IP地址，都会被绑定一个这样格式的DNS记录，如下所示：

```
<pod-name>.<svc-name>.<namespace>.svc.cluster.local
```

这个DNS记录，正是Kubernetes项目为Pod分配的唯一“可解析身份”。有了这个“可解析身份”，只要你知道了一个Pod的名字，以及它对应的Service的名字，你就可以非常确定地通过这条DNS记录访问到Pod的IP地址

pod_name + service_name -> dns ->  pod ip

#### 18.03.03 StatefulSet通过DNS记录来维持Pod的拓扑状态

首先，编写一个StatefulSet的Yaml文件，如下：

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
```

这个YAML文件，和前面使用过的nginx-deployment的唯一区别，就是多了一个serviceName=nginx字段。这个字段的作用，就是告诉StatefulSet控制器，在执行控制循环（Control Loop）的时候，请使用nginx这个Headless Service来保证Pod的“可解析身份”。

接下来，分别创建Service和StatefulSet，就会看到如下两个对象：

```
$ kubectl apply -f servie.yaml
service/nginx created
$ kubectl get service
NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   15d
nginx        ClusterIP   None         <none>        80/TCP    44s

$ kubectl apply -f statefulset.yaml
statefulset.apps/web created
$ kubectl get statefulset
NAME   READY   AGE
web    2/2     77s
```

通过Watch功能，实时查看StatefulSet创建两个有状态实例的过程：

```
$ watch kubectl get pod 
Every 2.0s: kubectl get pod                                                                  Sun Apr 19 17:05:38 2020

NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          119s
web-1   1/1     Running   0          118s

```

通过上面这个Pod的创建过程，我们不难看到，StatefulSet给它所管理的所有Pod的名称，进行了编号，编号规则是："-"。 这些编号都是从0开始累加，与StatefulSet的每个Pod实例一一对应绝不重复。

更重要的是，这些Pod的创建，也是严格按照编号顺序进行的。比如，在web-0进入到Running状态，并且细分状态（Conditions）成为Ready之前，web-1会一直处于Pending状态。（Ready状态再一次提醒了我们，为Pod设置livenessProbe和readinessProbe的重要性）。当两个Pod都进入了Running状态之后，你就可以查看到它们各自唯一的“网络身份”了。

我们通过kubectl exec命令进入到容器中查看它们的hostname:

```
$ kubectl exec web-0 -- sh -c 'hostname'
web-0
$ kubectl exec web-1 -- sh -c 'hostname'
web-1
```

可以看到，这两个Pod的hostname与Pod的名字是一致的，都被分配了对应的编号。接下来，我们再试着以DNS的方式，访问一下这个Headless Service：

```
$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh 
```

通过这条命令，我们启动了一个一次性的Pod，因为--rm意味着Pod推出后就会被删除掉。然后，在这个Pod的容器里面，我们尝试用nslookup命令，解析一下Pod对应的Headless Service：通过<pod_name>.<service_name>.<namespace>.svc.cluster.local 的方式访问

```
$  kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh
$ nslookup web-0.nginx.default.svc.cluster.local
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-0.nginx.default.svc.cluster.local
Address 1: 10.42.0.5 web-0.nginx.default.svc.cluster.local

$ nslookup web-1.nginx.default.svc.cluster.local
Server:    10.96.0.10
Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

Name:      web-1.nginx.default.svc.cluster.local
Address 1: 10.44.0.8 web-1.nginx.default.svc.cluster.local
```

从nslookup命令的输出结果中，我们可以看到，在访问web-0.nginx的时候，最后解析到的，正是web-0这个Pod的IP地址；而当访问web-1.nginx的时候，解析到的则是web-1的IP地址。

这时候，如果在另一个Terminal里把这两个“有状态应用”的Pod删掉：

```
$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted
```

然后，再在当前Termial里Watch一下这两个Pod的状态变化，就会发现一个有趣的现象：

```
$ kubectl get pod -w -l app=nginx
NAME      READY     STATUS              RESTARTS   AGE
web-0     0/1       ContainerCreating   0          0s
NAME      READY     STATUS    RESTARTS   AGE
web-0     1/1       Running   0          2s
web-1     0/1       Pending   0         0s
web-1     0/1       ContainerCreating   0         0s
web-1     1/1       Running   0         3
```

可以看到，当我们把这两个Pod删除之后，Kubernetes会按照原先编号的顺序，创建出了两个新的Pod。并且，Kubernetes依然为它们分配了与原先相同的“网络身份”：web-0.nginx和web-1.nginx。通过这种严格的对应规则，StatefulSet就保证了Pod网络标识的稳定性。

所以，如果我们再用nslookup命令，查看一下这个新Pod对应的Headless Service的话：

我们可以看到，再这个StatefulSet中，这两个新Pod的“网络标识”（比如： web-0.nginx和web-1.nginx），再次解析到了正确的IP地址（ip地址可能会改变，但是网络标识不变 <pod_name>.<service_name>.<namespace>.svc.cluster.local  ）

通过这种发那个方式，Kubernetes就成功地将Pod的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照Pod的“名字+编号”的方式固定了下来。此外，Kubernetes还为每一个Pod提供了一个固定并且唯一的访问入口，即：这个Pod对应的DNS记录。这些状态，在StatefulSet的整个生命周期都会保持不变，绝不会因为对应Pod的删除或者重新创建而失败。

不过，景观web-0.nginx这条记录本身不会变，但它解析到的Pod的IP地址，并不是固定的。这也意味活着，对于“有状态应用”的实例的访问，你必须使用DNS记录或者hostname的方式，而绝不应该直接访问这些Pod的IP地址。

### 18.04 小结

StatefulSet这个控制器的主要作用之一，就是使用Pod模版创建Pod的时候，对它们进行编号，并且按照编号顺序逐一完成创建工作。而当StatefulSet的“控制循环”发现Pod的“实际状态” 与“期望状态”不一致，需要新建或者删除Pod进行“调谐”的时候，它会严格按照这些Pod编号的顺序，逐一完成这些操作。

所以，StatefulSet其实可以认为是对Deployment的改良。与此同时，通过Headless Service的方式，StatefulSet为每个Pod创建了一个固定并且稳定的DNS记录，来作为它的访问入口。实际上，在部署“有状态应用”的时候，应用的每一个实例拥有唯一并且稳定的“网络标识”，是一个非常重要的假设



## 19 深入理解StatefulSet（二）： 存储状态



### 19.01 为什么会有PVC和PV

在前面介绍Pod的时候提到过，要在一个Pod里声明Volume，只要在Pod里加上spec.volumes字段即可。然后，你就可以在这个字段里定义一个具体类型的Volume了，比如: hostPath。可是，如果你并不知道有哪些Volume类型可以用，要怎么办呢？

更具体地说，作为一个应用开发者，我可能对持久化存储项目（比如Ceph，ClusterFS等）一窍不通，也不知道公司的Kubernetes集群到底是怎么搭建出来的，自然也不会编写它们对应的Volume定义文件。所谓“术业有专攻”，这些关于Volume的管理和远程持久化存储的知识，不仅超越了开发者的知识储备，还会有暴露公司基础设施秘密的风险。比如，下面这个例子，就是一个声明了Ceph RBD类型Volume的Pod：

```
apiVersion: v1
kind: Pod
metadata:
  name: rbd
spec:
  containers:
    - image: kubernetes/pause
      name: rbd-rw
      volumeMounts:
      - name: rbdpd
        mountPath: /mnt/rbd
  volumes:
    - name: rbdpd
      rbd:
        monitors:
        - '10.16.154.78:6789'
        - '10.16.154.82:6789'
        - '10.16.154.83:6789'
        pool: kube
        image: foo
        fsType: ext4
        readOnly: true
        user: admin
        keyring: /etc/ceph/keyring
        imageformat: "2"
        imagefeatures: "layering"
```

其一，如果不懂得Ceph RBD的使用方法，那么这个Pod里的Volumes字段，十有八九也完全看不懂。其二，这个Ceph RBD对应的存储服务器的地址、用户名、授权文件的位置，也都被轻易地暴露给了全公司的所有开发人员，这是一个典型的信息被“过渡暴露”的例子。

这也是为什么，在后来的演化中，Kubernetes项目引入了一组叫做Persistent Volume Claim（PVC）和Persistent Volume（PV）的API对象，大大降低了用户声明和使用持久化Volume的门槛。

举个例子，有了PVC之后，一个开发人员想要使用一个Volume，只需要简单的两步即可。
第一步：定义一个PVC，声明想要的Volume的属性：

```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: pv-claim
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```

这个PVC对象里，不需要任何关于Volume细节的字段，只有描述性的属性和定义。比如，storage:1Gi，表示想要的Volume大小至少是1GiB；accessModes:ReadWriteOnce，表示这个Volume的挂载方式是可读写，并且只能被挂载在一个节点上而非被多个节点共享。
第二步：在应用的Pod中，声明使用这个PVC：

```
apiVersion: v1
kind: Pod
metadata:
  name: pv-pod
spec:
  containers:
    - name: pv-container
      image: nginx
      ports:
        - containerPort: 80
          name: "http-server"
      volumeMounts:
        - mountPath: "/usr/share/nginx/html"
          name: pv-storage
  volumes:
    - name: pv-storage
      persistentVolumeClaim:
        claimName: pv-claim
```

可以看到，在这个Pod的Volumes定义中，我们只需要声明它的类型是persistentVolumeCliam，然后指定PVC的名字，而完全不必关心Volume本身的定义。这时候，只要我们创建这个PVC对象，Kubernetes就会自动为它绑定一个符合条件的Volume。可是，这些符合条件的Volume又是从哪里来的呢？
答案是，它们来自于运维人员维护的PV（Persistent Volume）对象。接下来，我们一起看一个常见的PV对象的YAML文件：

```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume
  labels:
    type: local
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    # 使用 kubectl get pods -n rook-ceph -o wide | grep rook-ceph-mon 查看 rook-ceph-mon- 开头的 POD IP 即可得下面的列表
    - '10.36.0.7:6789'
    - '10.44.0.4:6789'
    - '10.42.0.2:6789'
    pool: kube
    image: foo
    fsType: ext4
    readOnly: true
    user: admin
    keyring: /etc/ceph/keyring
```

可以看到，这个PV对象的spec.rbd字段，正是我们前面介绍过的Ceph RBD Volume的详细定义。而且，它还声明了这个PV的容量是10GiB。这样，Kubernetes就会为我们刚刚创建的PVC对象绑定这个PV。

所以，Kubernetes中的PVC和PV的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。PVC、PV的设计，也使得StatefulSet对存储状态的管理成为了可能。

### 19.02 StatefulSet对存储状态的管理

我们创建一个StatefulSet （注意，需要提前创建好上文的PV）

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
```

这是，我们为这个StatefulSet额外添加了一个volumeCliamTemplates字段。凡是被这个StatefulSet管理的Pod ，都会声明一个对应的PVC；而这个PVC的定义，就来自于volumeCliamTemplates这个模版字段。更重要的是，这个PVC的名字，会被分配一个与这个Pod完全一致的编号。 这个自动创建的PVC，与PV绑定成功后，就会进入Bound状态，这也意味着这个Pod可以挂载并使用这个PV了。

如果你还是不台理解PVC的话，可以先记住这样一个结论：PVC其实就是一种特殊的Volume 。只不过一个PVC具体是什么类型的Volume，要在跟某个PV绑定之后才知道。

我们在创建这个StatefulSet之后，就会看到集群中出现了两个PVC：

```
$ kubectl ap -f pv.yaml
$ kubectl create -f statefulset.yaml
$ kubectl get pvc -l app=nginx
NAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE
www-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s
www-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s
```

可以看到，这些PVC，都以"<PVC_name>-<StatefulSet_name>-<编号 >"的方式命名，并且处于Bound状态。我们前面已经讲到过，这个StatefulSet创建出来的所有Pod，都会声明使用编号的PVC。比如，在鸣叫web-0的Pod的volumes字段，它会声明使用名叫www-web-0的PVC，从而挂载到这个PVC所绑定的PV。

所以，我们可以使用如下所示的命令，在Pod的Volume目录里写入一个文件，来验证一下上述Volume的分配情况：

```
$ for i in 0 1; do kubectl exec web-$i -- sh -c 'echo hello $(hostname) > /usr/share/nginx/html/index.html'; done
```

如上所示，通过kubectl exec指令，我们在每个Pod的Volume目录里，写入了一个index.html文件。这个文件的内容，正是Pod的hostname。比如，我们在web-0的index.html里写入的内容就是"hello web-0"。此时，如果你在这个Pod容器里访问“http://localhost”，你实际访问到的就是Pod里Nginx服务器进程，而它会为你返回/usr/share/nginx/html/index.html里的内容。操作方法如下：

```
$ for i in 0 1; do kubectl get pod web-$i -o wide -o json | jq -j '.status.podIP' | xargs echo | xargs curl  ; done
hello web-0
hello web-1
```

接下来我们删除这两个Pod，看看这些Volume的文件会不会丢失

```
$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted
```

在被删除之后，这两个Pod会按照编号的顺序被重新创建出来。而这时候，我们在新创建的容器里通过访问“http://localhost”的方式去访问web-0里的nginx服务：

```
# 在被重新创建出来的Pod容器里访问http://localhost
$ kubectl exec -it web-0 -- curl localhost
hello web-0
```

就会发现，这个请求依然返回：hello web-0。也就是说，原先与名叫web-0的Pod绑定的PV，在这个Pod被重新创建之后，依然同新的名叫web-0的Pod绑定在一起。对于web-1来说，也是一样。

### 19.03 StatefulSet如何恢复PVC

当把一个Pod，比如web-0删除之后，这个Pod对应的PVC和PV，并不会被删除，因此数据也依然保存在远程存储服务里（比如，我们在这个例子里用到的是Ceph服务器）。

此时，StatefulSet控制器发现，一个名叫web-0的Pod消失。所以，控制器就会重新创建一个新的、名字还是叫做web-0的Pod来“纠正”这个不一致的情况。而需要注意的是，在这个新的Pod对象的定义里，它声明使用的PVC的名字，还是叫做：www-web-0。这个PVC的定义，还是来自PVC模版（volumeClaimTemplates），这是StatefulSet创建Pod的标准流程。

所以，在这个新的web-0 Pod被创建出来之后，Kubernetes为它查找名叫www-web-0的PVC时，就会直接找到旧Pod遗留下来的同名的PVC，进而找到跟这个PVC绑定在一起的PV。这样，新的Pod就可以挂载到旧Pod对应的那个Volume，并且获取到保存在Volume里的数据。

通过这种方式，Kubernetes的StatefulSet就实现了对应用存储状态的管理。

### 19.04 StatefulSet的工作原理

首先，StatefulSet的控制器直接管理的是Pod。这是因为，StatefulSet里的不同Pod实例，不再像ReplicaSet中那样都是完全一样的，而是又了细微区别。比如，每个Pod的hostname，名字等都是不同的、携带了编号的。而StatefulSet区分这些实例的方式，就是通过在Pod的名字里加上事先约定好的编号。

其次，Kubernetes通过Headless Service，为这些有编号的Pod，在DNS服务器中生成带有同样编号的DNS记录。只要StatefulSet能够保证这些Pod名字里的编号不变，那么Service里类似于web-0.nginx.default.svc.cluster.local这样的DNS记录也就不会变，而这条记录解析出来的Pod的IP地址，则会随着后端Pod的删除和再创建而自动更新。这当然是Service机制本身的能力，不需要StatefulSet操心。

在这种情况下，即使Pod被删除，它所对应的PVC和PV依然会保留下来。所以当这个Pod被重新创建出来之后，Kubernetes会为它找到同样编号的PVC，挂载这个PVC对应的Volume，从而获取到以前保存在Volume里的数据。

### 19.05 通过hostPath创建PV

由于前面介绍的PV需要有ceph，搭建起来比较费劲。因此介绍一种使用hostPATH创建PV的方式，可以用于开发和测试，

尝试使用hostPath创建pv，用来开发和测试

pv2.yaml

```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```



```
#kubectl apply -f pv2.yaml
```

修改 statefulset.yaml

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      storageClassName: manual
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
```



```
#kubectl apply -f statefulset.yaml
```

观察到 web-0： running，web-1： pengding

```
#kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          5m38s
web-1   0/1     Pending   0          5m36s
```



```
#kubectl describe pod web-0
...
Node:         bd011088191061.na610/11.88.191.61
...
```

可以到机器 bd011088191061.na610上看到，机器出现了 /mnt/data目录

查看一下web-1为什么pengding

```
#kubectl describe pod web-1
...
Warning  FailedScheduling  <unknown>  default-scheduler  running "VolumeBinding" filter plugin for pod "web-1": pod has unbound immediate PersistentVolumeClaims
...
```

查看一下pvc是否正常

```
#kubectl get pvc
NAME        STATUS    VOLUME      CAPACITY   ACCESS MODES   STORAGECLASS   AGE
www-web-0   Bound     pv-volume   10Gi       RWO            manual         16m
www-web-1   Pending                                         manual         16m

#kubectl describe pvc www-web-1
...
Warning  ProvisioningFailed  43s (x102 over 25m)  persistentvolume-controller  storageclass.storage.k8s.io "manual" not found
....

```

看报错是没有找到manual这个storageclass，那鬼知道为什么www-web-0为什么能创建成功？先不管，手工创建一个storageclass

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: manual
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer
```

```
#kubectl apply -f storageclass.yaml
```

此时，pvc的异常信息变化了

```
...
Normal   WaitForFirstConsumer  36s (x88 over 22m)   persistentvolume-controller  waiting for first consumer to be created before binding
...
```

可以知道，是缺少pv导致的，即原先创建的pv-volume已经被挂载了，因此再创建一个pv

```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pv-volume2
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
```

可以看到，两个pod都成功运行

```
#kubectl get pod
NAME    READY   STATUS    RESTARTS   AGE
web-0   1/1     Running   0          56m
web-1   1/1     Running   0          56m
```



### 19.06 使用NSF创建持久化存储

搭建NFS服务

```
#master节点安装nfs
yum -y install nfs-utils -b current

#创建nfs目录
mkdir -p /nfs/data/

#修改权限
chmod -R 777 /nfs/data

#编辑export文件
vim /etc/exports
/nfs/data *(rw,no_root_squash,sync)

#配置生效
exportfs -r
#查看生效
exportfs

#启动rpcbind、nfs服务
systemctl restart rpcbind && systemctl enable rpcbind
systemctl restart nfs && systemctl enable nfs

#查看 RPC 服务的注册状况
rpcinfo -p localhost

#showmount测试
showmount -e 11.88.191.61

#所有node节点安装客户端
yum install nfs-utils -b current -y
systemctl start nfs && systemctl enable nfs

```

#### 静态申请PV卷

添加pv卷对应目录,这里创建2个pv卷，则添加2个pv卷的目录作为挂载点。

```
#创建pv卷对应的目录
mkdir -p /nfs/data/pv001
mkdir -p /nfs/data/pv002

#配置exportrs
vim /etc/exports
/nfs/data *(rw,no_root_squash,sync)
/nfs/data/pv001 *(rw,no_root_squash,sync)
/nfs/data/pv002 *(rw,no_root_squash,sync)

#配置生效
exportfs -r
#重启rpcbind、nfs服务
systemctl restart rpcbind && systemctl restart nfs

```

创建两个PV

nfs-pv001.yaml:

```
$cat nfs-pv001.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv001
  labels:
    pv: nfs-pv001
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  nfs:
    path: /nfs/data/pv001
    server: 11.88.191.61
```

nfs-pv002.yaml:

```
$cat nfs-pv002.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs-pv002
  labels:
    pv: nfs-pv002
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  nfs:
    path: /nfs/data/pv001
    server: 11.88.191.61
```

创建PV

```
$kubectl apply -f nfs-pv001.yaml
$kubectl apply -f nfs-pv002.yaml
$kubectl get pv
NAME        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
nfs-pv001   1Gi        RWO            Recycle          Available           nfs                     38s
nfs-pv002   1Gi        RWO            Recycle          Available           nfs                     3s
```

修改statefulset.yaml

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      storageClassName: nfs
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
```

创建statefulset

```
$kubectl apply -f statefulset.yaml
```

我们可以使用如下所示的命令，在Pod的Volume目录里写入一个文件，来验证一下上述Volume的分配情况：

```
$ for i in 0 1; do kubectl exec web-$i -- sh -c 'echo hello $(hostname) > /usr/share/nginx/html/index.html'; done
```

如上所示，通过kubectl exec指令，我们在每个Pod的Volume目录里，写入了一个index.html文件。这个文件的内容，正是Pod的hostname。比如，我们在web-0的index.html里写入的内容就是"hello web-0"。此时，如果你在这个Pod容器里访问“http://localhost”，你实际访问到的就是Pod里Nginx服务器进程，而它会为你返回/usr/share/nginx/html/index.html里的内容。操作方法如下：

```
$ for i in 0 1; do kubectl get pod web-$i -o wide -o json | jq -j '.status.podIP' | xargs echo | xargs curl  ; done
hello web-0
hello web-1
```

接下来我们删除这两个Pod，看看这些Volume的文件会不会丢失

```
$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted
```

在被删除之后，这两个Pod会按照编号的顺序被重新创建出来。而这时候，我们在新创建的容器里通过访问“http://localhost”的方式去访问web-0里的nginx服务,就会发现，这个请求依然返回：hello web-0。也就是说，原先与名叫web-0的Pod绑定的PV，在这个Pod被重新创建之后，依然同新的名叫web-0的Pod绑定在一起。对于web-1来说，也是一样。

#### 动态申请PV卷

https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client/deploy

PV 支持 Static 静态请求，即提前准备好固定大小的资源。但是每次都需要管理员手动去创建对应的 PV资源，确实不方便。还好 PV 同时支持 Dynamic 动态请求，k8s 提供了 provisioner 来动态创建 PV，不仅大大节省了时间，而且还可以根据不同的 StorageClasses 封装不同类型的存储供 PVC 使用。接下来，我们演示下如何配置 NFS 类型的 StorageClasses 来动态创建 PV。

这里要说一下，k8s 默认内部 [provisioner](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner) 支持列表中，是不支持 NFS 的，如果我们要使用该 provisioner 该怎么办呢？方案就是使用外部 provisioner，这里可参照 [kubernetes-incubator/external-storage](https://github.com/kubernetes-incubator/external-storage) 这个来创建，文章中演示类似，参照这个项目，可以提供外部 provisioner 来支持动态创建 PV 的功能。

```
$ vim deployment.yaml 
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nfs-client-provisioner
  labels:
    app: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app: nfs-client-provisioner
  template:
    metadata:
      labels:
        app: nfs-client-provisioner
    spec:
      serviceAccountName: nfs-client-provisioner
      containers:
        - name: nfs-client-provisioner
          image: quay.io/external_storage/nfs-client-provisioner:latest
          volumeMounts:
            - name: nfs-client-root
              mountPath: /persistentvolumes
          env:
            - name: PROVISIONER_NAME
              value: fuseim.pri/ifs
            - name: NFS_SERVER
              value: 11.88.191.61
            - name: NFS_PATH
              value: /nfs/data
      volumes:
        - name: nfs-client-root
          nfs:
            server: 11.88.191.61
            path: /nfs/data

```

创建storageclass

```
$vim class.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: managed-nfs-storage
provisioner: fuseim.pri/ifs # or choose another name, must match deployment's env PROVISIONER_NAME'
parameters:
  archiveOnDelete: "false"
```

查看创建的storageclass

```
$kubectl get storageclass
NAME                  PROVISIONER      RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
managed-nfs-storage   fuseim.pri/ifs   Delete          Immediate           false                  66s
```

测试

```
$vim statefulset2.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  serviceName: "nginx"
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.9.1
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
      annotations:
    volume.beta.kubernetes.io/storage-class: "managed-nfs-storage"
    spec:
      #storageClassName: nfs
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 1Gi
```

创建statefulset后，发现pvc有如下报错：

```
waiting for a volume to be created, either by external provisioner "fuseim.pri/ifs" or manually created by system administrator
```

看起来provisioner并没有创建出来PV

因为kuberentes使用了基于角色的访问控制（RBAC），因为我们必须执行如下命令授权provisioner

```
$vim rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: nfs-client-provisioner-runner
rules:
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list", "watch", "create", "delete"]
  - apiGroups: [""]
    resources: ["persistentvolumeclaims"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "update", "patch"]
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: run-nfs-client-provisioner
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: ClusterRole
  name: nfs-client-provisioner-runner
  apiGroup: rbac.authorization.k8s.io
---
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
rules:
  - apiGroups: [""]
    resources: ["endpoints"]
    verbs: ["get", "list", "watch", "create", "update", "patch"]
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: leader-locking-nfs-client-provisioner
  # replace with namespace where provisioner is deployed
  namespace: default
subjects:
  - kind: ServiceAccount
    name: nfs-client-provisioner
    # replace with namespace where provisioner is deployed
    namespace: default
roleRef:
  kind: Role
  name: leader-locking-nfs-client-provisioner
  apiGroup: rbac.authorization.k8s.io
```

可以看到 pod、pvc、pvc都正常了

```
$kubectl get pod
NAME                                      READY   STATUS      RESTARTS   AGE
nfs-client-provisioner-6b8d55dfc9-4jktd   1/1     Running     0          25m
recycler-for-nfs-pv002                    0/1     Completed   0          73m
web-0                                     1/1     Running     0          14m
web-1                                     1/1     Running     0          15s
$kubectl get pvc
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE
www-web-0   Bound    pvc-a2034dcc-ee16-45f0-9133-2d1d9fa12ca7   1Gi        RWO            managed-nfs-storage   15m
www-web-1   Bound    pvc-b9bbf36f-36f7-4792-9fc7-d9c254d95578   1Gi        RWO            managed-nfs-storage   49s
$kubectl get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS          REASON   AGE
pvc-a2034dcc-ee16-45f0-9133-2d1d9fa12ca7   1Gi        RWO            Delete           Bound    default/www-web-0   managed-nfs-storage            73s
pvc-b9bbf36f-36f7-4792-9fc7-d9c254d95578   1Gi        RWO            Delete           Bound    default/www-web-1   managed-nfs-storage            61s
```

nfs的服务器上，也创建自动创建了对应的目录

```
$ll
total 16
drwxrwxrwx 2 root root 4096 May  2 16:41 default-www-web-0-pvc-a2034dcc-ee16-45f0-9133-2d1d9fa12ca7
drwxrwxrwx 2 root root 4096 May  2 16:42 default-www-web-1-pvc-b9bbf36f-36f7-4792-9fc7-d9c254d95578
```

我们可以使用如下所示的命令，在Pod的Volume目录里写入一个文件，来验证一下上述Volume的分配情况：

```
$ for i in 0 1; do kubectl exec web-$i -- sh -c 'echo hello $(hostname) > /usr/share/nginx/html/index.html'; done
```

如上所示，通过kubectl exec指令，我们在每个Pod的Volume目录里，写入了一个index.html文件。这个文件的内容，正是Pod的hostname。比如，我们在web-0的index.html里写入的内容就是"hello web-0"。此时，如果你在这个Pod容器里访问“http://localhost”，你实际访问到的就是Pod里Nginx服务器进程，而它会为你返回/usr/share/nginx/html/index.html里的内容。操作方法如下：

```
$ for i in 0 1; do kubectl get pod web-$i -o wide -o json | jq -j '.status.podIP' | xargs echo | xargs curl  ; done
hello web-0
hello web-1
```

接下来我们删除这两个Pod，看看这些Volume的文件会不会丢失

```
$ kubectl delete pod -l app=nginx
pod "web-0" deleted
pod "web-1" deleted
```

在被删除之后，这两个Pod会按照编号的顺序被重新创建出来。而这时候，我们在新创建的容器里通过访问“http://localhost”的方式去访问web-0里的nginx服务：

```
# 在被重新创建出来的Pod容器里访问http://localhost
$for i in 0 1; do kubectl get pod web-$i -o wide -o json | jq -j '.status.podIP' | xargs echo | xargs curl  ; done
```

就会发现，这个请求依然返回：hello web-0。也就是说，原先与名叫web-0的Pod绑定的PV，在这个Pod被重新创建之后，依然同新的名叫web-0的Pod绑定在一起。对于web-1来说，也是一样。



## 20 深入理解StatefulSet（三）： 有状态应用实践



### 20.01 “容器化”高可用Mysql的“三座大山”

今天选择的实例是部署一个MySQL集群，这也是Kubernetes官方文档里的一个经典案例。首先，用自然语言来描述一下我们想要部署的“有状态应用”。
1：是一个“主从复制”（Master-Slave Replication）的MySQL集群；
2：有1个主节点（Master）
3：有多个从节点（Slave）
4：从节点需要能水平扩展
5:  所有的写操作，只能在主节点上执行 
6:  读操作可以在所有节点上执行

在常规环境里，部署这样一个主从模式的MySQL集群的主要难点在于：如何让从节点能够拥有主节点的数据，即：如何配置主（Master）从（ Slave）节点的复制与同步。
所以，在安装好MySQL的Master节点之后。
第一步：就是通过XtraBackup将Master节点的数据备份到指定目录。XtraBackup是业界主要是用的开源MySQL备份和恢`复工具。 工具会自动在目标目录里生成一个备份信息文件，这个文件一般会包含如下两个信息：

```
$ cat xtrabackup_binlog_info
TheMaster-bin.000001     481
```

第二步：配置Slave节点。Slave节点在第一次启动前，需要把Master节点的备份数据，连通备份信息文件，一起拷贝到自己的数据目录（/var/lib/mysql）下。然后，我们执行这样一句SQL:

```
TheSlave|mysql> CHANGE MASTER TO
                MASTER_HOST='$masterip',
                MASTER_USER='xxx',
                MASTER_PASSWORD='xxx',
                MASTER_LOG_FILE='TheMaster-bin.000001',
                MASTER_LOG_POS=481;
```

其中，MASTER_LOG_FILE和MASTER_LOG_POS，就是该备份对应的二进制日志（Binary Log）文件的名称和开始的位置（偏移量），也正是xtrabackup_binlog_info文件里的那两部分内容（即：TheMaster-bin.000001和481）
第三步：启动Slave节点。在这一步，我们需要执行这样这句SQL:

```
TheSlave|mysql> START SLAVE;
```

这样，Slave节点就启动了。它会使用备份信息中的二进制日志文件和偏移量，与主节点进行数据同步。
第四步：在集群中添加更多的Slave节点

通过上面的叙述，我们不难看到，将部署MySQL集群的流程迁移到Kubernetes项目上，需要能够“容器化”地解决下面的“三座大山”：

1: Master节点和Slave节点需要有不同的配置文件（即：不同的my.conf）
2: Master节点和Slave节点需要能够传输备份文件信息
3: 在Slave节点第一次启动之前，需要执行一些初始化SQL操作

### 20.02 Master节点和Slave节点需要有不同的配置文件

“第一座大山：Master节点和Slave节点需要有不同的配置文件”，我们只需要给主从节点分别准备两份不同的MYSQL配置文件，然后根据Pod的序号（Index）挂载进去即可。

这样的配置文件信息，应该保存在ConfigMap里供Pod使用。它的定义如下所示：

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: mysql
  labels:
    app: mysql
data:
  master.cnf: |
    # 主节点MySQL的配置文件
    [mysqld]
    log-bin
  slave.cnf: |
    # 从节点MySQL的配置文件
    [mysqld]
    super-read-only
```

这里，我们定义了一个master.cnf和slave.cnf两个MySQL的配置文件：
master.cnf开启了log-bin，即：使用二进制日志文件的方式进行主从复置，这是一个标准的设置
slave.cnf开启了super-read-only，即：它对用户是只读的

而上述ConfigMap定义里的data部分，是Key-Value格式的。比如，master.conf就是这份配置的Key，而"|"后面的内容，就是这份配置的Value。这份数据将来挂载进Master节点对应的Pod后，就会在Volume目录里生成一个叫做master.conf的文件。

后续我们可以这样使用这个Config Map，通过一个initContainer来判断Pod是Master还是Slave，如果是Master就获取ConfigMap里的master.cnf配置，如果是Slave，就获取Config Map里的slave.cnf配置，如下：

```
...
      # template.spec
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          .....
          # 如果Pod序号是0，说明它是Master节点，从ConfigMap里把Master的配置文件拷贝到/mnt/conf.d/目录；
          # 否则，拷贝Slave的配置文件
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
```

可以看到，我们的配置文件都是从一个名叫config-map的valume中获取的，再看一下这个volume的配置：

```
  ...
      # template.spec
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
```

这个名叫config-map的volume，使用的是名叫mysql的Config Map，因此就映射到了我们之前创建的那个名叫mysql的ConfigMap对象。

接下来，我们需要创建两个Service来供StatefulSet以及用户使用，这两个Service的定义如下所示：

```
apiVersion: v1
kind: Service
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  clusterIP: None
  selector:
    app: mysql
---
apiVersion: v1
kind: Service
metadata:
  name: mysql-read
  labels:
    app: mysql
spec:
  ports:
  - name: mysql
    port: 3306
  selector:
    app: mysql
```

 可以看到，这两个service都代理了所有携带app=mysql标签的Pod，也就是所有的MySQL Pod。端口映射都是用Service的3306端口对应Pod的3306端口。不同的是，第一个名叫“mysql”的Service是一个Headless Servcie。所以它的作用，是通过为Pod分配DNS记录来固定它的拓扑状态，比如“mysql-0.mysql”和"mysql-1.mysql"这样的DNS名字。其中，编号为0的节点就是我们的主节点。而第二个名叫“mysql-1.mysql”的Service，则是一个常规的Service。

### 20.03 Master节点和Slave节点需要能够传输备份文件

“第二座大山：Master节点和Slave节点需要能够传输备份文件”，比较推荐的做法是：先搭建框架，再完善细节。其中，Pod部分如何定义，是完善细节时的重点。

所以首先，我们先为StatefulSet对象规划一个大致的框架，如下图所示：

![image-20200427092512186](/Users/canghong/Library/Application Support/typora-user-images/image-20200427092512186.png)

在这一步，我们可以先为StatefulSet定义一些通用的字段。比如：selector表示，这个StatefulSet要管理的Pod必须携带app=mysql标签；它声明要使用的Headless Service的名字是：mysql。这个StatefulSet的replicas值是3，表示它定义的MySQL集群有三个节点： 一个Master节点，两个Slave节点。可以看到，StatefulSet管理的“有状态应用”的多个实例，也都是通过同一份Pod模版创建出来的额，使用的是同一个Docker镜像。这也意味着：如果你的应用要求不同节点的镜像不一样，那就不能再使用StatefulSet了。对于这种情况，应该考虑后面讲解到的Operator。

除了这些基本的字段外，作为一个有存储状态的MySQL集群，StatefulSet还需要管理存储状态。所以，我们需要通过volumeCliamTemplate（PVC）模版的resources.requests.storage指定了存储的大小为10GiB；ReadWriteOnce指定了该存储的属性为可读，并且一个PV只允许挂载在一个宿主机上。将来，这个PV对应的Volume就回充当MySQL Pod的存储数据目录。

然后，我们重点设计一下这个StatefulSet的Pod模版，也就是template字段。由于StatefulSet管理的Pod都来自同一个镜像，这就要求我们在编写Pod时，一定要保持清醒：
1: 如果这个pod是Master节点，我们需要怎么做；
2: 如果这个pod是Slave节点，我们需要怎么做；

第一步，从ConfigMap中，获取MySQL的Pod对应的配置文件：为此，我们需要进行一个初始化操作，根据节点的角色是Master还是Slave节点，为Pod分配对应的配置文件。此外，MySQL还要求集群里的每个节点都有一个唯一的ID文件，名叫server-id.cnf。这些初始化操作显然通过InitContainer来完成比较合适，所以我们定义了一个InitContainer：

```
 ...
      # template.spec
      initContainers:
      - name: init-mysql
        image: mysql:5.7
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 从Pod的序号，生成server-id
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          echo [mysqld] > /mnt/conf.d/server-id.cnf
          # 由于server-id=0有特殊含义，我们给ID加一个100来避开它
          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf
          # 如果Pod序号是0，说明它是Master节点，从ConfigMap里把Master的配置文件拷贝到/mnt/conf.d/目录；
          # 否则，拷贝Slave的配置文件
          if [[ $ordinal -eq 0 ]]; then
            cp /mnt/config-map/master.cnf /mnt/conf.d/
          else
            cp /mnt/config-map/slave.cnf /mnt/conf.d/
          fi
        volumeMounts:
        - name: conf
          mountPath: /mnt/conf.d
        - name: config-map
          mountPath: /mnt/config-map
```

在这个名叫init-mysql的InitContainer的配置，它从Pod的hostname里，读取到了Pod的序号，以此作为MySQL节点里的server-id。然后，init-mysql通过这个序号，判断当前Pod到底是Master节点（即：序号为0）还是Slave节点（即：序号部位0），从而把对应的配置文件从/mnt/config-map目录拷贝到/mnt/conf.d目录下。其中，文件拷贝的源目录/mnt/config-map，正是ConfigMap在这个Pod的Volume，如下所示：

```
 ...
      # template.spec
      volumes:
      - name: conf
        emptyDir: {}
      - name: config-map
        configMap:
          name: mysql
```

通过这个定义，init-mysql在声明挂载config-map这个Volume之后，ConfigMap里保存的内容，就会以文件的方式出现在它的/mnt/config-map目录当中。而文件拷贝的目标目录，即容器里的/mnt/conf.d目录，对应的则是是一个名叫conf的，emptyDir类型的Volume。基于Pod  Volume共享的原理，当InitContainer复制完配置文件退出后，后面启动的MySQL容器只需要直接声明挂载这个名叫conf的Volume，它所需要的.cnf配置文件已经出现在这里面了。

第二步：在Slave Pod启动前，从Master或者其他Slave Pod里拷贝数据库数据到自己的目录下。为了实现这个操作，我们需要再定义第二个InitContainer，如下所示：

```
...
      # template.spec.initContainers
      - name: clone-mysql
        image: gcr.io/google-samples/xtrabackup:1.0
        command:
        - bash
        - "-c"
        - |
          set -ex
          # 拷贝操作只需要在第一次启动时进行，所以如果数据已经存在，跳过
          [[ -d /var/lib/mysql/mysql ]] && exit 0
          # Master节点(序号为0)不需要做这个操作
          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1
          ordinal=${BASH_REMATCH[1]}
          [[ $ordinal -eq 0 ]] && exit 0
          # 使用ncat指令，远程地从前一个节点拷贝数据到本地
          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql
          # 执行--prepare，这样拷贝来的数据就可以用作恢复了
          xtrabackup --prepare --target-dir=/var/lib/mysql
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
```

这个名叫clone-mysql的InitContainer里，使用的是xtrabackup镜像（它里面安装了xtrabackup工具）。而在它们的启动命令里，我们首先做了一个判断。即：当初始化所需的数据（/var/lib/mysql 目录）已经存在，或者当前Pod是Master节点的时候，不需要做拷贝操作。

接下来，clone-mysql会使用Linux自带的ncat指令，向DNS记录为“mysql-(当前序号减一).mysql”的Pod，也就是当前Pod的前一个Pod，发起数据传输请求，并且直接用xbstream指令将收到的备份数据保存在/var/lib/mysql目录下。（备注：3307是一个特殊端口，运行着一个专门负责备份MySQL数据的辅助进程。）。你可能已经注意到，这个容器里的/var/lib/mysql目录，实际上正式一个名为data的PVC，即：我们在前面声明的持久化存储。

这就可以保证，哪怕宿主机宕机了，我们数据库的数据也不会丢失。更重要的是，由于Pod Volume是被Pod里的容器共享的，所以后面启动的MySQL容器，就可以把Volume挂载到自己的/var/lib/mysql目录下，直接使用里面的备份数据进行恢复操作。不过，clone-mysql容器还是要对/var/lib/mysql目录，执行一句 xtrabackup --prepare操作，目的是让拷贝来的数据进入一致性状态，这样，这些数据才能被用作数据恢复。

至此，我们就通过InitContainer完成了对“主、从节点间备份文件传输”操作的处理过程，也就是翻越了“第二座大山”。



### 20.04 如何在Slave节点的MySQL容器第一次启动之前，进行初始化SQL

接下来，我们可以开始定义MySQL容器，启动MySQL服务了。由于StatefulSet里的所有Pod都来自同一个Pod模版，所以我们还要“人格分裂”地去思考：这个MySQL容器的启动命令，在Master和Slave两种情况下有什么不同。有了Docker镜像，在Pod里声明一个Master角色的MySQL容器并不是什么困难的事情：直接执行MySQL的启动命令即可。但是，如果这个Pod是一个第一次启动的Slave节点，在执行MySQL启动命令之前，它就需要使用前面InitContainer拷贝来的备份数据进行初始化。可是，别忘了，容器是一个单进程模型。

所以，一个Slave角色的MySQL容器启动之前，谁能负责给它执行初始化的SQL语句呢？

我们可以为这个MySQL容器额外定义一个sidecar容器，来完成这个操作，它的定义如下所示：

```
...
      # template.spec.containers
      - name: xtrabackup
        image: gcr.io/google-samples/xtrabackup:1.0
        ports:
        - name: xtrabackup
          containerPort: 3307
        command:
        - bash
        - "-c"
        - |
          set -ex
          cd /var/lib/mysql
          
          # 从备份信息文件里读取MASTER_LOG_FILEM和MASTER_LOG_POS这两个字段的值，用来拼装集群初始化SQL
          if [[ -f xtrabackup_slave_info ]]; then
            # 如果xtrabackup_slave_info文件存在，说明这个备份数据来自于另一个Slave节点。这种情况下，XtraBackup工具在备份的时候，就已经在这个文件里自动生成了"CHANGE MASTER TO" SQL语句。所以，我们只需要把这个文件重命名为change_master_to.sql.in，后面直接使用即可
            mv xtrabackup_slave_info change_master_to.sql.in
            # 所以，也就用不着xtrabackup_binlog_info了
            rm -f xtrabackup_binlog_info
          elif [[ -f xtrabackup_binlog_info ]]; then
            # 如果只存在xtrabackup_binlog_inf文件，那说明备份来自于Master节点，我们就需要解析这个备份信息文件，读取所需的两个字段的值
            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1
            rm xtrabackup_binlog_info
            # 把两个字段的值拼装成SQL，写入change_master_to.sql.in文件
            echo "CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\
                  MASTER_LOG_POS=${BASH_REMATCH[2]}" > change_master_to.sql.in
          fi
          
          # 如果change_master_to.sql.in，就意味着需要做集群初始化工作
          if [[ -f change_master_to.sql.in ]]; then
            # 但一定要先等MySQL容器启动之后才能进行下一步连接MySQL的操作
            echo "Waiting for mysqld to be ready (accepting connections)"
            until mysql -h 127.0.0.1 -e "SELECT 1"; do sleep 1; done
            
            echo "Initializing replication from clone position"
            # 将文件change_master_to.sql.in改个名字，防止这个Container重启的时候，因为又找到了change_master_to.sql.in，从而重复执行一遍这个初始化流程
            mv change_master_to.sql.in change_master_to.sql.orig
            # 使用change_master_to.sql.orig的内容，也是就是前面拼装的SQL，组成一个完整的初始化和启动Slave的SQL语句
            mysql -h 127.0.0.1 <<EOF
          $(<change_master_to.sql.orig),
            MASTER_HOST='mysql-0.mysql',
            MASTER_USER='root',
            MASTER_PASSWORD='',
            MASTER_CONNECT_RETRY=10;
          START SLAVE;
          EOF
          fi
          
          # 使用ncat监听3307端口。它的作用是，在收到传输请求的时候，直接执行"xtrabackup --backup"命令，备份MySQL的数据并发送给请求者
          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \
            "xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root"
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
```

可以看到，在这个名叫xtrabackup的sidecar容器的启动命令里，其实实现了两部分工作。第一部分工作，当然是MySQL节点上的初始化工作。这个初始化工作需要使用的SQL，是sidercar容器拼装出来、保存在一个名为change_master_to.sql.in 的文件里，具体过程如下所示：

sidecar容器首先会判断当前Pod的/var/lib/mysql目录下，是否有xtrabackup_slave_info这个备份文件信息。

如果有： 说明这个目录下的备份数据是由一个Slave节点生成的。这种情况下，XtraBackup工具在备份的时候，就已经在这个文件里自动生成了"CHANGE MASTER TO"SQL语句。所以，我们只需要把这个文件重命名为change_master_to.sql.in，后面直接使用即可
如果没有：但是存在xtrlabackup_slave_info文件、那说明备份数据来源于Master节点。这种情况下，sidecar容器就需要解析这个备份信息文件，读取MASTER_LOG_FILE和MASTER_LOG_POS这两个字段的值，用它们拼装出初始化SQL语句，然后把这个SQL写入到change_master_to.sql.in文件中。

接下来，sidecar容器就可以执行初始化操作了。从上面的叙述中可以看到，只要这个change_master_to.sql.in文件存在，那就说明接下来需要进行集群初始化操作。这时候，sidecar容器值需要读取并执行change_master_to.sql.in里面的“CHANGE MASTER TO ”指令，再执行一句START SLAVE命令，一个Slave节点就被成功启动了。需要注意的是：Pod里的容器并没有先后顺序，所以在执行初始化SQL之前，必须先执行一句SQL（select 1）来检查一下MySQL服务是否已经可用。

当然，上述这些初始化操作完成后，我们还要删除掉前面用到的这些备份信息文件。否则，下次这个容器重启时，就会发现这些文件存在，所以又会重新执行一次数据恢复和集群初始化的操作。

在完成MySQL节点的初始化后，这个sidecar容器的第二个工作，则是启动一个数据传输服务。具体做法是：sidecar容器会使用ncat命令启动一个工作在3307端口上的网络发送服务。一旦收到数据传输请求时，sidecar容器就会调用xtrabackup --backup指令备份当前MySQL的xtrabackup --backup指令备份当前MySQL的数据，然后把这些备份数据返回给请求者。这就是为什么我们在InitContainer里定义数据拷贝的时候，访问的是“上一个MySQL节点”的3307端口。

值得一提的是，由于sidecar容器和MySQL容器同处于一个Pod里，所以它是直接通过Localhost来访问和备份MySQL容器里的数据的，非常方便。同样地，这里举例的只是一种备份方法而已，你完全可以选择其他自己喜欢的方案。比如，你可以使用innobackupex命令做数据备份和准备，它的使用方法几乎和本文的备份方法一样。

至此，我们也就翻越了“第三座大山”，完成了Slave节点第一次启动前的初始化工作。

### 20.05 初始化MySQL容器 

扳倒了这“三座大山”后，我们终于可以定义Pod里的主角，MySQL容器了。有了前面这些定义和初始化工作，MySQL容器本身的定义就非常简单了，如下所示：  

```
...
      # template.spec
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ALLOW_EMPTY_PASSWORD
          value: "1"
        ports:
        - name: mysql
          containerPort: 3306
        volumeMounts:
        - name: data
          mountPath: /var/lib/mysql
          subPath: mysql
        - name: conf
          mountPath: /etc/mysql/conf.d
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
        livenessProbe:
          exec:
            command: ["mysqladmin", "ping"]
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
        readinessProbe:
          exec:
            # 通过TCP连接的方式进行健康检查
            command: ["mysql", "-h", "127.0.0.1", "-e", "SELECT 1"]
          initialDelaySeconds: 5
          periodSeconds: 2
          timeoutSeconds: 1
```

在这个容器的定义里，我们使用了一个标准的MySQL 5.7的官方镜像。它的数据目录是/var/lib/mysql，配置文件是/etc/mysql/conf.d。这时候，你应该能够明白，如果MySQL容器是Slave节点的话，它的数据目录里的数据，就来自于InitContainer从其他节点里拷贝而来的备份。它的配置文件目录 /etc/mysql/conf.d里的内容，则来自于ConfigMap对应的Volume。而它的初始化工作，则是由同一个Pod里的sidecar容器完成。这些操作，正式我们刚刚为你讲述的大部分内容。

另外，我们定义了一个livenessProbe，通过mysqladmin ping命令来检查它是否健康；还定义了一个readinessProbe，通过查询SQL（select 1）来检查MySQL服务是否可用。当然，凡是readinessProbe检查失败的MySQL，都会从Service里被摘除。

至此，一个主从复置模式的MySQL集群就定义完了。

现在，我们就可以使用kubectl命令，尝试运行一下这个StatefulSet了。首先，我们需要在Kubernetes集群里创建满足条件的PV，如果是使用rook-ceph这个持久化存储的话，可以按如下方式使用存储插件Rook：

````
$ kubectl create -f rook-storage.yaml
$ cat rook-storage.yaml
apiVersion: ceph.rook.io/v1beta1
kind: Pool
metadata:
  name: replicapool
  namespace: rook-ceph
spec:
  replicated:
    size: 3
---
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: rook-ceph-block
provisioner: ceph.rook.io/block
parameters:
  pool: replicapool
  clusterNamespace: rook-ceph
````

在这里，我用到了StorageClass来完成这个操作。它的作业，是自动地为集群里存在的每一个PVC，调用存储插件（Rook）创建对应的PV，从而省去了我们手动创建PV的机械劳动。后续会详细介绍这个机制。（在使用Rook的情况下，mysql-statefulset.yaml里的volumeCliamTempaltes字段需要加上声明storageClassName=rook-ceph-block，才能使用到这个Rook提供的持久化存储）。

然后我们就可以创建这个StatefulSet了，如下所示：

```
$ kubectl create -f mysql-statefulset.yaml
$ kubectl get pod -l app=mysql
NAME      READY     STATUS    RESTARTS   AGE
mysql-0   2/2       Running   0          2m
mysql-1   2/2       Running   0          1m
mysql-2   2/2       Running   0          1m
```

可以看到，StatefulSet启动成功后，会有三个Pod。接下来，我们可以尝试向这个MySQL集群发起请求，执行一些SQL操作来验证它是否正常：

```
$ kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\
  mysql -h mysql-0.mysql <<EOF
CREATE DATABASE test;
CREATE TABLE test.messages (message VARCHAR(250));
INSERT INTO test.messages VALUES ('hello');
EOF

```

如上所示，我们通过启动一个容器，使用MySQL client执行了创建数据库和表、以及插入数据的操作。需要注意的是，我们连接的MySQL的地址必须是mysql-0.mysql（即：Master节点的DNS记录）。因为，只有Master节点才能处理写操作。而通过连接mysql-read这个Service，我们就可以用SQL进行读操作，如下所示：

```
$ kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
 mysql -h mysql-read -e "SELECT * FROM test.messages"
Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
```

在有StatefulSet以后，就可以像Deployment那样，非常方便地扩展这个MySQL集群，比如：

```
$ kubectl scale statefulset mysql  --replicas=5
```

这时候，会发现新的Slave Pod mysql-3和mysql-4被自动创建了出来。而如果你像如下所示的这样，直接连接mysql-3.mysql，即mysql-3这个Pod的DNS名字来进行查询操作:

```
$ kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\
  mysql -h mysql-3.mysql -e "SELECT * FROM test.messages"
Waiting for pod default/mysql-client to be running, status is Pending, pod ready: false
+---------+
| message |
+---------+
| hello   |
+---------+
pod "mysql-client" deleted
```

就会看到，从StatefulSet为我们新创建的mysql-3上，同样可以读取到之前插入的记录，也就是说，我们的数据备份和恢复，都是有效的。

完整的yaml文件可见： https://github.com/oracle/kubernetes-website/blob/master/docs/tasks/run-application/mysql-statefulset.yaml




























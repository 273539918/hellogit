# 容器编排与Kubernetes作业管理

## 20 容器守护进程的意义：DaemonSet

DaemonSet的主要作用，是让你在Kubernetes集群里，运行一个Daemon Pod。所以，这个Pod有如下三个特征：
1：这个Pod运行在Kubernetes集群里的每一个（Node）上；
2：每个节点上只有一个这样的Pod实例
3：当有新的节点加入Kubernetes集群后，该Pod会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的Pod也会相应地被回收掉。

如下，是一个DaemonSet的API对象：

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: k8s.gcr.io/fluentd-elasticsearch:1.20
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

这个DaemonSet，管理的是一个fluentd-elasticsearch镜像的Pod。这个镜像的功能非常实用：通过fluented将Docker容器里的日志转发到ElasticSearch中。

可以看到，DaemonSet跟Deployment其实非常相似，只不过是没有replicas字段；它也使用selector选择管理所有携带了name=fluentd-elasticsearch标签的Pod。

### 20.01 DaemonSet实现原理

DaemonSet是如何保证每个Node上有且只有一个被管理的Pod？

DaemonSet Controller，首先从Etcd里获取所有的Node列表，然后遍历所有的Node。这时，它就可以很容地检查，当前这个Node上是不是有一个携带了name=fluentd-elasticsearch标签的Pod在运行。而检查的结果，可能有这么三种情况：

1. 没有这种Pod，那么意味着要在这个Node上创建这样一个Pod；
2. 有这种Pod，但是数量大于1，那就说明要把多余的Pod从这个Node上删除掉；
3. 正好只有一个这种Pod，那说明这个节点是正常的。

其中，删除节点（Node）上多余的Pod非常简单，直接调用Kubernetes API就可以了。

但是，如何在指定的Node上创建新Pod呢？

如果你已经熟悉了Pod API对象的话，那一定可以立即说出答案：用nodeSelector，选择Node的名字即可。

```
nodeSelector:
    name: <Node名字>
```

不过，在Kubernetes项目里，nodeSelector其实已经是一个将要被废弃的字段了。因为，现在有一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：

```
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: metadata.name
            operator: In
            values:
            - node-geektime
```

所以，我们的DaemonSet Controller会在创建Pod的时候，自动在这个Pod的API对象里，加上这样一个nodeAffinity定义。其中，需要绑定的节点名字，正是这个当前正在遍历的这个Node。当然，DaemonSet并不需要修改用户提交的YAML文件里的Pod模版，而是在向Kubernetes发起请求之前，直接修改根据模版生成的Pod对象。此外，DaemonSet还会给这个Pod自动加上另外一个与调度相关的字段，叫做tolerations。这个字段意味着这个Pod，会“容忍”某些Node的“污点”。DaemonSet自动加上的tolerations字段，如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  name: with-toleration
spec:
  tolerations:
  - key: node.kubernetes.io/unschedulable
    operator: Exists
    effect: NoSchedule
```

这个Toleration的含义是：“容忍”所有被标记为unschedulable “污点”的Node；容忍的效果是允许调度。而在正常情况下，被标记了unschedulable“污点”的Node，是不会有任何Pod调度上去的（effect:NoSchedule）。

假如当前DaemonSet管理的，是一个网络插件的Agent Pod，那么你就必须在这个DaemonSet的YAML文件里，给它的模版加上一个能够“容忍”node.kubernetes.io/network-unavailabel“污点”的Toleration。正如下面这个例子所示：

```
...
template:
    metadata:
      labels:
        name: network-plugin-agent
    spec:
      tolerations:
      - key: node.kubernetes.io/network-unavailable
        operator: Exists
        effect: NoSchedule
```

在Kubernetes项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为node.kubernetes.io/network-unavailable的“污点”。而通过这样一个Toleration，调度器在调度这个Pod的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的Agent组件调度到这台机器上启动起来。

这种机制，正式我们在部署Kubernets集群的时候，能够先部署Kubernetes本身、再部署网络插件的根本原因：因为当时我们缩创建的Weave的YAML：因为当时我们所创建的Weave的YAML，实际上就是一个DaemonSet。

备注： 需要注意的是，在Kubernetes v1.11之前，由于调度器尚不完善，DaemonSet是由DaemonSet Controller自行调度的，即它会直接设置Pod的spec.nodename字段，这样就可以跳过调度器了。但是，这样的做法很快就会被废除，所以这里不再介绍。

### 20.02 实践DaemonSet

首先，我们先创建这个DaemonSet的对象:

```
$ kubectl create -f fluentd-elasticsearch.yaml
```

需要注意的是，在DaemonSet上，我们一般都应该加上resources字段，来限制它的CPU和内存使用，防止它占用过多的宿主机资源。

创建成功后，来查看生成的对象：

```
$ kubectl get daemonset -n kube-system fluentd-elasticsearch
NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
fluentd-elasticsearch   4         4         0       4            0           <none>          87s
```

可以看到，DaemonSet和Deployment一样，也有Desired和Current等字段。这也意味着，DaemonSet可以像Deployment那样，进行版本管理。这个版本，可以使用kubectl rollout history 看到:

```
$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system
daemonset.apps/fluentd-elasticsearch
REVISION  CHANGE-CAUSE
1         <none>
```

接下来，我们把这个DaemonSet的容器镜像版本升级到v2.2.0：

```
$ kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record -n=kube-system
```

这个kubectl set image命令里，第一个fluentd-elasticsearch是DaemonSet的名字，第二个fluent-elasticsearch是容器的名字。这时候，我们可以使用kubectl rollout status命令看到这个“滚动更新”的过程，如下所示：

```
$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 0 out of 2 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 0 out of 2 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 1 of 2 updated pods are available...
daemon set "fluentd-elasticsearch" successfully rolled out
```

注意，由于这一次我在升级命令后面加上了record参数，所以这次升级使用到的指令就会自动出现在DaemonSet的rollout history里面，如下所示：

```
$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system
daemonsets "fluentd-elasticsearch"
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --namespace=kube-system --record=true

```

有了版本号，你也就可以像Deployment一样，将DaemonSet回滚到某个指定的历史版本了。

而在我们前面的文章中讲解Deployment对象的时候，曾经提到过，Deployment管理这些版本，靠的是“一个版本对应一个ReplicaSet对象”。可是，DaemonSet控制器操作的直接就是Pod，不可以有ReplicaSet这样的对象参与其中。那么，它的这些版本又是如何维护的呢？

所谓一切皆对象，Kubernetes v1.7之后添加了一个API对象，名叫ControllerRevision，专门用来记录各种Controller对象的版本。比如，你可以通过如下命令查看fluentd-elasticsearch对应的ControllerRevision：

```
$ kubectl get controllerrevision -n kube-system  -l name=fluentd-elasticsearch
NAME                               CONTROLLER                             REVISION   AGE
fluentd-elasticsearch-6c85f7f4d6   daemonset.apps/fluentd-elasticsearch   2          128m
fluentd-elasticsearch-7f875bbfb5   daemonset.apps/fluentd-elasticsearch   1          6h9m

```

而如果你用kubectl describe查看这个ControllerRevision对象： 

```
$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 -n kube-system
....

Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Creation Timestamp:  <nil>
        Labels:
          Name:  fluentd-elasticsearch
      Spec:
        Containers:
          Image:              k8s.gcr.io/fluentd-elasticsearch:v2.2.0
          Image Pull Policy:  IfNotPresent
          Name:               fluentd-elasticsearch
...
Revision:                  2
Events:                    <none>
```

就会看到，这个ControllerRevision对象，实际上是在Data字段保存了该版本对应的完成的DaemonSet的API对象，并且，在Annotation字段保存了创建这个对象所使用的kubectl命令。

接下来，我们就可以尝试将这个DaemonSet回滚到Revision=1时的状态：

```
$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision=1 -n kube-system
daemonset.extensions/fluentd-elasticsearch rolled back
```

这个kubectl rollout undo操作，实际上相当于读取到了Revision=1的ControllerRevision对象保存的Data字段。而这个Data字段里保存的信息，就是Revision=1时这个DaemonSet的完整API对象。所以，现在DaemonSet Controller就可以使用这个历史API对象，对现有的DaemonSet做一次PATCH操作（等价于执行一次kubectl apply -f "旧的DaemonSet对象"），从而把这个DaemonSet“更新”到一个旧版本。这也是为什么，在执行完这次回滚之后，你就发现，DaemonSet的Revision并不会从Revison=2退回到1，而是会增加成Revision=3。这是因为，一个新的ControllerRevision被创建了出来。
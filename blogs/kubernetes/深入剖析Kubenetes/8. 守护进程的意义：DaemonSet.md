# 容器编排与Kubernetes作业管理



## 学习总结

DaemonSet在每个Node上启一个Pod实例，随着Node的增删来起停对应的Pod。

Deployment、StatefuleSet、DaemonSet三者的对比：

| 类别         | Deployment   | StatefulSet | DaemonSet  |
| ------------ | ------------ | ------------ | ------------ |
| 数量 | Replicas | Replicas | Node数量 |
| 版本 | Replicates | ControllerVevision | ControllerVevision |

在 Kubernetes 项目里，ControllerRevision 是一个通用的版本管理对象。这样，Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题。





## 20 容器守护进程的意义：DaemonSet

DaemonSet的主要作用，是让你在Kubernetes集群里，运行一个Daemon Pod。所以，这个Pod有如下三个特征：
1：这个Pod运行在Kubernetes集群里的每一个（Node）上；
2：每个节点上只有一个这样的Pod实例
3：当有新的节点加入Kubernetes集群后，该Pod会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的Pod也会相应地被回收掉。

如下，是一个DaemonSet的API对象：

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: k8s.gcr.io/fluentd-elasticsearch:1.20
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

这个DaemonSet，管理的是一个fluentd-elasticsearch镜像的Pod。这个镜像的功能非常实用：通过fluented将Docker容器里的日志转发到ElasticSearch中。

可以看到，DaemonSet跟Deployment其实非常相似，只不过是没有replicas字段；它也使用selector选择管理所有携带了name=fluentd-elasticsearch标签的Pod。

### 20.01 DaemonSet实现原理

DaemonSet是如何保证每个Node上有且只有一个被管理的Pod？

DaemonSet Controller，首先从Etcd里获取所有的Node列表，然后遍历所有的Node。这时，它就可以很容地检查，当前这个Node上是不是有一个携带了name=fluentd-elasticsearch标签的Pod在运行。而检查的结果，可能有这么三种情况：

1. 没有这种Pod，那么意味着要在这个Node上创建这样一个Pod；
2. 有这种Pod，但是数量大于1，那就说明要把多余的Pod从这个Node上删除掉；
3. 正好只有一个这种Pod，那说明这个节点是正常的。

其中，删除节点（Node）上多余的Pod非常简单，直接调用Kubernetes API就可以了。

但是，如何在指定的Node上创建新Pod呢？

如果你已经熟悉了Pod API对象的话，那一定可以立即说出答案：用nodeSelector，选择Node的名字即可。

```
nodeSelector:
    name: <Node名字>
```

不过，在Kubernetes项目里，nodeSelector其实已经是一个将要被废弃的字段了。因为，现在有一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：

```
apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: metadata.name
            operator: In
            values:
            - node-geektime
```

所以，我们的DaemonSet Controller会在创建Pod的时候，自动在这个Pod的API对象里，加上这样一个nodeAffinity定义。其中，需要绑定的节点名字，正是这个当前正在遍历的这个Node。当然，DaemonSet并不需要修改用户提交的YAML文件里的Pod模版，而是在向Kubernetes发起请求之前，直接修改根据模版生成的Pod对象。此外，DaemonSet还会给这个Pod自动加上另外一个与调度相关的字段，叫做tolerations。这个字段意味着这个Pod，会“容忍”某些Node的“污点”。DaemonSet自动加上的tolerations字段，如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  name: with-toleration
spec:
  tolerations:
  - key: node.kubernetes.io/unschedulable
    operator: Exists
    effect: NoSchedule
```

这个Toleration的含义是：“容忍”所有被标记为unschedulable “污点”的Node；容忍的效果是允许调度。而在正常情况下，被标记了unschedulable“污点”的Node，是不会有任何Pod调度上去的（effect:NoSchedule）。

假如当前DaemonSet管理的，是一个网络插件的Agent Pod，那么你就必须在这个DaemonSet的YAML文件里，给它的模版加上一个能够“容忍”node.kubernetes.io/network-unavailabel“污点”的Toleration。正如下面这个例子所示：

```
...
template:
    metadata:
      labels:
        name: network-plugin-agent
    spec:
      tolerations:
      - key: node.kubernetes.io/network-unavailable
        operator: Exists
        effect: NoSchedule
```

在Kubernetes项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为node.kubernetes.io/network-unavailable的“污点”。而通过这样一个Toleration，调度器在调度这个Pod的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的Agent组件调度到这台机器上启动起来。

这种机制，正式我们在部署Kubernets集群的时候，能够先部署Kubernetes本身、再部署网络插件的根本原因：因为当时我们缩创建的Weave的YAML：因为当时我们所创建的Weave的YAML，实际上就是一个DaemonSet。

备注： 需要注意的是，在Kubernetes v1.11之前，由于调度器尚不完善，DaemonSet是由DaemonSet Controller自行调度的，即它会直接设置Pod的spec.nodename字段，这样就可以跳过调度器了。但是，这样的做法很快就会被废除，所以这里不再介绍。

### 20.02 实践DaemonSet

首先，我们先创建这个DaemonSet的对象:

```
$ kubectl create -f fluentd-elasticsearch.yaml
```

需要注意的是，在DaemonSet上，我们一般都应该加上resources字段，来限制它的CPU和内存使用，防止它占用过多的宿主机资源。

创建成功后，来查看生成的对象：

```
$ kubectl get daemonset -n kube-system fluentd-elasticsearch
NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
fluentd-elasticsearch   4         4         0       4            0           <none>          87s
```

可以看到，DaemonSet和Deployment一样，也有Desired和Current等字段。这也意味着，DaemonSet可以像Deployment那样，进行版本管理。这个版本，可以使用kubectl rollout history 看到:

```
$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system
daemonset.apps/fluentd-elasticsearch
REVISION  CHANGE-CAUSE
1         <none>
```

接下来，我们把这个DaemonSet的容器镜像版本升级到v2.2.0：

```
$ kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record -n=kube-system
```

这个kubectl set image命令里，第一个fluentd-elasticsearch是DaemonSet的名字，第二个fluent-elasticsearch是容器的名字。这时候，我们可以使用kubectl rollout status命令看到这个“滚动更新”的过程，如下所示：

```
$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 0 out of 2 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 0 out of 2 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 1 of 2 updated pods are available...
daemon set "fluentd-elasticsearch" successfully rolled out
```

注意，由于这一次我在升级命令后面加上了record参数，所以这次升级使用到的指令就会自动出现在DaemonSet的rollout history里面，如下所示：

```
$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system
daemonsets "fluentd-elasticsearch"
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --namespace=kube-system --record=true

```

有了版本号，你也就可以像Deployment一样，将DaemonSet回滚到某个指定的历史版本了。

而在我们前面的文章中讲解Deployment对象的时候，曾经提到过，Deployment管理这些版本，靠的是“一个版本对应一个ReplicaSet对象”。可是，DaemonSet控制器操作的直接就是Pod，不可以有ReplicaSet这样的对象参与其中。那么，它的这些版本又是如何维护的呢？

所谓一切皆对象，Kubernetes v1.7之后添加了一个API对象，名叫ControllerRevision，专门用来记录各种Controller对象的版本。比如，你可以通过如下命令查看fluentd-elasticsearch对应的ControllerRevision：

```
$ kubectl get controllerrevision -n kube-system  -l name=fluentd-elasticsearch
NAME                               CONTROLLER                             REVISION   AGE
fluentd-elasticsearch-6c85f7f4d6   daemonset.apps/fluentd-elasticsearch   2          128m
fluentd-elasticsearch-7f875bbfb5   daemonset.apps/fluentd-elasticsearch   1          6h9m

```

而如果你用kubectl describe查看这个ControllerRevision对象： 

```
$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 -n kube-system
....

Data:
  Spec:
    Template:
      $ Patch:  replace
      Metadata:
        Creation Timestamp:  <nil>
        Labels:
          Name:  fluentd-elasticsearch
      Spec:
        Containers:
          Image:              k8s.gcr.io/fluentd-elasticsearch:v2.2.0
          Image Pull Policy:  IfNotPresent
          Name:               fluentd-elasticsearch
...
Revision:                  2
Events:                    <none>
```

就会看到，这个ControllerRevision对象，实际上是在Data字段保存了该版本对应的完成的DaemonSet的API对象，并且，在Annotation字段保存了创建这个对象所使用的kubectl命令。

接下来，我们就可以尝试将这个DaemonSet回滚到Revision=1时的状态：

```
$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision=1 -n kube-system
daemonset.extensions/fluentd-elasticsearch rolled back
```

这个kubectl rollout undo操作，实际上相当于读取到了Revision=1的ControllerRevision对象保存的Data字段。而这个Data字段里保存的信息，就是Revision=1时这个DaemonSet的完整API对象。所以，现在DaemonSet Controller就可以使用这个历史API对象，对现有的DaemonSet做一次PATCH操作（等价于执行一次kubectl apply -f "旧的DaemonSet对象"），从而把这个DaemonSet“更新”到一个旧版本。这也是为什么，在执行完这次回滚之后，你就发现，DaemonSet的Revision并不会从Revison=2退回到1，而是会增加成Revision=3。这是因为，一个新的ControllerRevision被创建了出来。
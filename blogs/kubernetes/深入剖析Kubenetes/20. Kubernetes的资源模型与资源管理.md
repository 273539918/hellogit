# Kubernetes作业调度与资源管理

## 40 Kubernetes的资源模型与资源管理



### 40.01 资源模型

在Kubernets里，Pod是最小的原子调度单位。这也意味着，所有跟调度和资源管理相关的属性都应该是属于Pod对象的字段。而这其中最重要的部分，就是Pod的CPU和内存配置，如下所示：

```

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
  - name: db
    image: mysql
    env:
    - name: MYSQL_ROOT_PASSWORD
      value: "password"
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
  - name: wp
    image: wordpress
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
```

 在Kubernetes中，像CPU这样的资源被称为“可压缩资源”（compressible resources）它的典型特点是，当可压缩资源不足时，Pod只会“饥饿”，但不会退出。而像内存这样的资源，则被称为“不可压缩资源（incompressible resources）”。当不可压缩资源不足时，Pod就会因为OOM（Out-Of-Memory）被内核杀掉。

而由于Pod可以由多个Container组成，所以CPU和内存资源的限额，是要配置在每个Container的定义上的。这样，Pod整体的资源配置，就由这些Container的配置值累加得到。

其中，Kubernetes里为CPU设置的单位是“CPU的个数”。比如， cpu=1指的就是，这个Pod的CPU限额是1个CPU。当然，具体“1个CPU”在宿主机上如何解释，是1个CPU核心，还是1个vCPU，还是1个CPU超线程（Hyperthread），完全取决于宿主机的CPU实现方式。Kubernetes只负责保证Pod能够使用到“1个CPU”的计算能力。此外，Kubernetes允许你将CPU限额设置为分数，比如在我们的例子里，CPU limits的值就是500m。所谓500m，指的就是500 millicpu，也就是0.5个CPU的意思。这样，这个Pod就会被分配到1个CPU一半的计算能力。

>CPU个数： 看得见摸得找的CPU个数，插在主版上面的。在linux系统下面的/proc/cpuinfo文件的条目中，有多少个不同的physical id就有多少个物理CPU。
>
>CPU物理核： 一个CPU可能有多个核。在linux系统下面的/proc/cpuinfo文件的条目中，可以看具体有多少个cpu cores，就表示一个CPU有多少个CPU物理核
>
>CPU逻辑核（CPU超线程）：用Intel的超线程技术(HT)将物理核虚拟而成的逻辑处理单元。在linux系统下面的/proc/cpuinfo文件的条目中，可以看具体有多少个siblings，就表示一个CPU有多少个逻辑核。逻辑核的概念是指将CPU物理核按时间片逻辑上分成了两个CPU（CPU频率很高，2.4hz 可以分成两块1.2hz的逻辑核）
>
>vCPU：vCPU是在虚拟化的时候提出来的概念，因此vCPU的讨论都是在虚拟化，划分cpu才会讨论的问题。通常一个CPU逻辑核，按照1：100来划分的话，就可以得到 1*100 = 100vCPU
>

当然，你也可以直接把这个配置写成cpu=0.5。但在实际使用时，我还是推荐你使用500m的写法，毕竟这才是Kubernetes内部通用的CPU表示方式。

而对于内存资源来说，它的单位自然就是bytes。Kubernetes支持你使用Ei、Pi、Ti、Gi、Mi（或者E、P、T、G、M、K）的方式来作为bytes的值。比如，在我们的例子里，Memory request的值就是 64MiB（2的26次方bytes）。这里要注意区分MiB（mebibyte）和MB（megabyte）的区别： 1Mi = 1024* 1024 ； 1M=1000*1000。

此外，不难看到，Kubernetes里的Pod的CPU和内存资源，实际上还要分为limits和requests两种情况，如下所示：

```
spec.containers[].resources.limits.cpu
spec.containers[].resources.limits.memory
spec.containers[].resources.requests.cpu
spec.containers[].resources.requests.memory
```

这两者的区别其实非常简单： 在调度的时候，kube-scheduler只会按照requests的值进行计算。而在真正设置Cgroups限制的时候，kubelet则会按照limits的值来进行设置。

更确切地说，当你指定了requests.cpu=250m之后，相当于将Cgroups的cpu.shares的值设置为（250/1000）*1024。而当你没有设置request.cpu的时候，cpu.shares默认则是1024。这样，Kubernetes就通过cpu.shares完成了对CPU时间的按比例分配。而如果你指定了limit.cpu=500m之后，则相当于将Cgroups的cpu.cfs_quota_us的值设置为（500/1000）\*100ms，而cpu.cfs_period_us的值始终是100ms。这样，Kubernetes就为你设置了这个容器只能用到CPU的50%。

> cpu.shares用来设置CPU的相对值，并且是针对所有的CPU（内核），默认值是1024，假如系统中有两个cgroup，分别是A和B，A的shares值是1024，B的shares值是512，那么A将获得1024/(1204+512)=66%的CPU资源，而B将获得33%的CPU资源。shares有两个特点：
> （1）如果A不忙，没有使用到66%的CPU时间，那么剩余的CPU时间将会被系统分配给B，即B的CPU使用率可以超过33%
> （2）如果添加了一个新的cgroup C，且它的shares值是1024，那么A的限额变成了1024/(1204+512+1024)=40%，B的变成了20%
> 从上面两个特点可以看出：
> （1）在闲的时候，shares基本上不起作用，只有在CPU忙的时候起作用，这是一个优点。
> （2）由于shares是一个绝对值，需要和其它cgroup的值进行比较才能得到自己的相对限额，而在一个部署很多容器的机器上，cgroup的数量是变化的，所以这个限额也是变化的，自己设置了一个高的值，但别人可能设置了一个更高的值，所以这个功能没法精确的控制CPU使用率。
>
> cpu.cfs_quota_us，cpu.cfs_period_us:
> cfs_period和cfs_quota这样的关键词，可以用来限制进程长度为cfs_period的一段时间内，只能被分配到总量为cfs_quota的CPU时间。可以用来精确控制cpu使用率）



而对于内存来说，当你指定了limits.memory=128Mi之后，相当于将Cgroups的memory.limit_in_bytes设置为128\*1024\*1024。而需要注意的是，在调度的时候，调度器只会使用requests.memory=64Mi来进行判断。

Kubernetes这种对CPU和内存资源限额的设计，实际上参考了Borg论文中对“动态资源边界”的定义，即：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。

而Kubernetes的requests+limits的做法，其实就是上述思路的一个简化版：用户在提交Pod时，可以声明一个相对较小的requests值供调度器使用，而Kubernetes真正设置给容器Cgroups的，则是相对较大的limits值。不难看到，这跟Borg的思路相通的。

### 40.02 Qos模型

在Kubernetes中，不同的requests和limits的设置方式，其实会将这个Pod划分到不同的QoS级别当中。

1） Guaranteed类别：requests=limits，即Pod里的每一个Container都同时设置了requests和limits，并且requests和limits值相等的时候，这个Pod就属于Guaranteed类别。如下所示：

```

apiVersion: v1
kind: Pod
metadata:
  name: qos-demo
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "700m"
      requests:
        memory: "200Mi"
        cpu: "700m"
```

当这个Pod创建之后，它的qosClass字段就会被Kubernetes自动设置为Guaranteed。需要注意的是，当Pod仅设置了limits没有设置requests的时候，Kubernetes会自动为它设置与limits相同的request值，这也属于Guaranteed情况。

2）Burstable类别： request，当Pod不满足Guaranteed的条件，但至少有一个Container设置了requests。那么这个Pod就会被划分到Burstable。如下：

```
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-2
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-2-ctr
    image: nginx
    resources:
      limits
        memory: "200Mi"
      requests:
        memory: "100Mi"
```

3）BestEffort：None，如果一个Pod即没有设置requests，也没有设置limits，那么它的QoS类别就是BestEffort。如下：

```

apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-3
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-3-ctr
    image: nginx
```

QoS划分的主要应用场景，是当宿主机资源紧张的时候，kubelet对Pod进行Eviction（即资源回收）时需要用到的。

具体地说，当Kubernetes所管理的宿主机上不可压缩资源短缺时，就有可能触发Eviction。比如，可用内存（memory.available）、可用的宿主机磁盘空间（nodefs.available），以及容器运行时镜像存储空间（imagefs.available）等等。

Eviction在Kubernetes里其实分为Soft和Hard两种模式。其中，Soft Eviction允许你为Eviction过程设置一段“优雅时间”。持续一段时间超过阈值才会开始。而Hard Eviction模式下，Eviction过程就会在阈值达到之后立刻开始。

PS ： Kubernetes计算Eviction阈值的数据来源，主要依赖于从Cgroups读取到的值，以及使用cAdvisor监控到的数据。

当宿主机的Eviction阈值达到后，就会进入MemoryPressure或者DiskPressure状态，从而避免新的Pod被调度到这台宿主机上。

### 40.03 cpuset

在使用容器的时候，可以通过设置cpuset把容器绑定到某个CPU的核上，而不是像cpushare那样共享CPU的计算能力。在这种情况下，由于操作系统在CPU之间进行上下文切换的次数大大减少，容器里应用的性能会得到大幅提升。事实上，cpuset方式，是生产环境里部署在线应用类型的Pod时，非常常用的一种方式。

那么，这样的需求在Kubernetes里该如何实现呢？

1）首先，你的Pod必须是Guaranteed的QoS类型；
2） 然后，你只需要将Pod的CPU资源的requests和limits设置为同一个相等的整数值即可。

比如下面这个例子：

```
spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
      requests:
        memory: "200Mi"
        cpu: "2"
```

这时候，该Pod就会被绑定在2个独占的CPU核上。当然，具体是哪两个CPU核，是由kubelet为你分配的。

在实际使用中，建议将DaemonSet的Pod都设置为Guaranteed的QoS类型。否则，一旦DaemonSet的Pod被回收，它又会立即在原宿主机上被重建出来，这就使得前面资源回收的动作完全没有意义。




























# 容器编排与Kubernetes作业管理

## 13  为什么我们需要Pod ?

Pod是Kubernetes项目的原子调度单位。

容器可以理解为是未来云计算系统中的进程，而Kubernetes就是未来云计算系统中的操作系统。

在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则地”组织在一起。比如，这里有一个叫做rsyslogd的程序，它负责的是Linux操作系统里的日志处理。rsyslogd的主程序main，和它要用到的内核日志模块imklog等，同属于一个进程组。这些进程相互协作，共同完成rsyslogd程序的职责。而Kubernetes项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”

### 13.01 成组调度（超亲密关系调度）

Kubernetes项目之所有要这么做的原因：在Borg项目的开发和实践过程中，Google公司的工程师发现，他们部署的应用，往往都存在类似于“进程和进程组”的关系。更具体地说，就是这些应用之间有着密切的协作关系，使得它们必须部署在同一台机器上。而如果事先没有“组”的概念，像这样的运维关系就会变得非常难处理。

以上面的rsyslogd为例，已知rsyslogd由三个进程组成：一个imklog模块、一个imuxsock模块、一个rsyslogd自己的main函数主进程。这三个进程一定要运行在同一台机器上，否则，它们之间基于Socket的通信和文件交换都会出现问题。现在，我们把rsyslogd这个应用容器化，由于受限于容器的“单进程模型”，这三个模块必须被分别制作成三个不同的容器，它们设置的内存额外都是1 GB。这时候，如果我们需要在同一台机器上拉起这三个容器，有很多种方式，比如 Docker Swarm 在另外两个容器中设置对一个容器的亲密性约束，它们必须和main容器运行在同一台机器上。但是这时候又会碰到调度的问题，如果容器main先调度到一台机器上，可能由于机器资源不足，另两个容器无法调度成功。

这就是典型的“成组调度”的问题。关于“成组调度”问题，工业界和学术界提出了很多解决方案，比如Mesos中的资源囤积和Google Omega论文提出的乐观调度处理冲突的方法。可是这些方式都谈不上完美。资源囤积带来了不可避免的调度效率损失和死锁的可能性；而乐观调度的复杂程度，则不是常规技术团队所能驾驭的。但是，到了Kubernetes项目中，这样的问题就引刃而解了： Pod是Kubernetes里的原子调度单位。这就意味着，Kubernetes项目的调度器，是统一按照Pod而非容器的资源需求进行计算的。

像这样容器间的紧密协作，我们可以成为“超亲密关系”。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用localhost或者Socket文件进行本地通信、会发生频繁的远程调用、需要共享某些Linux Namespace等等。这也意味着，并不是所有有“关系”的容器都属于同一个Pod。比如，PHP应用容器和MySQL虽然会发生访问关系，但并没有必要、也不应该部署在同一台机器中它们更适合做成两个Pod。

注意：容器的“单进程模型”，并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力。这是因为容器里的PID=1的进程就是应用本身，其他的进程都是这个PID=1进程的子进程。可是，用户编写的应用，并不能像正常操作系统里的init进程或者systemd那样拥有进程管理的功能，比如，你的应用是一个Java Web程序（PID=1），然后你执行docker exec在后台启动了一个Nginx进程（PID=3）。可是，当这个Nginx进程异常退出的时候，你该怎么知道呢？这个进程退出后的垃圾收集工作，又应该由谁去做呢？

### 13.02 容器设计模式

Kubernetes设计Pod不仅仅是为了超亲密关系的成组调度，Pod在Kubernetes项目里还有更重要的意思，那就是：容器设计模式。

为了理解容器设计模式，需要先了解pod的实现原理

#### 13.02.01 Pod实现原理

首先，关于Pod最重要的一个事实是：它只是一个逻辑概念。

Pod其实是一组共享了某些资源的容器。具体的说，Pod里的所有容器，共享的是同一个Network Namespace，并且可以声明共享同一个Volume。

那这么看来的话，一个有A、B两个容器的Pod，不就等同于一个容器（容器A）共享另外一个容器（容器B）的网络和Volume么？这好像通过 docker run --net --volumes-from这样的命令就能实现，比如：

```
$ docker run --net=B --volumes-from=B --name=A image-A ...
```

但是，如果真这样做的话，容器B就必须比容器A先启动，这样一个Pod里的多个容器就不是对等关系，而是拓扑关系了。所以，在Kubernetes项目里，Pod的实现需要使用一个中间容器，这个容器叫做Infra容器，在这个Pod中，Infra容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过Join Network Namespace的方式，与Infra容器关联在一起。而在Infra容器“Hold住”Network Namespace后，用户容器就可以加入到Infra容器的Network Namespace当中了。

PS: Infra容器占用极少的资源，它使用的是一个非常特殊的镜像，叫做： k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小只有100~200 KB左右。

而对于同一个Pod里面的所有用户容器来说，它们的进出流量，也可以认为都是通过Infra容器完成的。这一点很重要，因为将来如果你要为Kubernetes开发一个网络插件时，应该重点考虑的是如何配置这个Pod的Network Namespace，而不是每一个用户容器如何使用你的网路配置。

有了这个设计之后，共享Volume就简单多了：Kubernetes项目只要把所有Volume的定义都设计在Pod层级即可。一个Volume对应的宿主机目录对于Pod来说就只有一个，Pod里的容器只要声明挂载这个Volume，就一定可以共享这个Volume对应的宿主机目录。比如下面这个例子：

```
apiVersion: v1
kind: Pod
metadata:
  name: two-containers
spec:
  restartPolicy: Never
  volumes:
  - name: shared-data
    hostPath:      
      path: /data
  containers:
  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: shared-data
      mountPath: /usr/share/nginx/html
  - name: debian-container
    image: debian
    volumeMounts:
    - name: shared-data
      mountPath: /pod-data
    command: ["/bin/sh"]
    args: ["-c", "echo Hello from the debian container > /pod-data/index.html"]

```

在这个例子中，debian-container和nginx-container都声明挂载了一个shared-data这个Volume。而shared-data是hostPath类型。所以，它对应宿主机上的目录就是：/data。而这个目录，其实就被同时挂载进了上述两个容器当中。这就是为什么，nginx-container可以从它的/usr/share/nginx/html目录中，读取到debian-container生成的index.html文件的原因。

#### 13.02.02 容器设计模式之Sidecar

Pod这种“超亲密关系”容器的设计模式，实际上就是希望，当用户想在一个容器里跑多个功能并不想管的应用时，应该有限考虑它们是不是更应该被描述成一个Pod里的多个容器。

例1:  WAR包与Web服务器

我们现在有一个Java Web应用的WAR包，它需要被放在Tomcat的webapps目录下运行起来。假如，你现在只能用Docker来做这件事情，那该如何处理这种组合关系呢？

方法1:  把WAR包直接放在Tomcat镜像的webapps目录下，做成一个新的镜像运行起来。可是，这时候，如果你要更新WAR包的内容，或者要升级Tomcat镜像，就要重新制作一个新的发布镜像，非常麻烦
方法2:  压根不管WAR包，永远只发布一个Tomcat容器。不过，这个容器的webapps目录，就必须声明一个hostPath类型的Volume，从而把宿主机上的WAR包挂载进Tomcat容器当中运行起来。不过，这样你就必须解决一个问题，即： 如何让每一台宿主机，都预先准备好这个存储有WAR包的目录？这样来看，你就只能维护一套分布式存储系统了

实际上，有了Pod之后，这样的问题就很容器解决了。我们可以把WAR包和Tomcat分别做成镜像，然后把它们作为一个Pod里的两个容器“组合”在一起，这个Pod的配置文件如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  name: javaweb-2
spec:
  initContainers:
  - image: geektime/sample:v2
    name: war
    command: ["cp", "/sample.war", "/app"]
    volumeMounts:
    - mountPath: /app
      name: app-volume
  containers:
  - image: geektime/tomcat:7.0
    name: tomcat
    command: ["sh","-c","/root/apache-tomcat-7.0.42-v2/bin/start.sh"]
    volumeMounts:
    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps
      name: app-volume
    ports:
    - containerPort: 8080
      hostPort: 8001 
  volumes:
  - name: app-volume
    emptyDir: {}
```

在这个Pod中，我们定义了两个容器，第一个容器使用的镜像是geektime/sample:v2，这个镜像里只有一个WAR包（sample.war）放在根目录下。而第二个容器则使用的是一个标准的Tomcat镜像。但是，WAR包容器的类型不再是一个普通容器，而是一个Init Container类型的容器。在Pod中，所有Init Container定义的容器，都会比spec.containers定义的用户容器先启动。并且，Init Container容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。所以，这个Init Container类型的WAR包容器启动后，执行了一句" cp /sample.war /app"，把应用的WAR包拷贝到/app目录下，然后退出。而后这个/app目录，就挂载了一个名叫app-volume的Volume。 另一个Tomcat容器，同样声明挂载app-volume到自己的webapps目录下。所以，等Tomcat容器启动时，它的webapps目录下就一定会存在sample.war文件：这个文件正是WAR包容器启动时拷贝到这个Volume里面的，而这个Volume是被这两个容器共享的。

像这样，我们就用一种“组合”方式，解决了WAR包与Tomcat容器之间耦合关系的问题。实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫做：sidecar。顾名思义，sidecar指的就是我们可以在一个Pod中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作

例2: 容器的日志收集

假如现在有一个应用，需要不断地把日志文件输出到容器的/var/log目录中。这时，我就可以把一个Pod里的Volume挂载到应用容器的/var/log目录上。然后，我在这个Pod里同时运行一个sidecar容器，它也声明挂载同一个Volume到自己的/var/log目录上。接下来sidecar容器就只需要做一件事情。那就是不断地从自己的/var/log目录里读取日志文件，转发到MongoDB或者Elasticsearch中存储起来。这样，一个最基本的日志收集工作就完成了。

跟第一个例子一样，这个例子中的sidecar的主要工作也是使用共享的Volume来完成对文件的操作。

但不要忘记，Pod的另一个重要特性是，它的所有容器都共享同一个Network Namespace。这就使得很多与Pod网络想管的配置和管理，也都可以交给sidecar完成，而完全无须干涉用户容器。

### 13.03 小结

我们可以这么理解Pod的本质： Pod，实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。

实际上，无论是从具体的实现原理，还是从使用方法、特性、功能等方面，容器与虚拟机几乎没有任何相似的地方；也不存在一种普遍的方法，能够把虚拟机里的应用无缝迁移到容器中。因为，容器的性能优势，必然伴随着相应缺陷，即：它不能像虚拟机那样，完全模拟本地物理机环境中的部署方法。
实际上，一个运行在虚拟机里的应用，哪怕再简单，也是被管理在systemd或者supervisord之下的一组进程，而不是一个进程。这跟本地物理机上运行的运行方式其实是一样的。这也是为什么，从物理机到虚拟机之间的应用迁移，往往并不困难。可是对容器来说，一个容器永远只能管理一个进程。更确切地说，一个容器，就是一个进程。这是容器技术的“天性”，不可能被修改。所以，将一个原本运行在虚拟机里的应用，“无缝迁移”到容器中的想法，实际上跟容器的本质是相悖的。
注意：容器的“单进程模型”，并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力。这是因为容器里的PID=1的进程就是应用本身，其他的进程都是这个PID=1进程的子进程。可是，用户编写的应用，并不能像正常操作系统里的init进程或者systemd那样拥有进程管理的功能，比如，你的应用是一个Java Web程序（PID=1），然后你执行docker exec在后台启动了一个Nginx进程（PID=3）。可是，当这个Nginx进程异常退出的时候，你该怎么知道呢？这个进程退出后的垃圾收集工作，又应该由谁去做呢？

所以，当你需要把一个运行在虚拟机里的应用迁移到Docker容器中时，一定要仔细分析到底有哪些进程（组件）运行在这个虚拟机里。然后，你就可以把整个虚拟机想象成为一个Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。



## 14  深入解析Pod对象（一）：基本概念

Pod扮演的是传统部署环境里“虚拟机”的角色。这样的设计，是为了使用户从传统环境（虚拟机环境）向Kubernetes（容器环境）的迁移，更加平滑。而如果你能把Pod看成传统环境里的“机器”，把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于Pod对象的设计就非常容易理解了。比如，凡事调度、网络、存储、以及安全相关的属性，基本上是Pod级别的。这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。比如，配置这个“机器”的网卡（即： Pod的网络定义），配置这个“机器”的磁盘（即：Pod的存储定义），配置这个“机器”的防火墙（即：Pod的安全定义）。更不用说，这台“机器”运行在哪个服务器之上（即：Pod的调度）

### 14.01 Pod重要字段的含义和用法

NodeSelector：是一个供用户将Pod与Node进行绑定的字段，如：

```
apiVersion: v1
kind: Pod
...
spec:
 nodeSelector:
   disktype: ssd
```

这样的配置，意味着Pod只能运行在携带了"disktype:ssd"标签（Label）的节点上；否则，它将调度失败

NodeName：一旦Pod的这个字段被赋值，Kubernetes项目就会被认为这个Pod已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到

HostAliases：定义了Pod的host文件（比如/etc/hosts）里的内容，用法如下：

```
apiVersion: v1
kind: Pod
...
spec:
  hostAliases:
  - ip: "10.1.2.3"
    hostnames:
    - "foo.remote"
    - "bar.remote"
...

```

需要指出的是，在Kubernetes项目中，如果要设置hosts文件里的内容，一定要通过这种方法。否则，如果直接修改了host文件的话，在Pod被删除重建之后，kubelet会自动覆盖掉被修改的内容

shareProcessNamespace=true：这个参数意味着Pod里的容器共享PID Namespace。比如下面这个YAML文件，定义了两个容器：一个是nginx容器，一个是开启了tty和stdin的shell容器（tty和stdin 等同于docker run 里的 -it , i 即stdin, t即tty），可以直接认为tty就是Linux给用户提供的一个常驻小程序，用于接受用户的标准输入，返回操作系统的标准输出。当然，为了能够在tty中输入信息，还需要同时开启stdin（标准输入流）

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  shareProcessNamespace: true
  containers:
  - name: nginx
    image: nginx
  - name: shell
    image: busybox
    stdin: true
    tty: true
```

于是，这个Pod被创建后，就可以使用shell容器的tty跟这个容器进行交互了。如：

```
$ kubectl create -f nginx.yaml
```

接下来，我们使用kubectl attach命令，连接到shell容器的tty上：

```
$ kubectl attach -it nginx -c shell
```

这样，我们就可以在shell容器里执行ps指令，查看所有正在运行的进程

```
$ kubectl attach -it nginx -c shell
/ # ps ax
PID   USER     TIME  COMMAND
    1 root      0:00 /pause
    8 root      0:00 nginx: master process nginx -g daemon off;
   14 101       0:00 nginx: worker process
   15 root      0:00 sh
   21 root      0:00 ps ax
```

可以看到，在这个容器里，我们不仅可以看到它本身的ps ax指令，还可以看到nginx容器的进程，以及Infra容器的/pause进程，对于所有容器来说都是可见的：它们共享了同一个PID Namespace。类似地，凡是Pod中的容器要共享宿主机的Namespace，也一定是Pod级别的定义，比如： 

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  hostNetwork: true
  hostIPC: true
  hostPID: true
  containers:
  - name: nginx
    image: nginx
  - name: shell
    image: busybox
    stdin: true
    tty: true
```

在这个Pod中，定义了共享宿主机的Network、IPC和PID Namespace。这意味着，这个Pod里的所有容器，会直接使用宿主机的网络、直接与宿主机进行IPC通信、看到宿主机里正在运行的所有进程

### 14.02 Pod中Container的字段

Kubernetes项目中对Container的定义，和Docker相比并没有什么太大区别。有几个属性需要额外关注

ImagePullPolicy：定义了容器的拉取策略，默认值是Always

Lifecycle：定义的是Container Lifecycle Hooks，是在容器状态发生变化时触发一系列“钩子”，例：

```
apiVersion: v1
kind: Pod
metadata:
  name: lifecycle-demo
spec:
  containers:
  - name: lifecycle-demo-container
    image: nginx
    lifecycle:
      postStart:
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
      preStop:
        exec:
          command: ["/usr/sbin/nginx","-s","quit"]
```

postStart指的是，在容器启动后，立即执行一个指定的操作。需要明确的是，postStart定义的操作，虽然是在Docker容器ENTRYPOINT执行之后，但它并不严格保证顺序。也就是说，在postStart启动时，ENTRYPOINT有可能还没有结束。当然，如果postStart执行超时或者错误，Kubernetes会在该Pod的Events中报出该容器启动失败的错误信息，导致Pod也处于失败的状态。

preStop发生的时机，则是在容器被杀死之前。它会阻塞当前的容器，直到这个Hook定义的操作完成之后，才允许容器被杀死，这跟postStart不一样。

所以，在这个例子中，我们在容器成功启动之后，在/usr/share/message里写入了一句“欢迎信息”。而在这个容器被删除之前，我们则先调用了nginx的退出指令，从而实现了容器的“优雅退出”

### 14.03 Pod的声明周期

Pod的生命周期变化，主要体现在Pod API对象的Status部分，这是它除了Metadata和Spec之外的第三个重要字段。其中，pod.status.phase，就是Pod的当前状态，它有如下几种可能：
1、Pending：表示Pod的YAML文件已经提交给Kubernetes，API对象已经被创建并保存在Etcd当中。但是，这个Pod里有些容器因为某种原因不能被顺利闯进啊。比如，调度不成功
2、Running：这个状态下，Pod已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中
3、Succeeded：这个状态意味着，Pod里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次任务时最为常见
4、Failed：这个状态下，Pod里至少有一个容器以不正常的状态（非0的返回码）退出。这个状态的出现，意味着你得想办法Debug这个容器的应用，比如查看Pod的Events和日志
5、Unknow：这是一个异常状态，意味着Pod的状态不能持续地被kubelet汇报给kube-apiserver，这很有可能是主从节点（Master和Kubelet）间的通信除了问题

更进一步，Pod的Status字段，还可以再细分出一组Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized、以及Unshedulabel。它们主要用于描述造成当前Status的具体原因是什么。比如，Pod当前的Status是Pending，对应的Condition是Unschedulabel，这意味着它的调度出了问题。



## 15  深入解析Pod对象（二）：使用进阶



### 15.01 Projected Volume 

在Kubernetes中，有几种特殊的Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机间的数据交换。这些特殊的Volume的作用，是为容器提供预先定义好的数据。所以，从容器的角度看，这些Volume里的信息就是仿佛被Kubernetes“投射”（Project）进入容器当中的。所以叫做 Projected Volume。到目前维持，Kubernetes支持的Projected Volume一共有四种：Secret 、ConfigMap、Download API、ServiceAccountToken

#### 15.01.01 Secret

Secret的作用，是帮你把Pod要访问的加密数据，存放到Etcd中。然后，你就可以通过在Pod的容器里挂载Volume的方式，访问到这些Secret里保存的信息了。比如，使用Secret保存数据库的Credential信息，如下：

```
apiVersion: v1
kind: Pod
metadata:
  name: test-projected-volume 
spec:
  containers:
  - name: test-secret-volume
    image: busybox
    args:
    - sleep
    - "86400"
    volumeMounts:
    - name: mysql-cred
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: mysql-cred
    projected:
      sources:
      - secret:
          name: user
      - secret:
          name: pass
```

在这个Pod中，定义了一个简单的容器。它生命挂载的Volume，并不是常见的emptyDir或者hostPath类型，而是projected类型。而这个Volume的数据源（sources），则是名为user和pass的Secret对象，分别对应的是数据库的用户名和密码。

这里用到的数据库和用户名、密码，正是以Secret对象的方式交给Kubernetes保存的。完成这个操作的指令，如下所示：

```
$ cat username.txt
admin123456
$ cat password.txt
password123456

$ kubectl create secret generic user --from-file=username.txt
secret/user created
$ kubectl create secret generic pass --from-file=password.txt
secret/pass created

```

其中，user和pass是我们为Secret对象指定的名字。想要查看这些Secret对象的话，只要执行一条kubectl get命令就可以了：

```
$ kubectl get secrets
NAME                  TYPE                                  DATA   AGE
default-token-zp682   kubernetes.io/service-account-token   3      8d
pass                  Opaque                                1      29s
user                  Opaque                                1      44s
```

除了使用kubectl create secret指令外，也可以直接通过编写YAML文件的方式来创建这个Secret对象，比如：

```
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  user: YWRtaW4=
  pass: MWYyZDFlMmU2N2Rm
```

可以看到，通过编写YAML文件创建出来的Secret对象只有一个。但它的data字段，却以Key-Value的格式保存了两份Secret数据。其中，"user"就是第一份数据的Key，“pass”是第二份数据的Key。

需要注意的是，Secret对象要求这些数据必须是经过Base64转码的，以免出现明文密码的安全隐患。这个转码操作也很简单，比如：

```
$ echo -n 'admin' | base64
YWRtaW4=
$ echo -n 'd' | base64
MWYyZDFlMmU2N2Rm
```

注意：例子中只进行了转码，还未进行加密。在生产环境中，还需要开启Secret的加密插件，增强数据安全性

接下来，我们尝试创建这个Pod：

```
$ kubectl apply -f test-projected-volume.yamld
```

当Pod变成Running状态之后，我们再验证一下这些Secret对象是不是已经在容器里了：

```
$ kubectl exec -it test-projected-volume -- /bin/sh
$ ls /projected-volume/
password.txt  username.txt
$  cat /projected-volume/username.txt
admin123456

```

更重要的是，像这样通过挂载方式进入到容器里的Secret，一旦其对应的Etcd里的数据被更新，这些Volume里的文件内容同样也会被更新。其实，这是kubelet组件在定时维护这些Volume。需要的注意的是，这个更新可能会有一定的延时。

#### 15.01.02 ConfigMap

ConfigMap保存的是不需要加密的，应用所需的配置信息。用法几乎与Secret类似

```
# .properties文件的内容
$ cat example/ui.properties
color.good=purple
color.bad=yellow
allow.textmode=true
how.nice.to.look=fairlyNice

# 从.properties文件创建ConfigMap
$ kubectl create configmap ui-config --from-file=example/ui.properties

# 查看这个ConfigMap里保存的信息(data)
$ kubectl get configmaps ui-config -o yaml
apiVersion: v1
data:
  ui.properties: |
    color.good=purple
    color.bad=yellow
    allow.textmode=true
    how.nice.to.look=fairlyNice
kind: ConfigMap
metadata:
  name: ui-config
  ...
```

备注： kubectl get -o yaml这样的参数，会将制定的Pod API对象以YAML的方式展示出来

#### 15.01.03 Downward API

Downward的作用是让Pod里的容器能够直接获取到这个Pod API对象本身的信息。例如：

```
apiVersion: v1
kind: Pod
metadata:
  name: test-downwardapi-volume
  labels:
    zone: us-est-coast
    cluster: test-cluster1
    rack: rack-22
spec:
  containers:
    - name: client-container
      image: busybox
      command: ["sh", "-c"]
      args:
      - while true; do
          if [[ -e /etc/podinfo/labels ]]; then
            echo -en '\n\n'; cat /etc/podinfo/labels; fi;
          sleep 5;
        done;
      volumeMounts:
        - name: podinfo
          mountPath: /etc/podinfo
          readOnly: false
  volumes:
    - name: podinfo
      projected:
        sources:
        - downwardAPI:
            items:
              - path: "labels"
                fieldRef:
                  fieldPath: metadata.labels
```

这个Pod里的YAML文件中，定义了一个简单的容器，声明了一个projected类型Volume。只不过这次Volume的数据来源，变成了Downward API。而这个Downward API Volume，则声明了要暴露Pod的metadata.labels信息给容器。通过这样的声明方式，当前Pod的Labels字段的值，就会被Kubernetes自动挂载成为容器里的/etc/podinfo/labels文件。

而这个容器的启动命令，则是不断打印出/etc/podinfo/labels里的内容。所以，创建这个Pod之后，就可以通过kubectl logs指令，查看到这些Labels字段被打印出来，如下所示：

```
$ kubectl apply -f test-downwardapi-volume.yaml
$ kubectl logs test-downwardapi-volume
cluster="test-cluster1"
rack="rack-22"
zone="us-est-coast"
```

需要注意的是，Downward API能够获取到的信息，一定是Pod里的容器进程启动之前就能够确定下来的信息。而如果你想要获取Pod容器运行后才会出现的信息，比如，容器进程的PID，那就肯定不能使用Downward API了，而应该考虑在Pod里定义一个sidecar容器。

其实，Secret、ConfigMap，以及Downward API这三种Projected Volume定义的信息，大多还可以通过环境变量的方式出现在容器里。但是，通过环境变量获取这些信息的方式，不具备自动更新的能力。所以，一般情况下，建议使用Volume文件的方式获取这些信息

#### 15.01.04 ServiceAccountToken

如果想要在Pod里安装一个Kubernetes的Client，以方便从容器里直接访问并且操作这个Kubernetes的API。就需要先解决 API Server的授权问题。 Service Account对象的作用，就是Kubernetes系统内置的一种“服务账号”，它是Kubernetes进行权限分配的对象。比如，Servcie Account A，可以只被允许对Kubernetes API进行GET操作，而Service Account B，则可以有Kubernetes API的所有操作权限。像这样的Service Account的授权信息和文件，实际上保存在它所绑定的一个特殊的Secret对象里。这个特殊的Secret对象，就叫做ServiceAccountToken。任何运行在Kubernetes集群上的应用，都必须使用这个ServiceAccountToken里保存的授权信息，也就是Token，才可以合法地访问API Server。

所以说，Kubenetes项目的Project Volume其实只有三种，因为第四种ServiceAcccountToken，只是一种特殊的Secret而已。另外，为了方便使用，Kubernetes已经为你提供了一个默认“服务账号”（default Service Account）。并且，任何一个运行在Kubernetes里的Pod，都可以直接使用这个默认的Service Account，而无需显示地声明挂载它。

如果查看一下任意一个运行在Kubernetes集群里的Pod，就会发现，每一个Pod，都已经自动声明了一个类型是Secret，名为default-token-xxxx的Volume，如：

```
$ kubectl describe pod test-downwardapi-volume
Containers:
...
  Mounts:
    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)
Volumes:
  podinfo:
    Type:         Projected (a volume that contains injected data from multiple sources)
    DownwardAPI:  true
  default-token-zp682:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-zp682
    Optional:    false
```

这样，一旦Pod创建完成，容器里的应用就可以直接从这个默认ServiceAccontToken的挂载目录里访问到授权信息和文件。这个容器内的路径在Kubernetes里是固定的，即：  /var/run/secrets/kubernetes.io/serviceaccout，而这个Secret类型的Volume里面的内容如下：

```
$ kubectl exec -it  test-downwardapi-volume -- /bin/sh
$ ls /var/run/secrets/kubernetes.io/serviceaccount 
ca.crt namespace  token
```

所以，你的应用程序只要直接加载这些授权文件，就可以访问并操作Kubernetes API了。这种把Kubernetes客户端以容器的方式运行在集群里，然后使用default Service Account自动授权的方式，被称为“InClusterConfig”

### 15.02 容器健康检查和恢复机制

在Kubernetes中，可以为Pod里的容器定义一个健康检查“探针”（Probe）。这样，kubelet就会根据这个Probe的返回值决定这个容器的状态，而不是以容器是否运行（来自Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。例： 

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: test-liveness-exec
spec:
  containers:
  - name: liveness
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 5
      periodSeconds: 5

```

在这个Pod中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在/tmp目录下创建了一个healthy文件，以此作为自己已经正常运行的标志。而30s过后，它会把这个文件删除掉。与此同时，我们定义了一个这样的livenessProbe（健康检查）。它的类型是exec，这意味着，它会在容器启动之后，在容器里执行一条我们指定的命令，比如："cat /tmp/headlthy"。这时，如果这个文件存在，这条命令的返回值就是0，Pod就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动5s后开始执行（initialDelaySeconds：5），每5s执行一次（periodSeconds：5）。

创建这个Pod：

```
kubectl apply -f test-liveness-exec.yaml
```

然后，查看这个Pod的状态

```
$ kubectl get pod | grep liveness
test-liveness-exec                 0/1     ContainerCreating   0          13s
```

可以看到，由于已经通过了健康检查，这个Pod就进入了Running状态。30秒之后，我们再查看一下这个Pod的Events：

```
$ kubectl describe pod test-liveness-exec
...
Warning  Unhealthy  16s (x3 over 26s)  kubelet, bd011088191061.na610  Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory
  Normal   Killing    16s                kubelet, bd011088191061.na610  Container liveness failed liveness probe, will be restarted
...
```

此时，我们再查看Pod的状态，会发现Pod并没有进入Failed状态，而是保持了Running状态。但是，RESTARTS字段从0变到了1，说明这个异常的容器已经被Kubernetes重启了。在这个过程中，Pod保持Running状态不变。

```
$ kubectl get pod test-liveness-exec
NAME           READY     STATUS    RESTARTS   AGE
liveness-exec   1/1       Running   1          1m
```

需要注意的是： Kubernetes中并没有Docker的Stop语义，虽然是Restart（重启），但实际却是重新创建了容器。

这个功能就是Kubernetes里的Pod恢复机制，也叫restartPolicy。它是Pod的Spec部分的一个标准字段（pod.spec.restartPolicy），默认值是Always，即：任何时候这个容器发生了异常，它一定会被重新创建。

```
$kubectl get pod test-liveness-exec -o yaml
...
restartPolicy: Always
...
```



但一定要强调的是，Pod的恢复过程，永远都是发生在当前节点上，而不会跑到其他节点上去。事实上，一旦一个Pod与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node字段被修改），否则它永远都不会离开这个节点。这也意味着，如果这个宿主机宕机了，这个Pod也不会主动迁移到其他节点上去。 而如果你想让Pod出现在其他可用节点上，就必须使用Deployment这样的“控制器”来管理Pod，哪怕你只需要一个Pod副本。

restartPolicy有一下几种值:
1、Always：在任何情况下，只要容器不在运行状态，就会自动重启容器；
2、OnFailure：只在容器异常时才会自动重启容器；
3、Never：从来不重启容器

在实际使用中，我们需要根据应用的特性，合理设置这三种恢复策略。比如，一个Pod，它只计算1+1=2，计算完成输出结果后退出，变成Succeeded状态。这时，你如果再用restartPolicy=Always强制重启这个Pod的容器，就没有任何意义了。而如果你要关心这个容器退出后的上下文环境，比如容器退出后的日志、文件和目录，就需要将restartPolicy设置为Never。因为一旦容器被自动重新创建，这些内容就有可能丢失掉了（被垃圾回收了）。所以，我们记住如下两个设计原理：

1、只要Pod的restartPolicy指定的策略允许重启异常的容器（比如： Always），那么这个Pod就会保持Running状态，并进行容器重启。否则，Pod就会进入Failed状态。
2、对于包含多个容器的Pod，只有它里面所有的容器都进入异常状态后，Pod才会进入Failed状态。在此之前，Pod都是Runnning状态。此时， Pod的READY字段会显示正常的容器个数。

除了再容器中执行命令外，livenessProbe也可以定义为发起HTTP或者TCP请求的方式，定义格式如下：

```
...
livenessProbe:
     httpGet:
       path: /healthz
       port: 8080
       httpHeaders:
       - name: X-Custom-Header
         value: Awesome
       initialDelaySeconds: 3
       periodSeconds: 3
```

```
...
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 15
      periodSeconds: 20
```

### 15.03 PodPreset（Pod预设置）

场景： 开发人员提交一个基本的、非常简单的Pod YAML。运维人员通过PodPreset自动给对应的Pod对象加上其他的必要信息，比如labels，annotations，volumes等等。这样一来，开发人员编写Pod YAML门槛就大大降低了。

比如，开发人员编写了如下一个pod.yaml文件：

```
apiVersion: v1
kind: Pod
metadata:
  name: website
  labels:
    app: website
    role: frontend
spec:
  containers:
    - name: website
      image: nginx
      ports:
        - containerPort: 80

```

运维人员想要对这个Pod添加一些必要的字段，可以这样编写preset.yaml：

```
apiVersion: settings.k8s.io/v1alpha1
kind: PodPreset
metadata:
  name: allow-database
spec:
  selector:
    matchLabels:
      role: frontend
  env:
    - name: DB_PORT
      value: "6379"
  volumeMounts:
    - mountPath: /cache
      name: cache-volume
  volumes:
    - name: cache-volume
      emptyDir: {}

```

在这个PodPreset的定义中，首先是一个selector。这就意味着后面这些追加的定义，只会作用于selector所定义的、带有“role:frontend”标签的Pod对象，这就可以防止“误伤”。然后，我们定义了一组Pod的Spec里的标准字段，以及对应的值。比如，env里定义了DB_PORT这个环境变量，volumeMounts定义了容器Volume的挂载目录，volumes定义了一个emptyDir的Volume。

接下来，运行这个Pod

```
$ kubectl create -f preset.yaml
$ kubectl create -f pod.yaml
```

Pod运行起来之后，我们查看一下这个Pod的API对象：

```
$ kubectl get pod website -o yaml
apiVersion: v1
kind: Pod
metadata:
  name: website
  labels:
    app: website
    role: frontend
  annotations:
    podpreset.admission.kubernetes.io/podpreset-allow-database: "resource version"
spec:
  containers:
    - name: website
      image: nginx
      volumeMounts:
        - mountPath: /cache
          name: cache-volume
      ports:
        - containerPort: 80
      env:
        - name: DB_PORT
          value: "6379"
  volumes:
    - name: cache-volume
      emptyDir: {}
```

这个时候，我们就可以清楚地看到，这个Pod里多了新添加的env、volumes和volumeMount的定义，它们的配置跟PodPreset的内容一样。此外，这个Pod还被自动加上了一个annotation表示这个Pod对象被PodPreset改动过。

需要说明的是，PodPreset里定义的内容，只会在Pod API对象被创建之前追加在这个对象本身上，而不会影响任何Pod的控制器的定义。比如，我们现在提交的是一个nginx-deployment，那么这个Deployment对象本身是永远不会被PodPreset改变的，被修改的只是这个Deployment创建出来的所有Pod。

如果有多个PodPreset，Kubernetes会进行合并（Merge），若有冲突，则冲突字段不会被修改。












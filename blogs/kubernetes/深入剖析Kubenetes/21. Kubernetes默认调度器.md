# Kubernetes作业调度与资源管理

##  学习总结

 调度器的作用就是为Pod寻找一个合适的Node。

调度过程：待调度Pod被提交到apiServer -> 更新到etcd -> 调度器Watch etcd感知到有需要调度的pod（Informer） -> 取出待调度Pod的信息 ->Predicates： 挑选出可以运行该Pod的所有Node  ->  Priority：给所有Node打分 -> 将Pod绑定到得分最高的Node上 -> 将Pod信息更新回Etcd -> node的kubelet感知到etcd中有自己node需要拉起的pod -> 取出该Pod信息，做基本的二次检测（端口，资源等） -> 在node 上拉起该pod 。

Predicates阶段会有很多过滤规则：比如volume相关，node相关，pod相关
Priorities阶段会为Node打分，Pod调度到得分最高的Node上，打分规则比如： 空余资源、实际物理剩余、镜像大小、Pod亲和性等

Kuberentes中可以为Pod设置优先级，高优先级的Pod可以： 1、在调度队列中先出队进行调度 2、调度失败时，触发抢占，调度器为其抢占低优先级Pod的资源。

Kuberentes默认调度器有两个调度队列：
activeQ：凡事在该队列里的Pod，都是下一个调度周期需要调度的
unschedulableQ:  存放调度失败的Pod，当里面的Pod更新后就会重新回到activeQ，进行“重新调度”

默认调度器的抢占过程： 确定要发生抢占 -> 调度器将所有节点信息复制一份，开始模拟抢占 ->  检查副本里的每一个节点，然后从该节点上逐个删除低优先级Pod，直到满足抢占者能运行 -> 找到一个能运行抢占者Pod的node -> 记录下这个Node名字和被删除Pod的列表 -> 模拟抢占结束 -> 开始真正抢占 -> 删除被抢占者的Pod，将抢占者调度到Node上 





## 41 十字路口上的Kubernetes默认调度器

在Kubernetes项目中，默认调度器的主要职责，就是为一个新创建出来的Pod，寻找一个最合适的节点（Node）。这里的“最合适”的含义，包括两层：
1）从集群所有的节点中，根据调度算法挑选出所有可以运行该Pod的节点；
2）从第一步的结果中，再根据调度算法挑选一个最符合条件的节点作为最终结果；

所以在具体的调度流程中，默认调度器会首先调用一组叫做Predicate的调度算法，来检查每个Node。然后，再调用一组叫做Priority的调度算法，来给上一步得到的结果里的每个Node打分。最终的调度结果，就是得分最高的那个Node。而调度器对一个Pod调度成功，实际上就是将它的spec.nodeName字段填上调度结果的节点名字。

在Kubernetes中，上述调度机制的工作原理，可以用如下所示的一副示意图来表示。

![img](https://static001.geekbang.org/resource/image/bb/53/bb95a7d4962c95d703f7c69caf53ca53.jpg)

可以看到，Kubernetes的调度器的核心，实际上就是两个相互独立的控制循环。

其中，第一个控制循环，我们可以称之为Informer Path。它的主要目得，是启动一系列Informer，用来监听（Watch）Etcd中Pod、Node、Service等与调度相关的API对象的变化。比如，当一个待调度Pod（即：它的nodeName字段是空的）被创建出来之后，调度器就会通过Pod Informer的Handler，将这个待调度Pod添加进调度队列。

在默认情况下，Kubernetes的调度队列是一个PriorityQueue（优先级队列），并且当某些集群信息发生变化的时候，调度器还会对调度队列里的内容进行一些特殊操作。此外，Kubernetes的默认调度器还要负责对调度器缓存（即：scheduler cache）进行更新。事实上，Kubernetes调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息Cache化，以便从根本上提高Predicate和Priority调度算法的执行效率。

而第二个控制循环，是调度器负责Pod调度的主循环，我们可以称之为Scheduling Path。Scheduling Path的主要逻辑，就是不断地从调度队列里出队一个Pod。然后，调用Predicates算法进行“过滤”。这一步“过滤”得到的一组Node，就是所有可以运行这个Pod的宿主机列表。当然，Predicates算法需要的Node信息，都是从Scheduler Cache里直接拿到的，这是调度器保证算法执行效率的主要手段之一。

接下来，调度就会再调用Priorities算法为上述列表里的Node打分，分数从0到10.得分最高的Node，就会作为这次调度的结果。调度算法执行完成后，调度器就需要将Pod对象的nodeName字段的值，修改为上述Node的名字。这个步骤在Kubernetes里面称为Bind。

但是，为了不在关键调度路径里远程访问APIServer，Kuberentes的默认调度器在Bind阶段，只会更新Scheduler Cache里的Pod和Node的信息。这种基于“乐观”假设的API对象更新方式，在Kuberentes里被称为Assume。

Assume之后，调度器才会创建一个Goruntime来异步地向APIServer发起更新Pod的请求，来真正完成Bind操作。如果这次异步Bind过程失败了，其实也没有太大关系，等Scheduler Cache同步之后一切就恢复正常。

当然，正是由于上述Kubernetes调度器的“乐观”绑定的设计，当一个新的Pod完成调度需要在某个节点上运行起来之前，该节点上的kubelet还会通过一个叫做Admit的操作来再此验证该Pod是否确实能够运行在该节点上。这一步Admit操作，实际上就是把一组叫作GeneralPredicates的、最基本的调度算法，比如：“资源是否可用”，“端口是否冲突”等再执行一遍，作为kubelet端的二次确认。

随着Kubernetes项目逐渐趋于稳定，越来越多的用户开始把Kubernetes用在规模更大、业务更加复杂的私有集群中。很多以前的Mesos用户，也开始尝试使用Kubernetes来替代其原有架构。在这些场景下，对默认调度器进行扩展和重新实现，就成了社区对Kubernetes项目最重要的一个诉求。而调度这个事情，在不同的公司和团队里的实际需求一定是大相径庭的，上游社区不可以提供一个大而全的方案出来。所以，将默认调度器进一步做轻做薄，并且插件化，才是kube-scheduler正确的演进方向。

## 42 Kubernetes默认调度器调度策略解析

这篇主要介绍在调度过程中Predicates和Priorities这两个调度策略主要发生作用的阶段。

### 42.1 Predicates

Predicates在调度过程中的作用，可以理解为Filter，即：它按照调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度Pod的宿主机。

而在Kubernetes中，默认的调度策略有如下几种：

#### 42.1.1 GeneralPredicates

第一种类型，叫作GeneralPredicates。顾名思义，这一组过滤规则，负责的是最基础的调度策略。比如，PodFitsResources计算的就是宿主机的CPU和内存资源等是否够用。当然，前面已经提到过，PodFitsResources检查的只是Pod的requests字段。需要注意的是，Kubernetes的调度器并没有为GPU等硬件资源定义具体的资源类型，而是统一用一种名叫Extended Resource的、Key-Value格式的扩展字段来描述的。比如下面这个例子：

```
apiVersion: v1
kind: Pod
metadata:
  name: extended-resource-demo
spec:
  containers:
  - name: extended-resource-demo-ctr
    image: nginx
    resources:
      requests:
        alpha.kubernetes.io/nvidia-gpu: 2
      limits:
        alpha.kubernetes.io/nvidia-gpu: 2
```

可以看到，我们这个Pod通过alpha.kubernetes.io/nvidia-gpu=2这样的定义方式，声明使用了两个NVIDIA类型的GPU。而在PodFitsResources里面，调度器其实并不知道这个字段Key的含义是GPU，而是直接使用后面的Value进行计算。当然，在Node的Capacity字段里，你也得相应地加上这台宿主机上GPU的总数，比如：alpha.kubernetes.io/nvidia-gpu=4。

而PodFitsHost检查的是，宿主机的名字是否跟Pod的spec.nodeName一致。PodFitsHostPorts检查的是，Pod申请的宿主机端口（ spec.nodePort）是不是跟已经被使用的端口有冲突。PodMatchNodeSelector检查的是，Pod的nodeSelector或者nodeAffinity指定你的节点，是否与待考察节点匹配。

可以看到，像上面这样一组GeneralPredicates，正是Kubernetes考察一个Pod能不能运行在一个Node上最基本的过滤条件。所以，GeneralPredicates也会被其他组件（比如kubelet）直接调用。

在上一篇文件中已经提到过， kubelet在启动Pod前，会执行一个Admit操作来进行二次确认。这里二次确认的规则，就是执行一遍GeneralPredicates



#### 42.1.2 Volume相关的过滤规则

这一组过滤规则，负责的是跟容器持久化Volume相关的调度策略。其中，NoDiskConflict检查的条件，是多个Pod声明挂载的持久化Volume是否有冲突。比如，AWS EBS类型的Volume，是不允许被两个Pod同时使用的。所以，当一个名叫A的EBS Volume已经被挂在了某个节点上，另一个同样声明使用这个A Volume的Pod，就不能被调度到这个节点上了。

而MaxPDVolumeCountPredicate检查的条件，则是一个节点上某种类型的持久化Volume是不是已经超过一定数量，如果是的话，那么声明使用该类型持久化Volume就不能再调度到这个节点了。

此外，还有一个叫做VolumeBindingPredicate的规则。它负责检查的，是该Pod对应的PV的nodeAffinity字段，是否跟某个节点的标签相匹配。比如，Local Persisitent Volume（本地化持久化），必须使用nodeAffinity来跟某个具体的节点绑定。这其实也意味着，在Predicates阶段，Kubernetes就必须能够根据Pod的Volume属性来进行调度。

此外，如果该Pod的PVC还没有跟具体的PV绑定的话，调度器还要负责检查所有待绑定PV，当有可用的PV存在并且该PV的nodeAffinity与待考察节点一致时，这条规则才会返回“成功”。比如下面这个例子：

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-local-pv
spec:
  capacity:
    storage: 500Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-storage
  local:
    path: /mnt/disks/vol1
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - my-node
```

可以看到，这个PV对应的持久化目录，只会出现在名叫my-node的宿主机上。所以，任何一个通过PVC使用这个PV的Pod，都必须被调度到my-node上才可以正常工作。VolumeBindingPredicate，正是调度器里完成这个决策的位置。

#### 42.1.3 宿主机相关的过滤规则

这一组规则，主要考察待调度Pod是否满足Node本身的某些条件。比如，PodToleratesNodeTaints，负责检查的就是我们前面经常用到的Node的"污点"机制。只有当Pod的Toleration字段与Node的Taint字段能够匹配的时候，这个Pod才能被调度到该节点上。而NodeMemoryPressurePredicate，检查的是当前节点的内存是不是已经不够充足，如果是的话，那么待调度Pod就不能被调度到该节点上。

#### 42.1.4 Pod相关的过滤规则

这一组规则，跟GeneralPredicates大多数是重合的。而比较特殊的，是PodAffinityPredicate。这个规则的作用，是检查待调度Pod与Node上的已有Pod之间的亲密（affinity）和反亲密（anti-affinity）关系。

上面这四种类型的Predicates，就构成了调度器确定一个Node可以运行待调度Pod的基本策略。在具体执行的时候，当开始调度一个Pod时，Kubernetes调度器会同时启动16个Goroutine，来并发地为集群里的所有Node计算Predicates，最后返回可以运行这个Pod的宿主机列表。

需要注意的是，在为每个Node执行Predicates时，调度器会按照固定的顺序来进行检查。这个顺序，是按照Predicates本身的含义来确定的。比如，宿主机相关的Predicates会被放在相对靠前的位置进行检查。要不然的话，在一台资源已经严重不足的宿主机上，上来就开始计算PodAffinityPredicate，是没有实际意义的。







### 42.2 Priorities

在Predicates阶段完成了节点“过滤”之后，Priorities阶段的工作就是为这些节点打分。这里打开的范围是0～10分，得分最高的节点就是最后被Pod绑定的最佳节点。

Priorities里最常用到的一个打分规则，是LeastRequestedPriority。它的计算方法，可以简单地总结为如下所示的公式：

```
score = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2
```

可以看到，这个算法实际上就是在选择空闲资源（CPU和Memory）最多的宿主机。而与LeastRequestedPriority一起发挥作用的，还是有BalancedResourceAllocation。它的计算公式如下所示：

```
score = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10
```

其中，每种资源的Fraction的定义是：Pod请求的资源/节点上的可用资源。而variance算法的作用，则是计算每两种资源Fraction之间的“距离”。而最后选择的，则是资源Fraction差距最小的节点。

所以说，BalancedResourceAllocation选择是，其实是在调度完成过后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上CPU被大量分配、而Memory大量剩余的情况。

此外，还有NodeAffinityPriority、TaintTolerationPriority和InterPodAffinityPriority这三种Priority。在默认Priorites里，还有一个叫做ImageLocalityPriority的策略。它是在Kubernetesv1.12里新开启的调度规则，即：如果待调度Pod需要使用的镜像很大，并且已经存在于某些Node上，那么这些Node的得分就会很高。当然，为了避免这个算法引起调度堆叠，调度器在计算得分的时候还会根据镜像的分布进行优化，即：如果大镜像分布的节点数目很少，那么这些节点的权重就会被调低，从而“对冲”掉引起调度堆叠的风险。

以上，就是Kubernetes调度器的Predicates和Priorities里默认调度规则的主要工作原理了。在实际执行过程中，调度器里关于集群和Pod的信息都已经缓存化，所以这些执行算法的执行过程还是比较快的。

需要注意的是，除了本篇讲述的这些规则，Kubernetes调度器里其实还有一些默认不会开启的策略。你可以通过为kube-scheduler制定一个配置文件或者创建一个ConfigMap，来配置哪些规则需要开启、哪些规则需要关闭。并且，你可以通过为Priorities设置权重，来控制调度器的调度行为。





## 43 Kubernetes默认调度器的优先级与抢占机制

在这篇文章里，讲解Kubernetes调度器里另一个重要机制，即：优先级（Priority）和抢占（Preemption）机制。

正常情况下，当一个Pod调度失败后，它就会被暂时“搁置”起来，直到Pod被更新，或者集群状态发生变化，调度器才会对这个Pod进行重新调度。但在有时候，我们希望的是这样一个场景。当一个高优先级的Pod调度失败后，该Pod并不会被“搁置”起来，而是会“挤走”某个Node上的一些低优先级的Pod。这样就可以保证这个高优先级Pod的调度成功。这个特性，其实也是一直以来就存在于Borg以及Mesos等项目里的一个基本功能。

要使用这个机制，首先需要在Kubernetes里提交一个PriorityClass的定义，如下所示：

```
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for high priority service pods only."
```

上面这个YAML文件，定义的额是一个名叫high-priority的PriorityClass，其中value的值是1000000（一百万）。

Kubernetes规定，优先级是一个32bit的整数，最大值不超过10亿，并且值越大代表优先级越高。而超出10亿的值，其实是被Kubernetes保留下来分配给系统Pod使用的。显然，这样做的目的，就是保证系统Pod不会被用户抢占掉。

而一旦上述YAML文件里的globalDefault被设置为true的话，那就意味着这个PriorityClass值会成为系统里的默认值。而如果这个值是false，就表示我们只希望声明使用该PriorityClass的Pod拥有值为1000000的优先级，而对于没有声明PriorityClass的Pod，它们的优先级就是0。

在创建了PriorityClass对象之后，Pod就可以声明使用它了，如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  priorityClassName: high-priority
```

当这个Pod被提交给Kubernetes之后，Kubernetes的PriorityAdmissionController就会自动将这个Pod的spec.priority的字段设置为1000000。调度器里维护着一个调度队列，当Pod拥有了优先级之后，高优先级的Pod就可能会比低优先级的Pod提前出队，从而尽早完成调度过程。这个过程，就是“优先级”这个概念在Kubernetes里的主要体现。

而当一个高优先级的Pod调度失败的时候，调度器的抢占能力就会被触发。这时，调度器就会试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级Pod被删除后，待调度的高优先级Pod就可以被调度到这个节点上这个过程，就是“抢占”这个概念在Kubernetes里的主要体现。

为了方便叙述，接下来会把待调度的高优先级Pod称为“抢占者”（Preemptor）。

当上述抢占过程发生时，抢占者并不会立即被调度到被抢占的Node上。事实上，调度器只会将抢占者的spec.nominatedNodeName字段，设置为被抢占的Node的名字。然后，抢占者会重新进入下一个调度周期，然后在新的调度周期里来决定是不是要运行在被抢占的节点上。这当然也意味着，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。

这样设计的一个重要原因是，调度器只会通过标准的DELETE API来删除被抢占的Pod，所以，这些Pod必然是有一定的“优雅退出”时间的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退出期间，集群的可调度行可能发生的变化，把抢占者交给下一个调度周期再处理，是一个非常合理的选择。

而在抢占者等待被调度的过程中，如果有其他更高优先级的Pod也要抢占同一个节点，那么调度器就会清空原抢占者的spec.nominatedNodeName字段，从而允许更高优先级的抢占者执行抢占，并且，这也就使得原抢占者本身，也有机会去重新抢占其他节点。这些，都是设置nominatedNodeName字段的主要目的。

前面已经提到过，抢占发生的原因，一定是一个高优先级的Pod调度失败。这一次，我们还是称这个Pod为“抢占者”，称被抢占的Pod为“牺牲者”（victims）。

而Kubernetes调度器实现抢占算法的一个最重要的设计，就是在调度队列的实现里，使用了两个不同的队列。
第一个队列，叫做activeQ。凡是在activeQ里的Pod，都是下一个调度周期需要调度的对象。所以，当你在Kubernetes集群里新创建一个Pod的时候，调度器会将这个Pod入队到activeQ里面。而我在前面提到过的、调度器不断从队列里出队（Pop）一个Pod进行调度，实际上都是从activeQ里出队的。
第二个队列，叫做unschedulableQ，专门用来存放调度失败的Pod。

而这里的一个关键点就在于，当一个unschedulableQ里的Pod被更新后，调度器会自动把这个Pod移动到activeQ里，从而给这些调度失败的Pod“重新做人”的机会。

调度失败后，抢占者就会被放进unschedulableQ里面。然后，这次失败就会触发调度器为抢占者寻找牺牲者的流程。

第一步，调度器会检查这次失败事件的原因，来确认抢占是不是可以帮助抢占者找到一个新节点。这是因为很多Predicates的失败者是不能通过抢占来解决的。比如，PodFitsHost算法（负责的是，检查Pod的nodeselector与Node的名字是否匹配），在这种情况下，除非Node的名字发生变化，否则你即使删除再多的Pod，抢占者也不可能调度成功。

第二步，如果确定抢占可以发生，那么调度器就会把自己缓存的所有节点信息复制一份，然后使用这个副本来模拟抢占过程。

这里的抢占过程很容易理解。调度器会检查缓存副本里的每一个节点，然后从该节点上最低优先级的Pod开始，逐一“删除”这些Pod。而每删除一个低优先级Pod，调度器都会检查一下抢占者是否能够运行在该Node上。一旦可以运行，调度器就记录下这个Node的名字和被删除Pod的列表，这就是一次抢占过程的结果了。

当遍历完所有的节点之后，调度器会在上述模拟产生的所有抢占结果里做一个选择，找出最佳结果。而这一步的判断原则，就是尽量减少抢占对整个系统的影响。比如，需要抢占的Pod越少越好，需要抢占的Pod的优先级越低越好，等等。

得到了最佳的抢占结果之后，这个结果里的Node，就是即将被抢占的Node；被删除的Pod列表，就是牺牲者。所以接下来，调度器就可以真正开始抢占的操作了，这个过程，可以分为三步。

第一步，调度器会检查牺牲者列表，清理这些Pod所携带的nominatedNodeName字段。

第二步，调度器会把抢占者的nominatedNodeName，设置为被抢占的Node的名字

第三步，调度器会开启一个Goruntine，同步地删除牺牲者

而第二步对抢占者Pod的更新操作，就会触发我前面提到的“重新做人”的流程，从而让抢占者在下一个调度周期重新进行调度流程。

所以接下来，调度器就会通过正常的调度流程把抢占者调度成功。这也是为什么，前面会说调度器并不保证抢占结果：在这个正常的调度流程里，是一切皆有可能的。

不过，对于任意一个待调度Pod来说，因为有上述抢占者的存在，它的调度过程，其实是有一些特殊情况需要特殊处理的。

具体来说，在为某一对Pod和Node执行Predicates算法的时候，如果待检查的Node是一个即将被抢占的节点，即：调度队列里有nominatedNodeName字段值是该Node名字的Pod。那么，调度器就会对这个Node，将同样的Predicates算法运行两遍。

第一遍，调度器会假设上述“潜在的抢占者”已经运行在这个节点上，然后执行Predicates算法；

第二遍，调度器会正常执行Predicates算法，即：不考虑任何“潜在的抢占者”。

而只有这两遍Predicates算法都能通过时，这个Pod和Node才会被认为是可以绑定的。

不难想到，这里需要执行第一遍Predicates算法的原因，是由于InterPodAntiAffinity规则的存在。由于InterPodAntiAffinity规则关心待考察节点上所有Pod之间的互斥关系，所以我们在执行调度算法时必须考虑，如果抢占者已经存在于待考察Node上时，待调度Pod还能不能调度成功。

当然，这也意味着，我们在这一步之需要考虑那些优先级等于或者大于待调度Pod的抢占者。毕竟对于其他较低优先级Pod来说，待调度Pod总是可以通过抢占运行在待考察Node上。

而我们需要执行第二遍Predicates算法的原因，则是因为“潜在的抢占者”最后不一定会运行在待考察的Node上。


# Kubernetes容器运行时

## 45 幕后英雄： SIG-Node与CRI

在调度完成后，Kuberente就需要负责将这个调度成功的Pod，在宿主机上创建出来，并把它所定义的各个容器启动起来。这些，都是kubelet这个核心组件的主要功能。

在Kubernetes社区里，与kubelet以及容器运行时管理相关的内容，都属于SIG-Node的范畴。

> SIG即特别兴趣小组

而kubelet这个组件本身，也是Kubernetes里面第二个不可被替代的组件（第一个不可被替代的组件当然是kube-apiserver）。也就是说，无论如何，都不太建议对kubelet的代码进行大量的改动。保持kubelet跟上游基本一致的重要性，就跟保持kube-apiserver跟上游一致是一个道理。

当然，kubelet本身，也是按照“控制器”模式来工作的。它实际的工作原理，可以用如下所示的一副示意图来表示清楚。

![img](https://static001.geekbang.org/resource/image/91/03/914e097aed10b9ff39b509759f8b1d03.png)

可以看到，kubelet的工作核心，就是一个控制循环，即：SyncLoop（图中的大圆圈）。而驱动这个控制循环运行的事件，包括四种：

1、Pod更新事件
2、Pod生命周期变化
3、kubelet本身设置的执行周期
4、定时的清理事件

所以，跟其他控制器类似，kubelet启动的时候，要做的第一件事，就是设置Listers，也就是注册它所关心的各种事件的Informer。这些Informer，就是SyncLoop需要处理的数据的来源。

此外，kubelet还负责维护着很多其他的子控制循环（也就是图中的小圆圈）。这些控制循环的名字，一般被称作某某Manager，比如Volume Manager、Image Manager、Node Status Manager等等。

不难想到，这些控制循环的责任，就是通过控制器模式，完成kubelet的某项具体职责。比如，Node Status Manager，就负责响应Node的状态变化，然后将Node的状态收集起来，并通过Heartbeat的方式上报给APIServer。再比如CPU Manager，就负责维护该Node的CPU核的信息，以便在Pod通过cpuset的方式请求CPU核的时候，能够正确地管理CPU核的使用量和可用量。

那么这个SyncLoop，又是如何根据Pod对象的变化，来进行容器操作的呢？

实际上，kubelet也是通过Watch机制，监听了自己相关的Pod对象的变化。当然，这个Watch的过滤条件是该Pod的nodeName字段与自己相同。kubelet会把这些Pod的信息缓存在自己的内存里。

而当一个Pod完成调度、与一个Node绑定起来之后，这个Pod的变化就会触发kubelet在控制循环里注册的Handler，也就是上图中的HandlePods部分。此时，通过检查该Pod在kubelet内存里的状态，kubelet就能够判断出这是一个新调度过来的Pod，从而触发Handler里ADD事件对应的处理逻辑。

在具体的处理过程中，kubelet会启动一个名叫Pod Update Worker的、单独的Gorouitine来完成对Pod的处理工作。比如，如果是ADD事件的话，kubelet就会为这个新的Pod生成对应的Pod Status，检查Pod所声明使用的Volume是不是已经准备好。然后，调用下层的容器运行时（比如 Docker），开始创建这个Pod所定义的容器。

而如果是UPDATE事件的话，kublet就会根据Pod对象具体的变更情况，调用下层容器运行时进行容器的重建工作。

在这里需要注意的是，kubelet调用下层容器运行时的执行过程，并不会直接调用Docker的API，而是通过一组叫作CRI（Container Runtime Interface，容器运行时接口）的gRPC接口来间接执行的。

Docker项目风靡全球后不久，CoreOS公司就推出了rkt项目来与Docker正面竞争。在这种背景下，Kubernetes项目的默认容器运行时，自然也就成了两家公司角逐的重要战场。毋庸置疑，Docker项目必然是Kubernetes项目最依赖的容器运行时。但凭借与Google公司非同一般的关系，CoreOS公司还是在2016成功地将对rkt容器的支持，直接添加进了kubelet的主干代码里。不过，这个“赶鸭子上架”的举动，并没有为rkt项目带来更多的用户，反而给kubelet的维护人员，带来了巨大的负担。

不难想象，在这种情况下，kubelet任何一次重要功能的更新，都不得不考虑Dokcer和rkt这两种容器运行时的处理场景，然后分别更新Docker和rkt这两部分代码。

于此同时，在2016年，Kata Containers项目的前身runV项目也开始逐渐成熟，这种基于虚拟化技术的强隔离容器，与Kubernetes和Linux容器项目之间具有良好的互补关系。所以，在Kubernetes上游，对虚拟化容器的支持很快就被提上了日程。

不过，虽然虚拟化容器运行时有各种优点，但它与Linux容器截然不同的实现方式，使得它跟Kubernetes的集成工作，比rkt要复杂得多。如果此时，再把runV支持的代码也一起添加到kubelet当中，那么接下来kubelet的维护工作就可以说完全没办法正常进行了。

> 虚拟化容器（runV? Kata Container ?）、 Linux容器( docker ,ret )？
>
> 概念：
>
> Hypervisor： 用来实现硬件资源的虚拟化 ，一台物理机，通过Hypervisor变成几台虚拟机
>
> Kvm: hyperviso的一种，基于kernal内核实现
>
> Docker： 一种容器的实现方式，本质使用的是Linux cgroup
>
> k8s : 用来管理容器，比如管理 docker container，kata container, rkt container .....
>
> 发展历程：
>
> 1、Linux cgroup实现容器的方式共享内核，有潜在的安全问题。因此有需求是需要通过虚拟机来隔离内核。
>
> 2、虚拟机一般通过hypervisor来对硬件资源进行虚拟化后实现。
>
> 3、为了通过虚拟机来隔离内核，出现了基于内核的虚拟机（**K**ernel-based **V**irtual **M**achine，缩写为**KVM**），而KVM本质上是一种hyperviso。因此能够实现在硬件上，直接通过KVM虚拟出来几台虚拟机，再在虚拟机上运行容器，达到容器+隔离内核的目标
>
> 4、为了更好的管理容器，出现了k8s。
>
> 关系：
>
> Hypervisor是一类软件的总称，这类软件通常用来对硬件资源进行虚拟化。KVM就是hyperviso的一种。 Docker是一种软件的实现方式，
>
> 可以运行在虚拟机或者物理机上，因此通过KVM产生的虚拟机上通常会运行一个或多个docker 容器。 k8s用来管理所有的容器（几百上前台虚拟机或物理机上的容器）

所以，在2016年，SIG-Node决定开始解决上述问题。而解决办法也很容易想到，那就是把kubelet对容器的操作，统一地抽象成一个接口。这样，kubelet就只需要跟这个接口打交道了。而作为具体的容器项目，比如Docker、rkt、runV，它们就只需要自己提供一个该接口的实现，然后对kubelet暴露gRPC服务即可。这一层统一的容器操作接口，就是CRI。而有了CRI之后，Kubernetes以及kubelet本身的架构，就可以用如下所示的一副示意图来描述。

![img](https://static001.geekbang.org/resource/image/51/fe/5161bd6201942f7a1ed6d70d7d55acfe.png)

可以看到，当Kubernetes通过编排能力创建了一个Pod之后，调度器会为这个Pod选择一个具体的节点来运行。这时候，kubelet当然就会通过前面讲解过的SyncLoop来判断需要执行的具体操作，比如创建一个Pod 。那么此时，kubelet实际上就会调用一个叫做GenericRuntime的通用组件来发起CRI请求。如果你使用的容器项目是Docker的话，那么负责响应这个请求的就是一个叫做dockershim的组件。它会把CRI请求里的内容拿出来，然后组装成Docker API请求发给 Docker Daemon。

需要注意的是，在Kubernetes目前的实现里，dockershim依然是kubelet代码的一部分。当然，在将来，dockershim肯定会被从kubelet里移出来，设置直接被废弃掉。

而更普遍的场景，就是你需要在每台宿主机上单独安装一个负责响应CRI的组件，这个组件，一般被称作CRI shim。顾名思义，CRI shim的工作，就是扮演kubelet与容器项目之间的“垫片”（shim）。所以它的作用非常单一，那就是实现CRI规定的每个接口，然后把具体的CRI请求“翻译”成对后端容器项目的请求或者操作。

## 46 解读CRI与容器运行时

有了CRI之后，Kubernetes的架构图如下所示：

![img](https://static001.geekbang.org/resource/image/70/38/7016633777ec41da74905bfb91ae7b38.png)

CRI机制能够发挥作用的核心，就在于每一种容器项目现在都可以自己实现一个CRI shim，自行对CRI请求进行处理。这样，Kubernetes就有了一个统一的容器抽象层，使得下层容器运行时可以自由地对接进入Kubernetes当中。所以说，这里的CRI shim，就是容器项目的维护者们自由发挥的“场地”了。而除了dockershim之外，其他容器运行时的CRI shim，都是需要额外部署在宿主机上的。

举个例子，CNCF里的containerd项目，就可以提供一个典型的CRI shim的能力，即：将Kubernetes发出的CRI请求，转换成对containerd的调用，然后创建出runC容器。而runC项目，才是负责执行我们前面讲解过的设置容器Namespace、Cgroups和chroot等基础操作的组件。所以，这几层的组合关系，可以用如下所示的示意图来描述。

![img](https://static001.geekbang.org/resource/image/62/3d/62c591c4d832d44fed6f76f60be88e3d.png)

而作为CRI shim，containerd对CRI的具体实现，又是怎么样的呢？

下面这幅示意图，就展示了CRI里主要的待实现接口。

![img](https://static001.geekbang.org/resource/image/f7/16/f7e86505c09239b80ad05aecfb032e16.png)

具体地说，我们可以把CRI分为两组：
1、第一组，是RuntimeService。它提供的接口，主要是跟容器相关的操作。比如，创建和启动容器、删除容器、执行exec命令等等。
2、第二组，则是ImageService。它提供的接口，主要是容器镜像相关的操作，比如拉取镜像、删除镜像等等。

接下来主要讲解一下RuntimeService部分。在第一部分， CRI设计的一个重要原则，就是确保这个接口本身，只关注容器，不关注Pod。这样做的原因，也很容易理解。
1、Pod是Kubernetes的编排概念，而不是容器运行时的概念。所以，我们就不能假设所有下层容器项目，都能够暴露出可以直接映射Pod的API
2、如果CRI里引入了关于Pod的概念，那么接下来只要Pod API对象的字段发生变化，那么CRI就很有可能需要变更。而在Kubernetes开发的前期，Pod对象的变化还是比较频繁的，但对于CRI这样的标准接口来说，这个变更频率就有点麻烦了。

所以，在CRI的设计里，并没有一个直接创建Pod或者启动Pod的接口。不过，相信你也已经注意到，CRI里还是有一组叫做RunPodSandBox的接口的。这个PodSandbox，对应的并不是Kubernetes里的Pod API对象，而只是抽取了Pod里的一部分与容器运行时相关的字段，比如HostName、DnsConfig、CgroupParent等。所以说，PodSandBox 这个接口描述的，其实是Kuberentes 将Pod这个概念映射到容器运行时层面所需要的字段，或者说是一个Pod对象子集。而作为具体的容器项目，你就需要决定如何使用这些字段来实现一个Kubernetes期望的Pod模型。这里的原因，可以用入戏啊所示的示意图来表示清楚。

![img](https://static001.geekbang.org/resource/image/d9/61/d9fb7404c5dc9e0b5c902f74df9d7a61.png)

比如，当我们执行kubectl run创建了一个名叫foo的、包括了A、B两个容器的Pod之后。这个Pod的信息最后来到kubelet，kubelet就会按照图中所示的顺序来调用CRI接口。

在具体的CRI shim中，这些接口的实现是可以完全不同的。比如，如果是Docker项目，dockershim就会创建出一个名叫foo的Infra容器（pause容器），用来“hold”住整个Pod的Network Namespace。而如果是基于虚拟化技术的容器，比如Kata Containers项目，它的CRI实现就会直接创建出一个轻量级虚拟机来充当Pod。此外，需要注意的是，在RunPodSandBox这个接口的实现中，你还需要调用networkPlugin.SetUpPod(...)来为这个SandBox设置网络。这个SetUpPod(...)方法，实际上就在执行CNI插件里的add(...)方法。

接下来，kubelet继续调用CreateContainer和StartContainer接口来创建和启动容器A、B。对应到dockershim里，就是直接启动A，B两个Docker容器。所以最后，宿主机上会出现三个Docker容器组成这一个Pod。而如果是Kata Containers的话，CreateContainer和StartContainer接口的实现，就只会在前面创建的轻量级虚拟机里创建A、B容器对应的 Mount Namespace。所以，最后在宿主机上，只会有一个叫做foo的轻量级虚拟机在运行。关于Kata Containers或者gVisor这种所谓的安全容器项目，后一篇文章再介绍。

除了上述对容器生命周期的实现之外，CRI shim还有一个重要工作，就是如何实现exec、logs等接口。这些接口跟前面的操作有一个很大的不同，就是这些gRPC接口调用期间，kubelet需要跟容器项目维护一个长连接来传输数据。这种API，我们就称之为Streaming API。

CRI shim里对Streaming API的实现，依赖于一套独立的Streaming Server机制。这一部分原理，我们可以用如下所示的示意图来描述

![img](https://static001.geekbang.org/resource/image/a8/ef/a8e7ff6a6b0c9591a0a4f2b8e9e9bdef.png)

可以看到，当我们对一个容器执行kubectl exec命令的时候，这个请求首先交给API Server，然后API Server就会调用kubelet的exec API。

这时，kubelet就会调用CRI的exec接口，而负责响应这个接口的，自然就是具体的CRI shim。但是在这一步，CRI shim并不会直接去调用后端的容器项目（比如Docker）来进行处理，而只会返回一个URL给kubelet。这个URL，就是该CRI shim对应的Streaming Server的地址和端口。而kubelet在拿到这个URL之后，就会把它以Redirect的方式返回给API Server。所以这时候，API Server就会通过重定向来向Streaming Server发起真正的/exec请求，与它建立长连接。

当然，这个Streaming Server本身，是需要通过使用SIG-Node为你维护的Streaming API 库来实现的。并且，Streaming Server会在CRI shim启动时就一起启动。此外，Stream Server这一部分具体怎么实现，完全可以由CRI shim的维护者自行决定。比如，对于Docker项目来说，dockershime就是直接调用Docker的exec API来作为实现的。




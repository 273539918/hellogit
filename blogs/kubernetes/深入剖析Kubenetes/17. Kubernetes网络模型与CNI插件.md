# Kubernetes网络容器

## 34. Kubernetes网络模型与CNI网络插件 

Kubernetes通过一个叫做CNI的接口，维护了一个单独的网桥来代替docker0。这个网桥的名字就叫作：CNI网桥，它在宿主机上的设备名称默认是：cni0。

以Flannel的VXLAN模式为例，在Kubernetes环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0网桥被替换成了CNI网桥而已，如下所示：

![img](https://static001.geekbang.org/resource/image/9f/8c/9f11d8716f6d895ff6d1c813d460488c.jpg)

在这里， Kubernetes为Flannel分配的子网范围是10.244.0.0/16。这个参数可以在部署的时候指定，比如：

```
$ kubeadm init --pod-network-cidr=10.244.0.0/16
```

也可以在部署完成后，通过修改kube-controller-manager的配置文件来指定。

这时候，假设Infra-container-1要访问Infra-container-2（也就是Pod-1要访问Pod-2），这个IP包的源地址就是10.244.0.2，目的IP地址是10.244.1.3。而此时，Infra-container-1里的eth0，同样是以Veth Pair的方式连接在Node1的cni0网桥上。所以这个IP包就会经过cni0网桥出现在宿主机上。

此时，Node1上的路由表，如下所示：

```
# 在Node 1上
$ route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
...
10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0
10.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.1
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0
```

因为我们的IP包的目的地址是10.244.1.3，只能匹配到10.244.1.0对应的这条路由规则。可以看到，这条规则指定了本机的flannel.1设备进行处理。并且，flannel.1在处理完后，要将IP包转发到网关（Gateway），正是“隧道”另一端的VTEP设备，也就是Node2的flannel.1设备。所以，接下来的流程，就跟上一篇文章中介绍过的Flannel VXLAN模式完全一样了。

需要注意的是，CNI网桥只是接管所有CNI插件负责的、即Kubernetes创建的容器（Pod），而此时，如果你用docker run单独启动一个容器，那么Docker项目还是会把这个容器连接到docker0网桥上。所以这个容器的IP地址，一定是属于docker0网桥的172.17.0.0/16网段。

Kubernetes之所有要设置这样一个与docker0网桥功能几乎一样的CNI网桥，主要原因包括两个方面：
1) 一方面，Kubernetes项目并没有使用Docker的网络模型（CNM），所以它并不希望、也不具备配置docker0网桥的能力
2)另一方面，这还与Kubernetes如何配置Pod，也就是Infra容器的Network Namespace密切相关

我们知道，Kubernetes创建一个Pod的第一步，就是创建并启动一个Infra容器，用来“hold”住这个Pod的Network Namespace。所以，CNI的设计思想，就是：Kubernetes在启动Infra容器之后，就可以直接调用CNI网络插件，为这个Infra容器的Network Namespace，配置符合预期的网络栈。

我们在部署Kubernetes的时候，有一个步骤是安装Kubernetes-cni包，它的目的就是在宿主机上安装CNI插件所需的基础可执行文件。在安装完成后，你可以在宿主机的/opt/cni/bin目录下看到它们，如下所示：

```

$ ls -al /opt/cni/bin/
total 73088
-rwxr-xr-x 1 root root  3890407 Aug 17  2017 bridge
-rwxr-xr-x 1 root root  9921982 Aug 17  2017 dhcp
-rwxr-xr-x 1 root root  2814104 Aug 17  2017 flannel
-rwxr-xr-x 1 root root  2991965 Aug 17  2017 host-local
-rwxr-xr-x 1 root root  3475802 Aug 17  2017 ipvlan
-rwxr-xr-x 1 root root  3026388 Aug 17  2017 loopback
-rwxr-xr-x 1 root root  3520724 Aug 17  2017 macvlan
-rwxr-xr-x 1 root root  3470464 Aug 17  2017 portmap
-rwxr-xr-x 1 root root  3877986 Aug 17  2017 ptp
-rwxr-xr-x 1 root root  2605279 Aug 17  2017 sample
-rwxr-xr-x 1 root root  2808402 Aug 17  2017 tuning
-rwxr-xr-x 1 root root  3475750 Aug 17  2017 vlan
```

从这些二进制文件中，我们可以看到，如果要实现一个给Kubernetes用的容器网络方案，其实需要做两部分工作，以Flannel项目为例：
1） 首先，实现这个网络方案本身。比如，flanneld进程的主要逻辑，创建和配置flanneld.1设备、配置宿主机路由、配置ARP和FDB表里的信息等
2）实现该网络方案对应的CNI插件。这一部分主要需要做的，就是配置Infra容器里面的网络栈，并把它连接在CNI网桥上

在宿主机上安装flanneld（网络方案本身），flanneld启动后会在每台宿主机上生成对应的CNI配置文件（它其实是一个ConfigMap），从而告诉Kubernetes，这个集群要使用Flannel作为容器网络方案。配置文件如下

```

$ cat /etc/cni/net.d/10-flannel.conflist 
{
  "name": "cbr0",
  "plugins": [
    {
      "type": "flannel",
      "delegate": {
        "hairpinMode": true,
        "isDefaultGateway": true
      }
    },
    {
      "type": "portmap",
      "capabilities": {
        "portMappings": true
      }
    }
  ]
}
```

dockershim（docker的CRI实现）会把这个CNI配置文件加载起来，并且把列表里的第一个插件、也就是flannel插件，设置为默认插件。而在后面的执行过程中，flannel和portmap插件会按照定义顺序被调用，从而依次完成“配置容器网络”和“配置端口映射”这两步操作。

当kubelet组件需要创建Pod的时候，它第一个创建的一定是Infra容器。所以在这一步，dockershim就会先调用Docker API创建并启动Infra容器，紧接着执行一个叫做SetUpPod的方法。这个方法的作用就是：为CNI插件准备参数，然后调用CNI插件为Infra容器配置网络。这里要调用的CNI插件，就是/opt/cni/bin/flannel；而Flannel CNI插件最终会调用CNI Bridget插件，也就是执行: /opt/cni/bin/bridge二进制文件。CNI Bridget会创建Veth Pair，一端在容器一端在cni网桥上。然后CNI Bridget会为容器的eth0添加IP地址，并为容器设置默认路由，还会为CNI网桥添加IP地址。在执行完上述操作之后，CNI插件会把容器的IP地址等信息返回给dockershim，然后被kubelet添加到Pod的Status字段。



## 35 . 解读Kubernets三层网络方案

除了前面介绍的UDP模式和VXLAN模式之外，还有一种纯三层网络方案，比如Flannel的host-gw模式和Calico项目。

### 35.01 Flannel host-gw模式

host-gw模式的工作原理如图所示：

![host-gw工作原理图](/Users/canghong/Documents/hellogit/blogs/kubernetes/深入剖析Kubenetes/host-gw工作原理图.png)

假设现在，Node1的Infra-container-1的10.244.0.2想要访问Node2的Infra-container-2的10.244.1.3。当你设置Flannel使用host-gw模式之后，Flanneld会在宿主机上创建这样一条规则，以Node1为例：

```
$ ip route
...
10.244.1.0/24 via 10.168.0.3 dev eth0
```

这条路由规则的含义是：目的IP地址属于10.244.1.0/24 网段的IP包，应该经过本机的eth0设备发出去（即 dev eth0）；并且，它的下一条地址（next-hop）是10.168.0.3（即 via 10.168.0.3）。一旦配置了下一跳地址，那么接下来，当IP包从网络层进入数据链路层封装成帧的时候，eth0设备就会把下一跳的MAC地址填充进去。显然，这个MAC地址，正式Node2的MAC地址。这样，这个数据帧就会从Node1通过宿主机的二层网络顺利到达Node2。

可以看到，host-gw模式的工作原理，其实就是将每个Flannel子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成该子网对应的IP地址。而Flannel子网和主机信息，都保存在Etcd中。Flannel只需要WATCH这些数据变化，然后实时更新路由表即可。当然，host-gw模式能够正常工作的核心，就在于IP包在封装成帧发出去的时候，会使用路由表里的“下一跳”来设置目的MAC地址。这样，它就会通过二层网络到达目的宿主机。所以说，Flannel host-gw模式必须要求集群宿主机之间是二层连通的。

需要注意的是，宿主机之间二层不连通的情况也是广泛存在的。比如，宿主机分布在了不同（VLAN）里。但是，在一个Kubernetes集群里，宿主机之间必须可以通过IP地址进行通信，也就是说至少是三层可达的。当然，“三层可达”也可以通过为几个子网设置三层转发来实现。

### 35.02 Calico

而在容器生态中，要说到像 Flannel host-gw 这样的三层网络方案，我们就不得不提到这个领域里的“龙头老大”Calico 项目了。

实际上，Calico项目提供的网络解决方案，与Flannel的host-gw模式，几乎是完全一样的。也就是说，Calico也会在每台宿主机上，添加一个格式如下所示的路由规则：

```
<目的容器IP地址段> via <网关的IP地址> dev eth0
```

其中，网关的IP地址，正是目的容器所在宿主机的IP地址。而正如前面所述，这个三层网络方案得以正常工作的核心，是为每个容器的IP地址，找到它所对应的，“下一跳”的网关

不过，不同于Flannel通过Etcd和宿主机上的flanneld来维护路由信息的做法，Calico项目使用了一个“重型武器”来自动地在整个集群中分发路由信息。这个“重型武器”，就是BGP。BGP的全称是Border Gateway Protocol，即：边界网关协议。它是一个Linux内核原生就支持的、专门用在大规模数据中心里维护不同的“自治系统”之间路由信息的、务中心的路由协议。用一个例子进行说明：

![BGP-自治系统](/Users/canghong/Documents/hellogit/blogs/kubernetes/深入剖析Kubenetes/BGP-自治系统.jpg)

在这个图中，我们有两个自治系统（Autonomous System，简称AS）：AS 1和 AS 2。而所谓的一个自治系统，指的是一个组织管辖下的所有IP和路由器的全体。你可以把它想象成一个小公司里所有主机和路由器。在正常情况下，自治系统之间不会有任何“来往”。

但是，如果这样两个自治系统里的主机，要通过IP地址直接进行通信，我们就必须使用路由器把这两个自治系统连接起来。比如，AS 1里面的主机10.10.0.2，要访问AS 2里面的主机172.17.0.3的话。它发出的IP包，就会先到达自治系统AS 1上的路由器 Router1。而在此时，Route 1的路由表里，有这样一条规则，即：目的地址是172.17.0.2 包，应该经过Route 1的 C接口，发往网关Route2（即：自治系统 AS2上的路由器）。所以IP包就会到达Router2上，然后经过Route2的路由表，从B接口出来到达目的主机172.17.0.3。

像上面这样负责把自治系统连接在一起的路由器，我们就把它形象地称为：边界网关。它跟普通路由器的不同之处在于，它的路由表里拥有其他自治系统里的主机路由信息。但是，如果依靠人工来对边界网关的路由表进行配置和维护，那是不现实的。而这种情况下，BGP大显身手的时候就到了。

在使用了BGP之后，你就可以认为，在每个边界网关上都会运行着一个小程序，它们会将各自的路由表信息，通过TCP传输给其他的边界网关。而其他边界网关上的这个小程序，则会对收到的这些数据进行分析，然后将需要的信息添加到主机的路由表里。这样，上图中Route2的路由表里，就会自动出现10.10.0.2和10.10.0.3对应的路由规则了。所以说，所谓BGP，就是在大规模网络中实现节点路由信息共享的一种协议。

而BGP的这个能力，正好可以取代Flannel维护主机上路由表的功能。

除了对路由信息的维护方式之外，Calico项目与Flannel的host-gw模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。这时候，Calico的工作方式，可以用一副示意图来描述，如下所示：

![Calico工作原理](/Users/canghong/Documents/hellogit/blogs/kubernetes/深入剖析Kubenetes/Calico工作原理.jpg)

其中的绿色实线标出的路径，就是一个IP包从Node1上的Container1，到达Node2上的Container4的完整路径。可以看到，Calico的CNI插件会为每个容器设置一个Veth Pari设备，然后把其中的一端放置在宿主机上（它的名字以cali前缀开头）。此外，由于Calico没有使用CNI的网桥模式，Calico的CNI插件还需要在宿主机上为每个容器的Veth Pair设备配置一条路由规则，用于接收传入的IP包。比如，宿主机Node2上的Container4对应的路由规则，如下所示：

```
10.233.2.3 dev cali5863f3 scope link
```

即；发往10.233.2.3的IP包，应该进入cali5863f3设备。

有了这样的Veth Pair设备之后，容器发出的IP包就会经过Veth Pair设备出现在宿主机上。然后，宿主机网络栈就会根据路由规则的下一跳IP地址，把它们转发给正确的网关。接下来的流程就跟Flannel host-gw模式完全一致了。其中，这里最核心的“下一跳”路由规则，就是由Calico的Felix进程负责维护的。这些路由规则信息，则是通过BGP Client也就是BIRD组件，使用BGP协议传输而来的。而这些通过BGP协议传输的消息，你可以简单地理解为如下格式：

```
[BGP消息]
我是宿主机192.168.1.3
10.233.2.0/24网段的容器都在我这里
这些容器的下一跳地址是我
```

不难发现，Calico项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网路，互相之间通过BGP协议交换路由规则。这些节点，我们称为BGP Peer。

需要注意的是，Calico维护的网路在默认配置下，是一个被称为“Node-to-Node Mesh”的模式。这时候，每台宿主机上的BGP Client都需要跟其他所有节点的BGP Client进行通信以便交换路由信息。但是，随着节点数量N的增加，这些连接的数量就会以N平方的规模快速增长，从而给集群本身的网络带来巨大的压力。所以，Node-to-Node Mesh模式一般推荐用在少于100个节点的集群里。而在更大规模的集群中，你需要用到的是一个叫做Route Reflector的模式。在这种模式下，Calico会指定一个或几个专门的节点，来负责跟所有节点建立BGP连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。这些专门的节点，就是所谓的Route Reflector节点，它们实际上扮演了“中间代理”的角色，从而把BGP连接的规模控制在N的数量级上。

此外，我们在前面提到过，Flannel host-gw模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于Calico来说，也同样存在。举个例子，假如我们有两台处于不同子网的宿主机Node1和Node2，对应的IP地址分别是192.168.1.2和192.168.2.2。需要注意的是，这两台机器通过路由器实现了三层转发，所以这两个IP地址之间是可以相互通信的。而我们现在的需求，还是Container1要访问Container4。按照我们前面的讲述，Calico会尝试在Node上添加如下所示的一条路由规则：

```
10.233.2.0/16 via 192.168.2.2 eth0
```

但是，这时候问题就来了。上面这条规则里的下一跳地址是192.168.2.2，可以是它对应的Node2跟Node1却根本不在一个子网里，没办法通过二层网络把IP包直接发送到下一跳地址（eth0的路由信息里找不到192.168.2.2的MAC地址）。在这种情况下，你就需要为Calico打开IPIP模式，IPIP模式如下图所示：

![Calico IPIP模式工作原理](/Users/canghong/Documents/hellogit/blogs/kubernetes/深入剖析Kubenetes/Calico IPIP模式工作原理.jpg)

在Calico的IPIP模式下，Felix进程在Node1上添加的路由规则，会稍有不同，如下所示：

```
10.233.2.0/24 via 192.168.2.2 tunl0
```

可以看到，尽管这条路由规则的下一跳地址仍然是Node2的IP地址，但这一次，要负责将IP包发出去的设备，变成了tunl0，注意，是T-U-N-L-0，而不是Flannel UDP模式使用的T-U-N-0（tun0），这两种设备的功能是完全不一样的。

Calico使用的这个tunl0设备，是一个IP隧道（IP tunnle）设备。

在上面的例子中，IP包进入IP隧道设备之后，就会被Linux内核的IPIP驱动接管。IPIP驱动会将这个IP包直接封装在一个宿主机网络的IP包中，如下所示：

![IPIP 封包方式](/Users/canghong/Documents/hellogit/blogs/kubernetes/深入剖析Kubenetes/IPIP 封包方式.jpg)

其中，经过封装后的新的IP包的目的地址（图5中的Outer IP Header部分），正是原IP包的下一跳地址，即Node2的IP地址：192.168.2.2。而原IP包本身，则会被直接封装成新IP包的Payload。这样，原先从容器到Node2的IP包，就被伪装成了一个从Node1到Node2的IP包（原本IP包的src是容器ip，dest也是容器ip，封装后src和desc变成了宿主机IP，伪装成从Node1发出的IP包）。由于宿主机之间已经使用路由器配置了三层转发，有就是设置了宿主机之间的“下一跳”。所以这个IP包在离开Node1之后，就可以经过路由器，最终“跳”到Node2上。这时，Node2的网络内核栈就会使用IPIP驱动进行解包，从而拿到原始的IP包。然后，原始IP包就会经过路由规则和Veth Pair设备到达目的容器内部。IPIP模式本质上也是使用了隧道。

以上就是Calco项目的工作原理了。

通过上面对Calico工作原理的讲述，你应该能发现这样一个事实：如果Calico项目能够让宿主机之间的路由设备（也就是网关），也通过BGP协议“学习”到Calico网络里的路由规则，那么容器发出的IP包，不就可以通过这些设备路由到目的宿主机了么？比如，只要在上面“IPIP示意图”中的Node1上，添加如下所示的一条路由规则： 

```
10.233.2.0/24 via 192.168.1.1 eth0
```

然后，在Router1上（192.168.1.1），添加如下所示的一条路由规则：

```
10.233.2.0/24 via 192.168.2.1 eth0
```

那么Container1发出的IP包，就可以通过两次“下一跳”，到达Router2（192.168.2.1）了。以此类推，我们可以继续在Router2上添加“下一跳”路由，最终把IP包转发到Node2上。

遗憾的是，上述流程虽然简单明了，但是在Kubernetes被广泛使用的公有云场景里，却完全不可行。这里的原因在于：公有云环境下，宿主机之间的网关，肯定不会允许用户进行干预和设置。（当然，在大多数公有云环境下，宿主机本身往往是二层连通的，所以这个需求也不强烈）。

不过，在私有部署的环境下，宿主机属于不同子网（VLAN）反而是更加常见的部署状态。这时候，想办法将宿主机网关（如：Router1）
也加入到BGP Mesh里从而避免使用IPIP，就成了一个非常迫切的需求。而在Calico项目中，它已经为你提供了两种将宿主机网关设置成BGP Peer的解决方案，这里不作介绍。



隧道技术（需要封装包和解包，因为需要伪装成宿主机的IP包，需要三层链通）：Flannel UDP / VXLAN  / Calico IPIP
三层网络（不需要封包和解封包，需要二层链通）：Flannel host-gw / Calico 


















































































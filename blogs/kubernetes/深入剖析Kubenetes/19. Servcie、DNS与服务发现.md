# Kubernetes网络容器

## 37 找到容器不容易：Service、DNS与服务发现

Kubernetes之所以需要Service，一方面是因为Pod的IP是不固定的，另一方面则是因为一组Pod实例之间总会有负载均衡的需求。一个典型的Service定义如下：

```
apiVersion: v1
kind: Service
metadata:
  name: hostnames
spec:
  selector:
    app: hostnames
  ports:
  - name: default
    protocol: TCP
    port: 80
    targetPort: 9376
```

使用selector字段来声明这个Service只代理携带了app=hostnames标签的Pod。并且，这个Service的80端口代理了Pod的9376端口。然后，我们应用的Deployment如下所示：

```

apiVersion: apps/v1
kind: Deployment
metadata:
  name: hostnames
spec:
  selector:
    matchLabels:
      app: hostnames
  replicas: 3
  template:
    metadata:
      labels:
        app: hostnames
    spec:
      containers:
      - name: hostnames
        image: k8s.gcr.io/d
        ports:
        - containerPort: 9376
          protocol: TCP
```

这个应用的功能，就是每次访问9376端口时，返回它自己的hostname。而被selector选中的Pod，就称为Service的Endpoints，你可以使用kubectl get ep命令查到它们，如下所示：

```
$ kubectl get endpoints hostnames
NAME        ENDPOINTS
hostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
```

需要注意的是，只有处于Running状态，且readinessProbe检查通过的Pod，才会出现在Service的Endpoints列表里。并且，当某一个Pod出现问题时，Kubernetes会自动把它从Service里摘除掉。而此时，通过该Service的VIP地址10.0.1.175，你就可以访问到它所代理的Pod了：

```

$ kubectl get svc hostnames
NAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
hostnames   ClusterIP   10.0.1.175   <none>        80/TCP    5s

$ curl 10.0.1.175:80
hostnames-0uton

$ curl 10.0.1.175:80
hostnames-yp2kp

$ curl 10.0.1.175:80
hostnames-bvc05
```

通过三次curl访问，印证了Service提供的是Round Robin方式的负载均衡。对于这种方式，我们称为：ClusterIP模式的Service。

你可能会比较好奇，Kubernetes里的Service究竟是如何工作的呢？实际上，Service是由kube-proxy组件，加上iptables来共同实现的。举个例子，对于我们前面创建的名叫hostnames的Service来说，一旦被它提交给Kubernetes，那么kube-proxy就可以通过Service的Informer感知到这样一个Service对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条iptables规则（你可以通过iptables-save看到它），如下所示：

```
-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment "default/hostnames: cluster IP" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3
```

可以看到，这条iptables规则的含义是：凡是目的地址是10.0.1.175、目的端口是80的IP包，都应该跳转到另一条名叫KUBE-SVC-NWV5X2332I4OT4T3的iptables链进行处理。而我们前面已经看到，10.0.1.175正式这个Service的VIP。所以这一条规则，就为这个Service设置了一个固定的入口地址。并且，由于10.0.1.175只是一条iptables规则上的配置，并没有真正的网络设备，所以你ping这个地址，是不会有任何响应的。那么，我们即将跳转到的KUBE-SVC-NWV5X2332I4OT4T3规则，又有什么作用呢？实际上，它是一组规则的集合，如下所示：

```
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR
```

可以看到，它是一组规则的集合，如下所示：

```
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3
-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment "default/hostnames:" -j KUBE-SEP-57KPRZ3JQVENLNBR
```

可以看到，这一组规则，实际上是一组随机模式（--mode random）的iptables链。而随机转发的目的地，分别是KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3、KUBE-SEP-57KPRZ3JQVENLNBR。而这三条链指向的最终目的地，其实就是这个Service代理的三个Pod。所以这一组规则，就是Service实现负载均衡的位置。需要注意的是，iptables规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的probability字段的值分别设置为1/3（0.333....）、1/2和1。这么设置的原理很简单：第一条规则被选中的概率就是1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的probability就必须设置为1/2；类似地，最后一条就必须设置为1。

通过查看上述三条链的明细，我们就很容易理解Service进行转发的具体原理了，如下所示：

```

-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.3.6:9376

-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.1.7:9376

-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment "default/hostnames:" -j MARK --set-xmark 0x00004000/0x00004000
-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment "default/hostnames:" -m tcp -j DNAT --to-destination 10.244.2.3:9376
```

可以看到，这三条链，其实是三条DNAT规则。但在DNAT规则之前，iptables对流入的IP包还设置了一个“标志”（--set-xmark）。这个“标志”的作用，在下一篇文章讲解。而DNAT规则的作用，就是在路由之前，将流入IP包的目的地址和端口，改成--to-destination所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理Pod的IP地址和端口。这样，访问Service VIP的ip包经过上述iptables处理之后，就已经变成了访问具体某一个后端Pod的IP包了。不难理解，这些Endpoints对应的iptables规则，正是kube-proxy通过监听Pod的变化事件，在宿主机生成并维护的。以上，就是Service最基本的工作原理。

此外，你可能已经听说过，Kubernetes的kube-proxy还支持一种叫做IPVS的模式。这又是怎么一回事呢？其实，通过上面的讲解，你可以看到，kube-proxy通过iptables处理Service的过程，其实需要在宿主机上设置相当多的iptables规则。而且，kube-proxy还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的。不难想到，当你的宿主机上有大量Pod的时候，成百上千条iptables规则不断刷新，会大量占用该宿主机的CPU资源，甚至会让宿主机“卡”在这个过程中。所以说，一直以来，基于iptables的Service实现，都是制约Kubernets项目承载更多量级的Pod的主要障碍。而IPVS模式的Service，就是解决这个问题的一个行之有效的方法。

IPVS模式的工作原理，其实跟iptables模式类似。当我们创建了前面的Service之后，kube-proxy首先会在宿主机上创建一个虚拟网卡（叫做:  kube-ipvs0），并为它分配Service VIP作为IP地址，如下所示：

```
# ip addr
  ...
  73：kube-ipvs0：<BROADCAST,NOARP>  mtu 1500 qdisc noop state DOWN qlen 1000
  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
  inet 10.0.1.175/32  scope global kube-ipvs0
  valid_lft forever  preferred_lft forever
```

而接下来，kube-rpoxy就会通过Linux的IPVS模块，为这个IP地址设置三个IPVS虚拟主机，并设置这三个虚拟主机之间使用轮询模式（rr）来作为负载均衡策略。我们可以通过ipvsadm查看到这个设置，如下所示：

```
# ipvsadm -ln
 IP Virtual Server version 1.2.1 (size=4096)
  Prot LocalAddress:Port Scheduler Flags
    ->  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     
  TCP  10.102.128.4:80 rr
    ->  10.244.3.6:9376    Masq    1       0          0         
    ->  10.244.1.7:9376    Masq    1       0          0
    ->  10.244.2.3:9376    Masq    1       0          0
```

可以看到，这三个IPVS虚拟机主机的IP地址和端口，对应的正式三个被代理的Pod。这时候，任何发往10.102.128.4:80的请求，就会被IPVS模块转发到某一个后端Pod上。而相比于iptables，IPVS在内核中的实现其实也是基于Netfilter的NAT模式，所以在转发这一层上，理论上IPVS并没有显著的性能提升。但是，IPVS并不需要在宿主机上为每个Pod设置iptales规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。不过需要注意的是，IPVS模块只负责上述的负载均衡和代理功能。而一个完整的Service流程正常工作所需要的包过滤、SNAT等操作，还是要靠iptables来实现。只不过，这些辅助性的iptables规则数量有限，也不会随着Pod数量的增加而增加。

在Kubernetes中，Service和Pod都会被分配对应的NDS A记录（从域名解析IP的记录）。对于ClusterIP模式的Servcie来说，它的A记录的格式是：<myservice>.<mynamespace>.svc.cluster.local。当你访问这条A记录的时候，它解析到的就是该Service的VIP地址。而对于指定了clusterIP=None的Headless Service来说，它的A记录的格式也是：<myservice>.<mynamespace>.svc.cluster.local。但是，当你访问这条A记录的时候，它返回的是所有被代理的Pod的IP地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只拿到第一个Pod的IP地址。

此外，对于ClusterIP模式的Service来说，它代理的Pod被自动分配的A记录的格式是：<pod_ip>.<myservcie>.svc.cluster.local。这条记录也指向Pod的IP地址。但如果你为Pod指定了Headless Service，并且Pod本身声明了hostname和subdomain字段，那么这时候Pod的A记录就会变成：<pod的hostname>.<subdomain>.<mynamespace>.svc.cluster.local，比如：

```

apiVersion: v1
kind: Service
metadata:
  name: default-subdomain
spec:
  selector:
    name: busybox
  clusterIP: None
  ports:
  - name: foo
    port: 1234
    targetPort: 1234
---
apiVersion: v1
kind: Pod
metadata:
  name: busybox1
  labels:
    name: busybox
spec:
  hostname: busybox-1
  subdomain: default-subdomain
  containers:
  - image: busybox
    command:
      - sleep
      - "3600"
    name: busybox
```

在上面这个Service和Pod被创建之后，你就可以通过busybox-1.default-subdomain.default.svc.cluster.local解析到这个Pod的IP地址了。

## 38 从外界连通Service与Service调试“三板斧”

通过对Service工作原理的介绍，你应该能够明白这样一个事实：Service 的访问信息在Kubernetes集群之外，其实是无效的。

这其实也很容易理解：所谓Service的访问入口，其实就是每台宿主机上由kube-proxy生成的iptables规则，以及kube-dns生成的DNS记录。而一旦离开了这个集群，这些信息对用户来说，也就自然没有作用了。

所以，在使用Kubernetes的Service时，一个必须要面对和解决的问题就是：如何从外部（Kubernetes集群之外），访问到Kubernetes里创建的Service？下面介绍的只是不同的Service类型，只要是Service，它就可以被Ingress作为backend

### 38.01 NodePort方式

目的： 外部client访问任何一台宿主机上的8080端口时，就是要访问对应Pod的80端口。

Service的例子如下：

```

apiVersion: v1
kind: Service
metadata:
  name: my-nginx
  labels:
    run: my-nginx
spec:
  type: NodePort
  ports:
  - nodePort: 8080
    targetPort: 80
    protocol: TCP
    name: http
  - nodePort: 443
    protocol: TCP
    name: https
  selector:
    run: my-nginx
```

在这个Service的定义里，我们声明它的类型是，type=NodePort。然后，我在port字段里声明了Service的8080端口代理Pod的80端口，Service的443端口代理Pod的443端口。那么这时候，要访问这个Service，你只需要访问：

```
<任何一台宿主机的IP地址>:8080
```

就可以访问某一个被代理的Pod的80端口了。

需要注意的是， 在NodePort方式下，Kubernetes会在IP包离开宿主机发往目的Pod时，对这个IP报做一次SNAT操作，将这个IP包的源地址替换成这台宿主机上的CNI网桥地址或者宿主机本身的IP地址（如果CNI网桥不存在的话）。当然，这个SNAT操作只需要对Service转发出来的IP包进行（否则，普通的IP包就被影响了）。而iptables做这个判断的依据，就是查看该IP包是否有一个“Ox4000”的“标志”。你应该还记得，这个标志正是在IP包被执行DNAT操作之前被打上去的。

为什么需要对流出的包做SNAT操作呢？这里的原理其实很简单，如下所示：

```

           client
             \ ^
              \ \
               v \
   node 1 <--- node 2
    | ^   SNAT
    | |   --->
    v |
 endpoint
```

当一个外部的client通过node2的地址访问一个Service的时候，node2上的负载均衡规则就可能把这个IP包转发给一个在node1上的Pod。这里没有任何问题。而当node1上的这个Pod处理完请求之后，它就会按照这个IP包的源地址发出回复。可是，如果没有做SNAT操作的话，这时候，被转发来的IP包的源地址就是client的IP地址。所以此时，Pod就会直接将回复发给client。对于client来说，它的请求发给了node2，收到回复的却来自node1，这个client很可能会报错。

所以，在上图中，当IP包离开node2之后，它的源IP地址就会被SNAT改成node2的CNI网桥地址或者node2自己的地址。这样，Pod在处理完成之后就会先回复给node2（而不是client），然后再由node2发送给client。当然，这也也为这这个Pod只知道该IP包来自于node2，而不是外部的client。对于Pod需要明确知道所有请求来源的场景来说，这是不可以的。

所以，这时候，你可以将Service的spec.externalTrafficPolicy字段设置为local，这样就保证了所有Pod通过Service收到请求之后，一定可以看到真正的、外部client的源地址。而这个机制的实现原理也非常简单：这时候，一台宿主机的iptables规则，会设置为之只将dIP包转发给运行在这台宿主机上的Pod。如下所示：

```

       client
       ^ /   \
      / /     \
     / v       X
   node 1     node 2
    ^ |
    | |
    | v
 endpoint
```

当然，这也意味着如果一台宿主机上，没有任何一个被代理的Pod存储，这个请求就会被DROP掉。

### 38.02 LoadBalance方式

在公有云提供的Kubernetes服务里，都使用了一个叫做CloudProvider的转接层，来跟公有云本身的API进行对接。所以，在LoadBalancer类型的Service被提交后，Kubernetes就会调用CloudProvider在公有云上为你创建一个负载均衡服务，并且把被代理的Pod的IP地址配置给负载均衡服务做后端

```

---
kind: Service
apiVersion: v1
metadata:
  name: example-service
spec:
  ports:
  - port: 8765
    targetPort: 9376
  selector:
    app: example
  type: LoadBalancer
```

### 38.03 ExternalName方式

目的：我访问一个服务，比如mysqlstream的时候，实际上访问的是my-service.default.svc.cluster.local

```

kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  type: ExternalName
  externalName: my.database.example.com
```

在上述Service的YAML文件中，我指定了一个externalName=my.database.example.com的字段。而且你应该会注意到，这个YAML文件里不需要指定selector。这时候，当你通过Service的DNS名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么Kubernetes为你返回的就是my.database.example.com。所以说，ExternalName类型的Service，其实是在kube-dns里为你添加了一条CNAME记录。这时，访问my-service.default.svc.cluster.local就和访问my.database.example.com这个域名是一个效果了。

此外，Kubernetes的Service还允许你为Service分配公有IP地址，比如下面这个例子：

```

kind: Service
apiVersion: v1
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 9376
  externalIPs:
  - 80.11.12.10
```

在上述Service中，我为它指定的externalIPs=80.11.12.10，那么此时，你就可以通过访问80.11.12.10:80访问到被代理的Pod了。不过，在这里Kubernetes要求externalIPs必须至少能够路由到一个Kubernetes节点。你可以想一想这是为什么。

### 38.04 Service调试

实际上，在理解了Kubernetes Service机制的工作原理之后，很多与Service相关的问题，其实都可以通过分析Service在宿主机对应的iptables规则（或者IPVS配置）得到解决。

比如，当你的Service没办法通过DNS访问到的时候。你就需要区分到底是Service本身的配置问题，还是集群的DNS出了问题。一个行之有效的方法，就是检查Kubernetes自己的Master节点的Service DNS是否正常：

```
# 在一个Pod里执行
$ nslookup kubernetes.default
Server:    10.0.0.10
Address 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local

Name:      kubernetes.default
Address 1: 10.0.0.1 kubernetes.default.svc.cluster.local
```

如果上面访问kubernets.default返回的值都有问题，那你就需要检查kube-dns的运行状态和日志了。否则的话，你应该去检查自己的Service定义是不是有问题。

而如果你的Service没办法通过ClusterIP访问到的时候，你首先应该检查的是这个Service是否有Endpoints：

```
$ kubectl get endpoints hostnames
NAME        ENDPOINTS
hostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376
```

需要注意的是，如果你的Pod的readniessProbe没通过，它也不会出现在Endpoints列表里。而如果Endpoints正常，那么你就需要确认kube-proxy是否在正确运行。在我们通过kubeadm部署的集群里，你应该看到kube-proxy输出的日志如下：

```

I1027 22:14:53.995134    5063 server.go:200] Running in resource-only container "/kube-proxy"
I1027 22:14:53.998163    5063 server.go:247] Using iptables Proxier.
I1027 22:14:53.999055    5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.
I1027 22:14:54.038140    5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns-tcp" to [10.244.1.3:53]
I1027 22:14:54.038164    5063 proxier.go:352] Setting endpoints for "kube-system/kube-dns:dns" to [10.244.1.3:53]
I1027 22:14:54.038209    5063 proxier.go:352] Setting endpoints for "default/kubernetes:https" to [10.240.0.2:443]
I1027 22:14:54.038238    5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master
I1027 22:14:54.040048    5063 proxier.go:294] Adding new service "default/kubernetes:https" at 10.0.0.1:443/TCP
I1027 22:14:54.040154    5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns" at 10.0.0.10:53/UDP
I1027 22:14:54.040223    5063 proxier.go:294] Adding new service "kube-system/kube-dns:dns-tcp" at 10.0.0.10:53/TCP
```

PS: 访问日志取决于您的Node OS。 在某些操作系统上，它是一个文件，例如/var/log/kube-proxy.log，或者/var/log/containers/kube-proxy-z9sjk_kube-system....log，而其他操作系统则使用journalctl访问日志。

当然，还有一种典型问题，就是Pod没办法通过Service访问到自己。这往往是因为kubelet的hairpin-mode没有被正确设置。

通过上述的讲解不难看出，所谓Service，其实就是Kubernetes为Pod分配的、固定的、基于iptables（或者IPVS）的访问入口。而这些访问入口代理的Pod信息，则来自于Etcd，由kube-proxy通过控制循环来维护。

## 39 谈谈Service与Ingress

Kubernetes中有一种全局的、为了代理不同的后端Service而设置的负载均衡服务，就是Kubernetes里的Ingress服务。所以，Ingress的功能其实很容易理解：所谓Ingress，就是Service的Service。

举个例子，假如我现在有这样一个站点：https://cafe.example.com。其中，https://cafe.example.com/coffee，对应的是“咖啡点餐系统”。而https://cafe.example.com/tea，对应的则是“茶水点餐系统”。这两个系统，分别由名叫coffee和tea这样两个Deployment来提供服务

那么现在，我如何能使用Kubernetes的Ingress来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的Deployment呢？

上述功能，在kubernetes里就需要通过Ingress对象来描述，如下所示：

```

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: cafe-ingress
spec:
  tls:
  - hosts:
    - cafe.example.com
    secretName: cafe-secret
  rules:
  - host: cafe.example.com
    http:
      paths:
      - path: /tea
        backend:
          serviceName: tea-svc
          servicePort: 80
      - path: /coffee
        backend:
          serviceName: coffee-svc
          servicePort: 80
```

在上面这个名叫cafe-ingress.yaml文件中，最值得我们关注的，是rules字段。在Kubernetes里，这个字段叫做：IngressRule。IngressRule的Key，就叫做:host。它必须是一个标准的域名格式的字符串，而不能是IP地址。而host字段定义的值，就是这个Ingress的入口。这也就意味着，当用户访问cafe.example.com的时候，实际上访问到的是这个Ingress对象。这样，Kubernetes就能使用IngressRule来对你的请求进行下一步转发。而接下来IngressRule规则的定义，则依赖于path字段，你可以简单地理解为，这里的每一个path都对应一个后端Service。所以在我们的例子里，我定义了两个path，它们分别对应coffee和tea这两个Deployment的Service (即：coffee-svc和tea-svc)。

一个Ingress对象的主要内容，实际上就是一个“反向代理”服务的配置文件的描述。而这个代理服务对应的转发规则，就是IngressRule。这就是为什么在每条IngressRule里，需要有一个host字段来作为这条IngressRule的入口，然后还需要有一系列path字段来声明具体的转发策略。这其实跟Nginx、HAProxy等项目的配置文件的写法是一致的。

而有了Ingress这样一个统一的抽象，Kubernetes的用户就无需关心Ingress的具体细节了。在实际的使用中，你只需要从社区里选择一个具体的Ingress Controller，把它部署在Kubernetes集群里即可。然后，这个Ingress Controller会根据你定义的Ingress对象，提供对应的代理能力。

接下来，我就以最常用的Nginx Ingress Controller为例，在我们前面用kubeadm部署的Bare-metal环境中，和你实践一下Ingress机制的使用过程。

部署Nginx Ingress Controller的方法非常简单，如下所示：

```
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/deploy.yaml 
```

其中，mandatory.yaml的内容如下：

```

kind: ConfigMap
apiVersion: v1
metadata:
  name: nginx-configuration
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/part-of: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
      annotations:
        ...
    spec:
      serviceAccountName: nginx-ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --publish-service=$(POD_NAMESPACE)/ingress-nginx
            - --annotations-prefix=nginx.ingress.kubernetes.io
          securityContext:
            capabilities:
              drop:
                - ALL
              add:
                - NET_BIND_SERVICE
            # www-data -> 33
            runAsUser: 33
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
            - name: http
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443
```

可以看到，在上述YAML文件中，我们定义了一个使用nginx-ingress-controller镜像的Pod。需要注意的是，这个Pod的启动命令需要使用该Pod所在的Namespace作为参数。而这个信息，当然是通过Downward API拿到的，即：Pod的env字段里的定义（env.valueFrom.fieldRef.fieldPath）。

而这个Pod本身，就是一个监听Ingress对象以及它所代理的后端Service变化的控制器。当一个新的Ingress对象由用户创建后，nginx-ingress-controller就会根据Ingress对象里定义的内容，生成一份对应的nginx配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个Nginx服务。而一旦Ingress对象被更新，nginx-ingress-controller就会更新这个配置文件。

此外，nginx-ingress-controller还允许你通过Kubernetes的ConfigMap对象来对上述Nginx配置文件进行定制。这个ConfigMap的名字，需要以参数的方式传递给nginx-ingress-controller。而你在这个ConfigMap里添加的字段，将会被合并到最后生成的Nginx配置文件当中。

可以看到，一个Nginx Ingress Controller为你提供的服务，其实是一个可以根据Ingress对象和被代理后端Service的变化，来自动进行更新的Nginx负载均衡器。

当然，为了让用户能够用到这个Nginx，我们就需要创建一个Service来把Nginx Ingress Controller管理的Nginx服务暴露出去，如下所示：

```
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml
```

我们使用的service-nodeport.yaml文件里的内容，是一个NodePort类型的Service，如下所示：

```
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx
  namespace: ingress-nginx
  labels:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
spec:
  type: NodePort
  ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    - name: https
      port: 443
      targetPort: 443
      protocol: TCP
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
```

可以看到，这个Service的唯一工作，就是将所有携带ingress-nginx标签的Pod的80和433端口暴露出去。上述操作完成后，你一定要记录下这个Service的访问入口，即：宿主机的地址和Nodeport的端口，如下所示：

```
$ kubectl get svc -n ingress-nginx
NAME                                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             NodePort    10.99.88.207    <none>        80:32529/TCP,443:31552/TCP   4m3s
ingress-nginx-controller-admission   ClusterIP   10.109.35.219   <none>        443/TCP                      4m3s
```

为了方便后续使用，我会把上述访问入口设置为环境变量：

```
$ IC_IP=10.168.0.2 # 任意一台宿主机的地址
$ IC_HTTPS_PORT=32529 # NodePort端口
```

在Ingress Controller和它所需要的Service部署完成后，我们就可以使用它了。接下来就是部署具体的Ingress。

首先，我们要在集群里部署我们的应用Pod和它们对应的Service，如下所示：

PS : 完成的文件地址： https://github.com/resouer/kubernetes-ingress/tree/master/examples/complete-example

```
$ kubectl create -f cafe.yaml
```

然后，我们需要创建Ingress所需的SSL证书（tls.crt）和密钥（tls.key），这些信息都是通过Secret对象定义好的，如下所示：

```
$ kubectl create -f cafe-secret.yaml
```

这一步完成后，我们就可以创建在本篇文章一开始定义的Ingress对象了，如下所示：

```
$ kubectl create -f cafe-ingress.yaml
```

这时候，我们就可以查看一下这个Ingress对象的信息，如下所示：

```
$ kubectl get ingress
NAME           CLASS    HOSTS              ADDRESS   PORTS     AGE
cafe-ingress   <none>   cafe.example.com             80, 443   7s

$ kubectl describe ingress cafe-ingress
Name:             cafe-ingress
Namespace:        default
Address:          11.88.191.46
Default backend:  default-http-backend:80 (<error: endpoints "default-http-backend" not found>)
TLS:
  cafe-secret terminates cafe.example.com
Rules:
  Host              Path  Backends
  ----              ----  --------
  cafe.example.com
                    /tea      tea-svc:80 (10.36.0.4:80,10.39.0.2:80,10.39.0.3:80)
                    /coffee   coffee-svc:80 (10.39.0.1:80,10.44.0.2:80)
Annotations:        <none>
Events:
  Type    Reason  Age   From                      Message
  ----    ------  ----  ----                      -------
  Normal  CREATE  30s   nginx-ingress-controller  Ingress default/cafe-ingress
  Normal  UPDATE  8s    nginx-ingress-controller  Ingress default/cafe-ingress
```

可以看到，这个Ingress对象最核心的部分，正是Rules字段。其中，我们定义的Host是cafe.example.com，它有两条转发规则（Path），分别转发给tea-svc和coffee-svc。

接下来，我们就可以通过访问这个Ingress的地址和端口，访问到我们前面部署的应用了，比如，当我们访问https://cafe.example.com:443/coffee时，应该是coffee这个Deployment负责响应我的请求，我们可以来尝试一下：

```
$ curl --resolve cafe.example.com:$IC_HTTPS_PORT:$IC_IP https://cafe.example.com:$IC_HTTPS_PORT/coffee --insecure
Server address: 10.244.1.56:80
Server name: coffee-7dbb5795f6-vglbv
Date: 03/Nov/2018:03:55:32 +0000
URI: /coffee
Request ID: e487e672673195c573147134167cf898
```

我们可以看到，访问这个URL得到的返回信息是：Server name: coffee-7dbb5795f6-vglbv。这正是coffee这个Deployment的名字。

而当我们访问https://cafe.example.com:433/tea的时候，则应该是tea这个Deployment负责响应我的请求（Server name: tea-7d57856c44-lwbnp），如下所示：

```

$ curl --resolve cafe.example.com:$IC_HTTPS_PORT:$IC_IP https://cafe.example.com:$IC_HTTPS_PORT/tea --insecure
Server address: 10.244.1.58:80
Server name: tea-7d57856c44-lwbnp
Date: 03/Nov/2018:03:55:52 +0000
URI: /tea
Request ID: 32191f7ea07cb6bb44a1f43b8299415c
```

可以看到，Nginx Ingress Controller为我们创建的ingress，已经成功地将请求转发给了对应的后端Service。

此外，Ingress Controller允许你设置一条默认规则，任何匹配path失败的请求，就会被转发到默认规则上。这样我们就可以通过部署一个专门的Pod，来为用户返回自定义的404页面了。
















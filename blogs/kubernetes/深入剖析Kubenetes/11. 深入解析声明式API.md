# 容器编排与Kubernetes作业管理

## 24 深入解析声明式API（一）：API对象的奥秘

在Kubernetes项目中，一个API对象在Etcd里的完整资源路径，是由：Group（API组）、Version（API版本）和Resource（API资源类型）三个部分组成的。比如，现在我要声明创建一个CronJob对象，那么我的YAML文件的开始部分会这么写:

```
apiVersion: batch/v2alpha1
kind: CronJob
...
```

在这个YAML文件中，"CronJob"就是这个API对象的资源类型（ Resource），"batch"就是它的组（Group），v2alpha就是它的版本（Version）。

那么，Kubernetes是如何对Resource、Group和Version进行解析，从而在Kubernetes项目里找到Cronjob对象的定义呢?1：首先，Kubernetes会匹配API对象的组，需要明确的是，对于Kubernetes里的核心API对象，比如：Pod、Node等，是不需要Group的（即：它们的Group是“”）。所以，对于这些API对象来说，Kubernetes会直接在/api这个层级进行下一步的匹配过程。而对于CronJob等非核心API对象来说，Kubernetes就必须在/apis这个层级里查找它对应的Group，进而根据“batch”这个Group的名字，找到/apis/batch。
2：然后，Kubernetes会进一步匹配到API对象的版本号。对于CronJob这个API对象来说，Kubernetes在batch这个Group下，匹配到的版本号就是v2alpha1。
3：最后，Kubernetes会匹配API对象的资源类型。在前面匹配到正确的版本之后，Kubernetes就知道，我要创建原来是一个 /apis/batch/v2alpha1下的CronJob对象。这时候，APIServer就可以继续创建这个CronJob对象了。

### 24.01 APIServer工作流程

首先，当我们发起了创建CronJob的Post请求之后，我们编写的YAML的信息就被提交给了APIServer。
APIServer的第一个功能，就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等。

然后，请求会进入MUX和Routes流程。MUX和Routes是APIServer完成URL和Handler绑定的场所。而APIServer的Handler要做的事情，就是按照刚刚介绍的匹配过程，找到对应的CronJob类型定义

接着，APIServer最重要的职责就来了：根据这个CronJob类型定义，使用用户提交的YAML文件里的字段，创建一个CronJob对象。而之在这个过程中，APIServer会进行一个Convert工作，即：把用户提交的YAML文件，转换成一个叫作Super Version的对象，它正是该API资源类型所有版本的字段全集，这样用户提交的不同版本的YAML文件、就都可以用这个Super Version对象来进行处理了。

接下来，APIServer会先后进行Admission()和Validation()操作。比如，我们在上一篇文章中提到的Adminssion Controller和Initializer，就属于Admission的内容。而Validation，则负责验证这个对象里的各个字段是否合法。这个被验证过的API对象，都保存了APIServer里一个叫作Registry的数据结构中。也就是说，只要一个API对象的定义能够在Registry里查到，它就是一个有效的Kubernetes API对象。

最后，APIServer会把验证过的API对象转换成用户最初提交的版本，进行序列化操作，并调用Etcd的API把它保存起来。

### 24.02 CRD插件机制

CRD的全程是Custom Resource Definition。它指的是，允许用户在Kubernetes中添加一个跟Pod、Node类似的、新的API资源类型，即：自定义API资源。

举个例子，我现在要为Kubernetes添加一个名叫Network的API资源类型。这个Network对象的YAML文件，名叫example-network.yaml，它的内容如下：

```
apiVersion: samplecrd.k8s.io/v1
kind: Network
metadata:
  name: example-network
spec:
  cidr: "192.168.0.0/16"
  gateway: "192.168.0.1"
```

可以看到： 组 samplecrd.k8s.io、版本 v1、资源类型 Network。

接下来，Kubernetes如何知道这个API的存在呢？其实，上面的这个YAML文件，就是一个具体的“自定义API资源”实例，也叫CR（Custom Resource）。而为了能够让Kubernetes认识这个CR，你就需要让Kubernetes明白这个CR的宏观定义是什么，也就是CRD（Custom Resource Definition）。

所以，接下来，编写一个CRD的YAML文件，它的名字叫做network.yaml，内容如下所示：

```
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: networks.samplecrd.k8s.io
spec:
  group: samplecrd.k8s.io
  version: v1
  names:
    kind: Network
    plural: networks
  scope: Namespaced
```

可以看到，在这个CRD中，我指定了“group：samplecrd.k8.io” “verison:v1” 这样的API信息，也指定了这个CR的资源类型叫做Network，复述（plural）是networks。然后，还声明了它的scope是Namespace，即：我们定义的这个Network是一个属于Namespace的对象，类似于Pod。

这就是一个Network API资源类型的API部分的宏观定义了。所以这时候，Kubernetes就能够认识和处理所有声明了API类型是"samplecrd.k8s.io/v1/network"的YAML文件了。

接下来，还需要让Kubernetes“认识”这种YAML文件里描述的“网络”部分，比如“cidr”（网段），“gateway”（网关）这些字段的含义。

首先，在GOPATH下，创建一个结构如下的项目（https://github.com/resouer/k8s-controller-custom-resource）：

```
$ tree $GOPATH/src/github.com/<your-name>/k8s-controller-custom-resource
.
├── controller.go
├── crd
│   └── network.yaml
├── example
│   └── example-network.yaml
├── main.go
└── pkg
    └── apis
        └── samplecrd
            ├── register.go
            └── v1
                ├── doc.go
                ├── register.go
                └── types.go
```

其中，pkg/apis/samplecrd就是API组的名字，v1是版本，而v1下面的types.go文件定义了Network对象的完整描述。

然后，我在pkg/apis/samplecrd目录下创建了一个register.go文件，用来放置后面要用到的全局变量。这个文件的内容如下所示：

```
package samplecrd

const (
 GroupName = "samplecrd.k8s.io"
 Version   = "v1"
)
```

接着，我需要在pkg/apis/samplecrd目录下添加一个doc.go文件（Golang的文档源文件）。这个文件里的内容如下所示：

```
// +k8s:deepcopy-gen=package

// +groupName=samplecrd.k8s.io
package v1

```

在这个文件中，你会看到+<tag_name>[=value]格式的注释，这就是kubernetes进行代码生成要用的Annotation风格的注释。其中，+k8s:deepcopy-gen=package的意思是，请为整个v1包里的所有类型定义自动生成DeepCopy方法；而+groupName=samplecrd.k8s.io，则定义了这个包对应的API组的名字。

可以看到，这些定义在doc.go文件的注释，起到的是全局的代码生成控制的作用，所以这被称为Global Tags。

接下来，我需要添加types.go文件。顾名思义，它的作用就是定义一个Network类型到底有哪些字段（比如，spce字段里的内容）。这个文件的主要内容如下所示：

```
package v1
...
// +genclient
// +genclient:noStatus
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// Network describes a Network resource
type Network struct {
 // TypeMeta is the metadata for the resource, like kind and apiversion
 metav1.TypeMeta `json:",inline"`
 // ObjectMeta contains the metadata for the particular object, including
 // things like...
 //  - name
 //  - namespace
 //  - self link
 //  - labels
 //  - ... etc ...
 metav1.ObjectMeta `json:"metadata,omitempty"`
 
 Spec networkspec `json:"spec"`
}
// networkspec is the spec for a Network resource
type networkspec struct {
 Cidr    string `json:"cidr"`
 Gateway string `json:"gateway"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// NetworkList is a list of Network resources
type NetworkList struct {
 metav1.TypeMeta `json:",inline"`
 metav1.ListMeta `json:"metadata"`
 
 Items []Network `json:"items"`
}
```

从上面这部分代码里，可以看到Network类型定义方法跟标准的Kubernetes对象一样，都包括了TypeMeta（API元数据）和ObjectMeta（对象元数据）字段。而其中的Spec字段，就是需要我们自己定义的部分。所以，在networkspec里，我定义了Cidr和Gateway两个字段。其中，每个字段最后面的部分比如json："cidr"，指的就是这个字段被转换成JSON格式之后的名字，也就是YAML文件里的字段名字。

此外，除了定义Network类型，你还需要定一个Networklist类型，用来描述一组Network对象应该包括哪些字段。之所以需要这样一个类型，是因为在Kubernetes中，获取所有X对象的List()方法，返回值都是List类型，而不是X类型的数组。

同样地，在Network和NetworkList类型上，也有代码生成注释。其中，+genclient的意思是：请为下面这个API资源类型生成对应的Client代码（这个Client，后面会介绍）。而+genclient:noStatus的意思是：这个API资源类型定义里，没有Status字段。否则，生成的Client就会自动带上UpdateStatus方法。

如果你定义的类型包括了Status字段的话，就不需要这句+genclient:noStatus注释了。比如下面这个例子：

```
// +genclient

// Network is a specification for a Network resource
type Network struct {
 metav1.TypeMeta   `json:",inline"`
 metav1.ObjectMeta `json:"metadata,omitempty"`
 
 Spec   NetworkSpec   `json:"spec"`
 Status NetworkStatus `json:"status"`
}
```

而由于我在Global Tags里已经定义了为所有类型生成DeepyCopy方法，所以这里就不需要再显示地加上+k8s:deepcopy-gen=true了。当然，这也就意味着你可以用+k8s：deepcopy-gen=false来阻止为某些类型生成DeepCopy。

你可以已经注意到，在这两个类型上面还有一句 +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object的注释。它的意思是，请在生成DeepyCopy的时候，实现Kubernetes提供的runtime.Object接口。否则，在某些版本的Kubernetes里，你的这个类型定义会出现编译错误。这是一个固定操作，记住即可。

不过，你或许会有顾虑：这些代码生成注释这么灵活，我该如何掌握？其实，上面讲诉的内容，已经足以应付99%的场景了。

最后，我需要再编写一个pkg/apis/samplecrd/v1/register.go文件。在前面对APIServer工作原理的讲解中，我已经提到，"registry"的作用就是注册一个类型（Type）给APIServer。其中，Network资源类型在服务器端注册的工作，APIServer会自动帮我们完成。但与之对应的，我们还需要让客户端也能“知道”Network资源类型的定义。这就需要我们在项目里添加一个register.go文件。它最主要的功能，就是定义了如下所示的addKnownTypes()方法：

```
package v1
...
// addKnownTypes adds our types to the API scheme by registering
// Network and NetworkList
func addKnownTypes(scheme *runtime.Scheme) error {
 scheme.AddKnownTypes(
  SchemeGroupVersion,
  &Network{},
  &NetworkList{},
 )
 
 // register the type in the scheme
 metav1.AddToGroupVersion(scheme, SchemeGroupVersion)
 return nil
}
```

有了这个方法，Kubernetes就能在后面生成客户端的时候，“知道”Network以及NetworkList类型的定义了。像上面这种register.go文件里的内容其实是非常固定的，你以后可以直接使用我提供的这部分代码做模版，然后把其中的资源类型、GroupName和Version替换成你自己的定义即可。

这样，Network对象的定义工作就全部完成了。可以看到，它其实定义了两部分内容：
第一部分是，自定义资源类型的API描述，包括：组（Group）、版本（Version）、资源类型（Resource）等。
第二部分是，自定义资源类型的对象描述，包括：Spec、Status等

接下来，就要使用Kubernetes提供的代码生成工具，为上面定义的Network资源类型自动生成clientset、informer和lister。其中，clientset就是操作Network对象所需要使用的客户端，而 informer和lister这两个包的主要功能在后面的文章中介绍。这个代码生成工具名叫k8s.io/code-generator，使用方法如下所示：

```
# 代码生成的工作目录，也就是我们的项目路径
$ ROOT_PACKAGE="github.com/resouer/k8s-controller-custom-resource"
# API Group
$ CUSTOM_RESOURCE_NAME="samplecrd"
# API Version
$ CUSTOM_RESOURCE_VERSION="v1"

# 安装k8s.io/code-generator
$ export GOPATH=/root/go
$ go get -u k8s.io/code-generator/...
$ cd $GOPATH/src/k8s.io/code-generator

# 执行代码自动生成，其中pkg/client是生成目标目录，pkg/apis是类型定义目录
$ ./generate-groups.sh all "$ROOT_PACKAGE/pkg/client" "$ROOT_PACKAGE/pkg/apis" "$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION"
```

代码生成工作完成之后，我们再查看一下这个项目的目录结构：

```
$ cd /root/go/src/github.com/resouer/k8s-controller-custom-resource
$ tree
.
├── controller.go
├── crd
│   └── network.yaml
├── example
│   └── example-network.yaml
├── main.go
└── pkg
    ├── apis
    │   └── samplecrd
    │       ├── constants.go
    │       └── v1
    │           ├── doc.go
    │           ├── register.go
    │           ├── types.go
    │           └── zz_generated.deepcopy.go
    └── client
        ├── clientset
        ├── informers
        └── listers
```

其中，pkg/apis/samplecrd/v1下面的zz-generated.deepcopy.go文件，就是自动生成的DeepCopy代码文件。而整个client目录，以及下面的三个包（clients、informers、listers），都是Kubernetes为Network类型生成的客户端库，这些库会再后面编写自定义控制器的时候用到。

有了上述这些内容，就可以在Kubernetes集群里创建一个Network类型的API对象了。我们来实验一下。

首先，使用network.yaml文件，在Kubernetes中创建Network对象的CRD（Custom Resource Definition）：

```
$ kubectl apply -f crd/network.yaml
customresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created
```

这个操作，就是告诉Kubernetes，我现在要添加一个自定义的API对象。而这个对象的API信息，正是network.yaml里定义的内容。我们可以通过kubectl get命令，查看这个CRD：

```
$kubectl get crd
NAME                                             CREATED AT
networks.samplecrd.k8s.io                        2020-05-10T14:03:34Z
```

通过这个操作，你就在Kubernetes集群里创建了一个Network对象，它的API资源路径是samplecrd.k8s.io/v1/networks。

```
$cat example-network.yaml
apiVersion: samplecrd.k8s.io/v1
kind: Network
metadata:
  name: example-network
spec:
  cidr: "192.168.0.0/16"
  gateway: "192.168.0.1"
  
$kubectl apply -f example-network.yaml
$kubectl get network
NAME              AGE
example-network   2s
```

这时候，你还可以通过kubectl describe命令，看到这个Network对象的细节：

```
$ kubectl describe network example-network
Name:         example-network
Namespace:    default
Labels:       <none>
...API Version:  samplecrd.k8s.io/v1
Kind:         Network
Metadata:
  ...
  Generation:          1
  Resource Version:    468239
  ...
Spec:
  Cidr:     192.168.0.0/16
  Gateway:  192.168.0.1
```



## 24 深入解析声明式API（二）：编写自定义控制器

前面的内容介绍了Kubernetes中声明式API的实现原理，并且通过一个添加Network对象的实例，讲诉了再Kubernetes里添加API资源的过程。这里继续完成剩下的一半工作，即：为Network这个自定义API对象编写一个自定义控制器（Custom Controller）。

总体来说，编写自定义控制器代码的过程包括：编写main函数、编写自定义控制器的定义，以及编写控制器里的业务逻辑三个部分。

### 24.01 自定义控制器原理

在Kubernetes项目中，一个自定义控制器的工作原理，可以用下面这样一幅流程图来表示：

![image-20200511193810291](/Users/canghong/Library/Application Support/typora-user-images/image-20200511193810291.png)

这个控制器要做的第一件事，是从Kubernetes的APIServer里获取它所关心的对象，也就是我定义的Network对象。这个操作，依靠的是一个叫作Informer（可以翻译为：通知器）的代码库完成的。Informer与API对象是一一对应的，所以我传递给自定义控制器的，正是一个Network对象的Informer（Network Informer）。在创建这个Informer工厂的时候，需要给它传递一个networkClient，事实上，Network Informer正是使用这个networkClient，跟APIServer建立了连接。不过，真正负责维护这个连接的，则是Informer所使用的Reflector包。更具体地说，Reflector使用的是一种叫做ListAndWatch的方法，来“获取”并“监听”这些Network对象实例的变化。

在ListAndWatch机制下，一旦APIServer端有新的Network实例被创建、删除或者更新，Reflector都会收到“事件通知”。这时，该事件及它对应的API对象这个组合，就被称为增量（Delta），它会被放进一个Delta FIFO Queue（即：增量先进先出队列）中。

而另一方面，Informe会不断地从这个Delta FIFO Queue里读取（Pop）增量。每拿到一个增量，Informer就会判断这个增量里的事件类型，然后创建或者更新本地对象缓存。这个缓存，在Kuberenetes里一般叫做Store。比如，如果事件类型是Added（添加对象），那么Informer就会通过一个叫做Indexer的库把这个增量里的API对象保存在本地缓存中，并为它创建索引。相反，如果增量的事件类型是Deleted（删除对象），那么Informer就会从本地缓存中删除这个对象。这个同步本地缓存的工作，是Informer的第一个职责，也是它最重要的职责。

而Informer的第二个职责，则是根据这些事件的类型，触发事先注册好的ResourceEventHandler。这些Handler，需要在创建控制器的时候注册给它对应的Informer。就是我们下面介绍的 “ 24.03编写自定义控制器”



### 24.02 main函数

首先，来编写这个自定义控制器的main函数。 main函数的主要工作就是，定义并初始化一个自定义控制器（Custom Controller），然后启动它。这部分的主要内容如下所示：

```
func main() {
  ...
  
  cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)
  ...
  kubeClient, err := kubernetes.NewForConfig(cfg)
  ...
  networkClient, err := clientset.NewForConfig(cfg)
  ...
  
  networkInformerFactory := informers.NewSharedInformerFactory(networkClient, ...)
  
  controller := NewController(kubeClient, networkClient,
  networkInformerFactory.Samplecrd().V1().Networks())
  
  go networkInformerFactory.Start(stopCh)
 
  if err = controller.Run(2, stopCh); err != nil {
    glog.Fatalf("Error running controller: %s", err.Error())
  }
}
```

可以看到，这个main函数主要通过三步完成了初始化并启动一个自定义控制器的工作。
第一步：main函数根据我提供的Master配置（APIServer 的地址和kubeconfig的路径），创建一个Kubernetes的client（KubeClient）和Network对象的client（networkClient）。如果没有提供master配置，main函数会直接使用一种名叫InClusterConfig的方式来创建这个client。这个方式，会假设你的自定义控制器是以Pod的方式运行在Kubernetes集群里的，而Kubernetes里所有的Pod都会以Volume的方式自动挂载Kubernetes的默认ServiceAccount。所以，这个控制器就会直接使用默认ServiceAccount数据卷里的授权信息，来访问APIServer。
第二步：main函数为Network对象创建一个叫作InformerFactory（即：networkInformerFactory）的工作，并使用它生成一个Network对象的Informer，传递给控制器
第三步：main函数启动上述的Informer，然后执行controller.Run，启动自定义控制器

### 24.03 编写自定义控制器

编写自定义控制器，它的主要内容如下：

```
func NewController(
  kubeclientset kubernetes.Interface,
  networkclientset clientset.Interface,
  networkInformer informers.NetworkInformer) *Controller {
  ...
  controller := &Controller{
    kubeclientset:    kubeclientset,
    networkclientset: networkclientset,
    networksLister:   networkInformer.Lister(),
    networksSynced:   networkInformer.Informer().HasSynced,
    workqueue:        workqueue.NewNamedRateLimitingQueue(...,  "Networks"),
    ...
  }
    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
    AddFunc: controller.enqueueNetwork,
    UpdateFunc: func(old, new interface{}) {
      oldNetwork := old.(*samplecrdv1.Network)
      newNetwork := new.(*samplecrdv1.Network)
      if oldNetwork.ResourceVersion == newNetwork.ResourceVersion {
        return
      }
      controller.enqueueNetwork(new)
    },
    DeleteFunc: controller.enqueueNetworkForDelete,
 return controller
}

```

我前面在main函数里创建了两个client（kubeclientset和networkclientset，然后在这段代码里，使用这两个client和前面创建的Informer，初始化了自定义控制器。

值得注意的是，在这个自定义控制器里，我还设置了一个工作队列（work queue），它正是处于示意图中间位置的WorkQueue。这个工作队里的作用是，负责同步Informer和控制循环之间的数据。

然后，我为networkInformer注册了三个Handler（AddFunc、UpdateFunc和DeleteFunc），分别对应API对象的“添加”“更新”和“删除”事件。而具体的处理操作，都是将该事件对应的API对象加入到工作队列中。需要注意的是，实际入队的并不是API对象本身，而是它们的Key，即：该API对象的<namespace>/<name>。而我们后面即将编写的控制循环，则会不断地从这个工作队列里拿到这些Key，然后开始执行真正的控制逻辑。

综合上面的讲诉，你现在应该就能明白，所谓Informer，其实就是一个带有本地缓存和索引机制的、可以注册EventHandler的client。它是自定义控制器跟APIServer进行数据同步的重要组件。更具体地说，Informer通过一种叫做ListAndWatch的方法，把APIServer中的API对象缓存在了本地，并负责更新和维护这个缓存。其中，ListAndWatch方法的含义是：首先，通过APIServer的LIST API “获取”所有最新版本的API对象；然后，再通过WATCH API来“监听”所有这些API对象的变化。而通过监听到的事件变化，Informer就可以实时地更新本地缓存，并且调用这些事件对应的EventHandler了。

此外，在这个过程中，每经过resyncPeriod指定的时间，Informer维护的本地缓存，都会使用最近一次LIST返回的结果强制更新一次，从而保证缓存的有效性。在Kubernetes中，这个缓存强制更新的操作就叫做：resync。需要注意的是，这个定时resync操作，也会触发Informer注册的“更新事件”。但此时，这个“更新”事件对应的Network对象实际上并没有发生变化，即：新、旧两个Network对象的ResourceVersion是一样的。在这种情况下，Informeriu不需要对这个更新事件再做进一步处理了。这也是为什么我在上面的UpdateFunc方法里，先判断了一下新、旧两个Network对象的版本（ResourceVersion）是否发生变化，然后才开始进行的入队操作。

以上，就是Kubernetes中的Informer库的工作原理了。接下来，我们就来到了示意图中最后面的控制循环（Control Loop）部分，也正是我在main函数最后调用controller.Run()启动的“控制循环”。它的主要内容如下所示：

```
func (c *Controller) Run(threadiness int, stopCh <-chan struct{}) error {
 ...
  if ok := cache.WaitForCacheSync(stopCh, c.networksSynced); !ok {
    return fmt.Errorf("failed to wait for caches to sync")
  }
  
  ...
  for i := 0; i < threadiness; i++ {
    go wait.Until(c.runWorker, time.Second, stopCh)
  }
  
  ...
  return nil
}
```

可以看到，启动控制循环的逻辑非常简单：首先，等待Informer完成一次本地缓存的数据同步操作；然后，直接通过goroutine启动一个（或者并发启动多个）“无限循环”的任务。而这个“无限循环”任务的每一个循环周期，执行的正是我们真正关心的业务逻辑。

### 24.04 编写自定义控制器的业务逻辑

自定义控制器的业务逻辑代码如下所示：

```
func (c *Controller) runWorker() {
  for c.processNextWorkItem() {
  }
}

func (c *Controller) processNextWorkItem() bool {
  obj, shutdown := c.workqueue.Get()
  
  ...
  
  err := func(obj interface{}) error {
    ...
    if err := c.syncHandler(key); err != nil {
     return fmt.Errorf("error syncing '%s': %s", key, err.Error())
    }
    
    c.workqueue.Forget(obj)
    ...
    return nil
  }(obj)
  
  ...
  
  return true
}

func (c *Controller) syncHandler(key string) error {

  namespace, name, err := cache.SplitMetaNamespaceKey(key)
  ...
  
  network, err := c.networksLister.Networks(namespace).Get(name)
  if err != nil {
    if errors.IsNotFound(err) {
      glog.Warningf("Network does not exist in local cache: %s/%s, will delete it from Neutron ...",
      namespace, name)
      
      glog.Warningf("Network: %s/%s does not exist in local cache, will delete it from Neutron ...",
    namespace, name)
    
     // FIX ME: call Neutron API to delete this network by name.
     //
     // neutron.Delete(namespace, name)
     
     return nil
  }
    ...
    
    return err
  }
  
  glog.Infof("[Neutron] Try to process network: %#v ...", network)
  
  // FIX ME: Do diff().
  //
  // actualNetwork, exists := neutron.Get(namespace, name)
  //
  // if !exists {
  //   neutron.Create(namespace, name)
  // } else if !reflect.DeepEqual(actualNetwork, network) {
  //   neutron.Update(namespace, name)
  // }
  
  return nil
}
```

可以看到，在这个执行周期里（processNextWorkItem），我们首先从工作队列里出队（workqueue.Get）了一个成员，也就是一个Key（Network对象的：namspace/name）。然后，在syncHandler方法中，我们使用这个key，尝试从Informer维护的缓存中拿到它所对应的Network对象。可以看到，在这里，我使用了networksLister来尝试获取这个key对应的Network对象。这个操作，其实就是在访问本地缓存的索引。实际上，在Kubernetes的源码中，你会经常看到控制器从各种Lister里获取对象，比如：podLister、nodeLister等等，它们使用的都是Informer和缓存机制。

而如果控制循环从缓存中拿不到这个对象（即：networkLister返回了IsNotFound错误），那就意味着这个Network对象的Key是通过前面“删除”事件添加进工作队列的。所以，尽管队列里有这个Key，但是对应的Network对象已经被删除了。这时候，我就需要调用Neutron的API，把这个Key对应的Neutron网络从真实的集群里删除掉。

而如果能够获取到对应的Network对象，我就可以执行控制器模式里的对比“期望状态”和“实际状态”的逻辑了。其中，自定义控制器“千辛万苦”拿到的这个Network对象，正是APIServer里保存的“期望状态”，即：用户通过YAML文件提交到APIServer里的信息。当然，在我们的例子里，它已经被Informer缓存在了本地。那么，“实际状态”又从哪里来呢？当然是来自于实际的集群了，我们的控制循环需要通过Neutron API来查询实际的网络情况。比如，可以先通过Neutron来查询这个Network对象对应的真实网络是否存在。

- 如果不存在，这就是一个典型的“期望状态”与“实际状态”不一致的情形。这时，我就需要使用这个Network对象里的信息（比如：CIDR和Gateway），调用Neutron API来创建真实的网络
- 如果存在，那么，我就要读取这个真实网络的信息，判断它是否跟Network对象里的信息一致，从而决定我是否要通过Neutron来更新这个已经存在的真实网络。

这样，我就通过对比“期望状态”和“实际状态”的差异，完成了一次调谐（Reconcile）的过程。至此，一个完整的自定义API对象和它所读经的自定义控制器，就编写完毕了。

### 24.05 运行实例

接下来运行一下这个项目，可以直接使用编译好的二进制文件，编译并启动这个项目的具体流程如下所示：

```
# Clone repo
$ git clone https://github.com/resouer/k8s-controller-custom-resource
$ cd k8s-controller-custom-resource

### Skip this part if you don't want to build
# Install dependency
$ go get github.com/tools/godep
$ godep restore
# Build
$ go build -o samplecrd-controller .

$ ./samplecrd-controller -kubeconfig=$HOME/.kube/config -alsologtostderr=true
I0515 20:48:26.044091  106657 controller.go:84] Setting up event handlers
I0515 20:48:26.050710  106657 controller.go:113] Starting Network control loop
I0515 20:48:26.050718  106657 controller.go:116] Waiting for informer caches to sync
I0515 20:48:26.150815  106657 controller.go:121] Starting workers
I0515 20:48:26.150833  106657 controller.go:127] Started workers
E0515 20:49:00.668589  106657 reflector.go:251] github.com/resouer/k8s-controller-custom-resource/pkg/client/informers/externalversions/factory.go:117: Failed to watch *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io)
...

```

可以看到，自定义控制器启动后，一开始会报错。这是因为，此时Network对象的CRD还没有被创建出来，所以Informer去APIServer里“获取”（List）Network对象时，并不能找到Network这个API资源类型的定义，即：

```
Failed to list *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io)
```

所以，接下来就需要创建Network对象的CRD，这个操作在上面已经介绍过。

```
$ kubectl apply -f crd/network.yaml
```

这时候，你就会看到控制器的日志恢复了正常，控制循环启动成功：

```
...
I0915 12:50:29.051630   27159 controller.go:116] Waiting for informer caches to sync
...
I0915 12:52:54.346854   25245 controller.go:121] Starting workers
I0915 12:52:54.346914   25245 controller.go:127] Started workers
```

接下来，就可以进行Network对象的增删改查操作了。
首先，创建一个Network对象：

```
$ cat example/example-network.yaml 
apiVersion: samplecrd.k8s.io/v1
kind: Network
metadata:
  name: example-network
spec:
  cidr: "192.168.0.0/16"
  gateway: "192.168.0.1"
  
$ kubectl apply -f example/example-network.yaml 
network.samplecrd.k8s.io/example-network created

```

这时候，查看一下控制器的输出：

```
...
I0515 20:50:29.524216  106657 controller.go:229] [Neutron] Try to process network: &v1.Network{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"example-network", GenerateName:"", Namespace:"default", SelfLink:"/apis/samplecrd.k8s.io/v1/namespaces/default/networks/example-network", UID:"de9b7684-9f9b-4926-9a7d-ebf7deb615ad", ResourceVersion:"13303405", Generation:1, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63725143829, loc:(*time.Location)(0x18addc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"samplecrd.k8s.io/v1\",\"kind\":\"Network\",\"metadata\":{\"annotations\":{},\"name\":\"example-network\",\"namespace\":\"default\"},\"spec\":{\"cidr\":\"192.168.0.0/16\",\"gateway\":\"192.168.0.1\"}}\n"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.NetworkSpec{Cidr:"192.168.0.0/16", Gateway:"192.168.0.1"}} ...
I0515 20:50:29.524346  106657 controller.go:183] Successfully synced 'default/example-network'
I0515 20:50:29.524388  106657 event.go:221] Event(v1.ObjectReference{Kind:"Network", Namespace:"default", Name:"example-network", UID:"de9b7684-9f9b-4926-9a7d-ebf7deb615ad", APIVersion:"samplecrd.k8s.io/v1", ResourceVersion:"13303405", FieldPath:""}): type: 'Normal' reason: 'Synced' Network synced successfully
...

```

可以看到，我们上面创建example-network的操作，触发了EventHandler的“添加”事件，从而被放进了工作队列。紧接着，控制循环就从队列里拿到了这个对象，并且打印出了正在“处理”这个Network对象的日志。可以看到，这个Network的ResourceVersion，也就是API对象的版本号，是13303405，而它的Spec字段的内容，跟我提交的YAML文件一模一样，比如，它的CIDR网段是：192.168.0.0/16。

这时候，我来修改一下这个YAML文件的内容，如下所示：

```
$cat example/example-network.yaml 
apiVersion: samplecrd.k8s.io/v1
kind: Network
metadata:
  name: example-network
spec:
  cidr: "192.168.1.0/16"
  gateway: "192.168.1.1"
```

可以看到，我把这个YAML文件里的CIDR和Gateway字段修改成了192.168.1.0/16网段。然后执行了kubectl apply命令来提交这次更新，如下所示： 

```
$ kubectl apply -f example/example-network.yaml 
network.samplecrd.k8s.io/example-network configured

```

这时候，我们就可以观察一下控制器的输出：

```
...
I0515 20:52:07.636974  106657 controller.go:229] [Neutron] Try to process network: &v1.Network{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"example-network", GenerateName:"", Namespace:"default", SelfLink:"/apis/samplecrd.k8s.io/v1/namespaces/default/networks/example-network", UID:"de9b7684-9f9b-4926-9a7d-ebf7deb615ad", ResourceVersion:"13303699", Generation:2, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63725143829, loc:(*time.Location)(0x18addc0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string{"kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"samplecrd.k8s.io/v1\",\"kind\":\"Network\",\"metadata\":{\"annotations\":{},\"name\":\"example-network\",\"namespace\":\"default\"},\"spec\":{\"cidr\":\"192.168.1.0/16\",\"gateway\":\"192.168.0.1\"}}\n"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, Spec:v1.NetworkSpec{Cidr:"192.168.1.0/16", Gateway:"192.168.0.1"}} ...
I0515 20:52:07.637100  106657 controller.go:183] Successfully synced 'default/example-network'
I0515 20:52:07.637120  106657 event.go:221] Event(v1.ObjectReference{Kind:"Network", Namespace:"default", Name:"example-network", UID:"de9b7684-9f9b-4926-9a7d-ebf7deb615ad", APIVersion:"samplecrd.k8s.io/v1", ResourceVersion:"13303699", FieldPath:""}): type: 'Normal' reason: 'Synced' Network synced successfully
```

可以看到，这一次，Informe注册的“更新”事件被触发，更新后Network对象的Key被添加到了工作队列之中。所以，接下来控制循环从工作队列里拿到的Network对象，与前一个对象是不通的：它的ResourceVersion的值变成了13303699；而Spec里的字段，则变成了192.168.1.0/16网段。

最后，我再把这个对象删除掉：

```
$ kubectl delete -f example/example-network.yaml
```

这一次，在控制器的输出里，我们就可以看到，Informer注册的“删除”事件被触发，并且控制循环“调用”Neutron API“删除”了真实环境里的网络。这个输出如下所示：

```
I0515 20:52:07.637120  106657 event.go:221] Event(v1.ObjectReference{Kind:"Network", Namespace:"default", Name:"example-network", UID:"de9b7684-9f9b-4926-9a7d-ebf7deb615ad", APIVersion:"samplecrd.k8s.io/v1", ResourceVersion:"13303699", FieldPath:""}): type: 'Normal' reason: 'Synced' Network synced successfully
W0515 20:53:56.872168  106657 controller.go:212] Network: default/example-network does not exist in local cache, will delete it from Neutron ...
I0515 20:53:56.872189  106657 controller.go:215] [Neutron] Deleting network: default/example-network ...
I0515 20:53:56.872197  106657 controller.go:183] Successfully synced 'default/example-network'
```

以上，就是编写和使用自定义控制器的全部流程了。实际上，这套流程不仅可以用在自定义API资源上，也完全可以用在Kubernetes原生的默认API对象上。比如，我们在main函数里，除了创建一个Network Informer外，还可以初始化一个Kubernetes默认API对象的Informer工厂，比如Deployment对象的Informer。这个具体做法如下所示：

```
func main() {
  ...
  
  kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)
  
  controller := NewController(kubeClient, exampleClient,
  kubeInformerFactory.Apps().V1().Deployments(),
  networkInformerFactory.Samplecrd().V1().Networks())
  
  go kubeInformerFactory.Start(stopCh)
  ...
}
```

在这段代码中，我们首先使用Kubernetes的client（kubeClient）创建了一个工厂；然后，我用跟Network类似的处理方法，生成了一个Deployment Informer；接着，我把Deployment Informer传递给了自定义控制器；当然，我也要调用start方法来启动这个Deployment Informer。而又了这个Deployment Informer后，这个控制器也就持有了所有Deployment对象的信息。接下来，它既可以通过deploymentInformer.Lister()来获取Etcd里的所有Deployment对象，也可以为这个Deployment Informer注册具体的Handler。更重要的是，这就使得在这个自定义控制器里面，我可以通过对自定义API对象和默认API对象进行协同，从而实现更加复杂的编排功能。

### 24.06 小结

所谓的Informer，就是一个自带缓存和索引机制，可以触发Handler的客户端库。这个本地缓存在Kubernetes中一般被称为Store，索引一般被称为Index。

Informer使用了Reflector包，它是一个可以通过ListAndWatch机制获取并监视API对象变化的客户端封装。Reflector和Informer之间，用到了一个“增量先进先出队列”进行协同。而Informer与你要编写的控制循环之间，则使用了一个工作队列来进行协同。在实际应用中，除了控制循环之外的所有代码，实际上都是Kubernetes为你自动生成的，即：pkg/client/{informers,listers,clientset}里的内容。而这些自动生成的代码，就为我们提供了一个可靠而高效地获取API对象“期望状态”的编程库。所以，接下来，作为开发者，你就只需要关注如何拿到“实际状态”，然后如何拿它去跟“期望状态”做对比，从而决定解析来要做的业务逻辑即可。


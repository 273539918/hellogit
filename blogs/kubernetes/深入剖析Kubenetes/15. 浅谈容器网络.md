# Kubernetes网络容器

## 学习总结

容器直接使用宿主机网络栈的方式，虽然简单且能提供比较好的网络性能，但是不可避免地引入了共享网络资源的问题，比如端口冲突。

容器使用Network Namespace进行隔离可以避免共享网络资源的问题，但该如何和其他容器进程交互？  
-> 联系到多台物理机的交互是通过网线，共同连接到一个交换机来实现物理机的相互连通。 
-> 到了容器，也有一个类似交换机的虚拟交换机设备，网桥（Bridge），凡是连接到这个网桥上的容器进程，都可以通过它来相互通信 
->  现在容器有了虚拟的交换机了，但是容器连接到“交换机”上的“网线”呢？  通过 Veth Pair来充当网线的功能，它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现，作为网线的两端。
-> 因此： 在默认情况下，被限制在Network Namespace里的容器进程实际上是通过Veth Pari设备+宿主机网桥的方式，实现了跟其他容器的数据交换

现在解决了容器和同一个宿主机下其他容器交互的问题了，那么如果容器想要和其他宿主机上的容器交互，要如何实现？
-> 答案和单个宿主机上容器互访相似，我们用软件或应将的方式创建一个整个集群“公用”的网桥，然后把集群里所有容器都连接到这个网桥上

-> 构建这种容器网络的核心在于需要在已有的宿主机网络上，再通过软件构件一个覆盖在已有宿主机网络之上的、可以把所有容器连接在一起的虚拟网络。所以，这种技术就被称为：Overlay Network（覆盖网络）。





## 32. 浅谈容器网络

Linux容器能看见的“网络栈”，实际上是被隔离在它自己的Network Namespace当中的。需要指出的是，作为一个容器，你可以直接声明使用宿主机网络，即：不开启Network Namespace，比如：

```
$ docker run –d –net=host --name nginx-host nginx
```

在这种情况下，这个容器启动后，直接监听的就是宿主机的80端口。像这样直接使用宿主机网络栈的方式，虽然可以为容器提供良好的网络性能，但也会不可避免地引入共享网络资源的问题，比如端口冲突。









### 32.01 同一宿主机的不同容器如何进行网络通信

所以，在大多数情况下，我们都希望容器进程能使用自己的Network Namespace里的网络栈，即：拥有属于自己的IP地址和端口。这时候，一个显而易见的问题就是：这个被隔离的容器进程，该如何跟其他Network Namespace里的容器进程进行交互呢？

通常情况下，如果想要实现两台主机之间的通信，最直接的办法，就是把它们用一根网线连接起来；而如果你想要实现多台主机之间的通信，那就需要用网线，把它们连接在一台交换机上。在Linux中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）。它是一个工作在数据链路层的设备，主要功能是根据MAC地址学习来将数据包转发到网桥的不同端口上。

为了实现Linux容器之间的通信，Docker项目会默认在宿主机上创建一个名叫docker0的网桥，凡是连接在docker0网桥上的容器，就可以通过它来进行通信。可是，我们又该如何把这些容器“连接”到docker0网桥上呢？

这时候，我们就需要使用一种名叫Veth Pair的虚拟设备了。Veth Pari设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的Network Namesapce里。这就使得Veth Pari常常被用作连接不同Network Namespace的“网线”。

比如，现在我们启动了一个名叫nginx-1的容器：

```
$ docker run --name nginx-1 -d nginx:latest
```

然后进入这个容器中查看一下它的网络设备：

```

# 在宿主机上
$ docker exec -it nginx-1 /bin/bash
# 在容器里
root@2b3c181aecf1:/# ifconfig
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 172.17.0.2  netmask 255.255.0.0  broadcast 0.0.0.0
        inet6 fe80::42:acff:fe11:2  prefixlen 64  scopeid 0x20<link>
        ether 02:42:ac:11:00:02  txqueuelen 0  (Ethernet)
        RX packets 364  bytes 8137175 (7.7 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 281  bytes 21161 (20.6 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
        inet 127.0.0.1  netmask 255.0.0.0
        inet6 ::1  prefixlen 128  scopeid 0x10<host>
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 0  bytes 0 (0.0 B)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 0  bytes 0 (0.0 B)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
        
$ route
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
default         172.17.0.1      0.0.0.0         UG    0      0        0 eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 eth0
```

可以看到，这个容器里有一张叫作eth0的网卡，它正是一个Veth Pari设备在容器里的这一端。

通过route命令查看nginx-1容器的路由表，我们可以看到，这个eth0网卡是这个容器里的默认路由设备；所有对172.17.0.0/16网段的请求，也会被交给eth0来处理（第二条 172.17.0.0 路由规则）。而这个Veth Pair设备的另一端，则在宿主机上。可以通过查看宿主机的网络设备看到它，如下所示：

```

# 在宿主机上
$ ifconfig
...
docker0   Link encap:Ethernet  HWaddr 02:42:d8:e4:df:c1  
          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0
          inet6 addr: fe80::42:d8ff:fee4:dfc1/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:309 errors:0 dropped:0 overruns:0 frame:0
          TX packets:372 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0 
          RX bytes:18944 (18.9 KB)  TX bytes:8137789 (8.1 MB)
veth9c02e56 Link encap:Ethernet  HWaddr 52:81:0b:24:3d:da  
          inet6 addr: fe80::5081:bff:fe24:3dda/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:288 errors:0 dropped:0 overruns:0 frame:0
          TX packets:371 errors:0 dropped:0 overruns:0 carrier:0
 collisions:0 txqueuelen:0 
          RX bytes:21608 (21.6 KB)  TX bytes:8137719 (8.1 MB)
          
$ brctl show
bridge name bridge id  STP enabled interfaces
docker0  8000.0242d8e4dfc1 no  veth9c02e56
```

通过ifconfig命令的输出，你可以看到，nginx-1容器对应的Veth Pair设备，在宿主机上是一张虚拟网卡。它的名字叫做veth9c02e56。并且，通过brctl show的输出，你可以看到这张网卡被“插”在了docker0上。

这时候，如果我们再在这台宿主机上启动另一个Docker容器，比如nginx-2:

```
$ docker run –d --name nginx-2 nginx
$ brctl show
bridge name bridge id  STP enabled interfaces
docker0  8000.0242d8e4dfc1 no  veth9c02e56
       vethb4963f3
```

你就会发现一个新的、名叫vethb4963f3的虚拟网卡，也被“插”在了docker0网桥上。这时候，如果你在nginx-1容器里ping一下nginx-2容器的IP地址（172.17.0.3），就会发现同一宿主机上的两个容器默认就是相互连通的。

这其中的原理其实非常简单，当你在nginx-1容器里访问nginx-2容器的IP地址（比如ping 172.17.0.3）的时候，这个目的IP地址会匹配到nginx-1容器里的第二条路由规则。可以看到，这条路由规则的网卡（Gateway）是0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的IP包，应该经过本机的eth0网卡，通过二层网络直接发往目的主机。而要通过二层网络达到nginx-2容器，就需要有172.17.0.3这个IP地址对应的MAC地址。所以，nginx-1容器的网络协议栈，就需要通过eth0网卡发送一个ARP广播，来通过IP地址查找对应的MAC地址。

我们前面提到过，这个eth0网卡，是一个Veth Pari，它的一端在这个nginx-1容器的Network Namespace里，而另一端则位于宿主机上，并且被“插”在了宿主机的docker0网桥上。一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包的“生杀大权”全部交给对应的网桥。

所以，在收到这些ARP请求之后，docker0网桥就会扮演二层交换机的角色，把ARP广播转发到其他被“插”在docker0上的虚拟网卡上。这样，同样连接在docker0上的nginx-2容器的网络协议栈就会收到这个ARP请求，从而将172.17.0.3所对应的MAC地址回复给nginx-1容器。有了这个目的MAC地址，nginx-1容器的eth0网卡就可以将数据包发出去。而根据Veth Pari设备的原理，这个数据包会立即出现在宿主机上的veth9c02e56虚拟网卡上。不过，此时这个veth9c02e56网卡的网络协议栈的资源已经被“剥夺”，所以这个数据包就直接流入到了docker0网桥里。docker0处理转发的过程，则继续扮演二层交换机的角色。此时，docker0网桥根据数据包的目的MAC地址（也就是nginx-2容器的MAC地址），在它的CAM表（即交换机通过MAC地址学习维护的端口和MAC地址映射表）里查到对应的端口（Port）为：vethb4963f3，然后把数据包发往这个端口。

而这个端口，正是nginx-2容器“插”在docker0网桥上的另一块虚拟网卡，当然，它也是一个Veth Pair设备。这样，数据包就进入到了nginx-2容器的Network Namespace里。所以，nginx-2容器看到的情况是，它自己的eht0网卡上出现了流入的数据包。这样，nginx-2的网络协议栈就会对请求进行处理，最后响应返回到nginx-1。

以上，就是同一个宿主机上的不同容器通过docker0网桥进行通信的流程了。如下图所示：

![img](https://static001.geekbang.org/resource/image/e0/66/e0d28e0371f93af619e91a86eda99a66.png)

熟悉了docker0网桥的工作方式，就可以理解在默认情况下，被限制在Network Namespace里的容器进程实际上是通过Veth Pari设备+宿主机网桥的方式，实现了跟其他容器的数据交换。

与之类似，当你在一台宿主机上，访问该宿主机上的容器的IP地址时，这个请求的数据包，也是先根据路由规则到达docker0网桥，然后被转发到对应的Veth Pair设备，最后出现在容器里。这个过程如下：

![img](https://static001.geekbang.org/resource/image/9f/01/9fb381d1e49318bb6a67bda3f9db6901.png)





### 32.02 容器与另一个宿主机如何进行网络通信

当一个容器试图连接到另一个宿主机时，比如: ping 10.168.0.3，它发出的请求数据包，首先经过docker0网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0），对10.168.0.3的访问请求就会交给宿主机的eth0处理。所以接下来，这个数据包就会经过宿主机的eht0网卡转发到宿主机网络上，最终到达10.168.0.3对应的宿主机上。如下所示：

![img](https://static001.geekbang.org/resource/image/90/95/90bd630c0723ea8a1fb7ccd738ad1f95.png)

但是，如果在10.168.0.3上也有一个Docker容器，那么nginx-1要如何访问呢？这个问题，其实就是容器的“跨主通信”问题。在Docker的默认配置下，一台宿主机上的docker0网桥和其他宿主机上的docker0网桥没有任何关联，它们相互之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了。不过，万变不离其宗。如果我们通过软件的方式，创建一个整个集群“公用”的网桥，然后把集群里所有容器都连接到这个网桥上，不就可以相互通信了吗？是的，这样以来，我们整个集群里的容器网络就会类似如下图所示的样子：

![img](https://static001.geekbang.org/resource/image/b4/3d/b4387a992352109398a66d1dbe6e413d.png)

可以看到，构建这种容器网络的核心在于：我们需要在已有的宿主机网络上，再通过软件构件一个覆盖在已有宿主机网络之上的、可以把所有容器连接在一起的虚拟网络。所以，这种技术就被称为：Overlay Network（覆盖网络）。

而这个Overlay Network本身，可以由每一台宿主机上的一个“特殊网桥”共同组成。比如，当Node1上的Container1要访问Node2上的Container3的时候，Node1上的“特殊网桥”在收到数据包之后，就能够通过某种方式，把数据包发送到正确的宿主机，比如Node2。而Node2上的“特殊网桥”在收到数据包后，也能购通过某种方式，把数据包转发给正确的容器，比如container3
# Kubernetes容器持久化存储

## 28、 PV 、PVC、StorageClass这些到底说什么？

PV描述的是持久化存储数据卷。这个API对象主要定义的是一个持久化存储在宿主机上的目录，比如一个NFS的挂载目录。
PVC描述的是Pod所希望使用的持久化存储的属性。比如，Volume存储的大小、可读写权限等等。

用户创建的PVC要真正被容器使用起来，就必须先和某个符合条件的PV进行绑定。这里要检查的条件，包括两部分：
1： 第一个条件，当然是PV和PVC的spec字段。比如，PV的存储（storage）大小，就必须满足PVC的要求
2： 第二个条件，则是PV与 pvc的storageClassName字段必须一样。

在成功地将PVC和PV进行绑定之后，Pod就能够像使用hostPath等常规类型的Volume一样，在自己的YAML文件里声明使用这个PVC了，如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    role: web-frontend
spec:
  containers:
  - name: web
    image: nginx
    ports:
      - name: web
        containerPort: 80
    volumeMounts:
        - name: nfs
          mountPath: "/usr/share/nginx/html"
  volumes:
  - name: nfs
    persistentVolumeClaim:
      claimName: nfs
```

可以看到，Pod需要做的，就是在volumes字段里声明自己要使用的PVC名字。接下来，等这个Pod创建之后，kubelet就会把这个PVC所对应的PV，也就是一个NFS类型的Volume，挂载在这个Pod容器内的目录上。

在Kubernetes中，实际上存在着一个专门处理持久化存储的控制器，叫作Volume Controller。这个Volume Controller维护者多个控制循环，其中有一个循环，扮演的就是撮合PV和PVC的角色。它的名字叫做PersistentVolumeController。
PersistentVolumeController会不断地查看当前每一个PVC，是不是已经处于Bound(已绑定)状态。如果不是，那它就会遍历所有的、可用的PV，并尝试将其与这个PVC进行绑定。这样，Kubernetes就可以保证用户提交的每一个PVC，只要有合适的PV出现，它就能很快进入绑定状态。而所谓将一个PV与PVC进行“绑定”，其实就是将这个PV对象的名字，填在了PVC对象的spec.volumeName字段上。所以，接下来，Kubernetes只要获取到这个PVC对象，就一定能够找到它所绑定的PV。

所谓容器的Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起。而所谓的“持久化Volume"，指的就是这个宿主机上的目录，具备“持久性”

### 28.01 StorageClass

Kubernetes为我们提供了一套可以自动创建PV的机制，即：Dynamic Provisioning
相比之下，前面人工管理PV的方式就叫做： Static Provisioning
Dynamic Provisioning机制工作的核心，就在于一个名叫StorageClass的API对象。而StorageClass对象的作用，其实就是创建PV的模版。

具体地说，StorageClass对象会定义如下两个部分的内容：
1:  第一，PV的属性。比如，存储类型、Volume的大小等等
2：第二，创建这种PV需要用到的存储插件。比如，Ceph等等

有了这样两个信息之后，Kubernetes就能够根据用户提交的PVC，找到一个对应的StorageClass了。然后，Kuberentes就会调用该StorageClass声明的存储插件，创建出需要的PV。

Kubenetes的官方文档里已经列出了默认支持Dynamic Provisioning的内置存储插件。而对于不在文档里的插件，比如NFS，或者其他非内置存储插件，你其实可以通过 kubernetes-incubator/external-storage这个库来自己编写一个外部插件完成这个工作。

需要注意的是，StorageClass并不是专门为了Dynamic Provisioning而设计的。比如，在本篇已开始的例子里，我在PV和PVC里都声明了storageClassName=manual。而我的集群里，实际上并没有一个叫manual的StorageClass对象。这完全没有问题，这个时候Kubernetes进行的是Static Provisioning，但在做绑定决策的时候，它依然会考虑PV和PVC的StorageClass定义。而这么做的好处也很明显：这个PVC和PV的绑定关系，就完全在我自己的掌握之中。

这里，你可能会有疑问，我在之前讲解StatefulSet存储状态的例子时，好像并没有声明StorageClass。实际上，如果你的集群开启了名叫DefaultStorageClass的Admission Plugin，它就会为PVC和PV自动添加一个默认的StorageClass；否则，PVC的storageClassName的值就是“”，这也意味着它只能够跟storageClassName也是""的PV进行绑定

### 28.02 小结

PV、PVC、StorageClass之间的关系：

![image-20200531173433729](/Users/canghong/Library/Application Support/typora-user-images/image-20200531173433729.png)

从图中我们可以看到，在这个体系中：
1： PVC描述的，是Pod想要使用的持久化存储的属性，比如存储的大小、读写权限等
2:   PV描述的，则是一个具体的Volume的属性，比如Volume的类型、挂载目录、远程存储服务器地址等
3:  StorageClass的作用，则是充当PV的模版。并且，只有同属于一个StorageClass的PV和PVC，才可以绑定在一起

当然，StorageClass的另一个重要作用，是指定PV的Provisioner（存储插件）。这时候，如果你的存储插件支持Dynamic Provisioning的话，Kubernetes就可以自动为你创建PV了

## 29 PV、PVC体系是不是多次一举？从本地持久化卷谈起



## 30 编写自己的存储插件：FlexVolume与CSI

在Kubernetes中，存储插件的开发又两种方式：FlexVolume和CSI。

### 30.01 FlexVolume

对于一个FlexVolume类型的PV来说，它的YAML文件如下所示：

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-flex-nfs
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteMany
  flexVolume:
    driver: "k8s/nfs"
    fsType: "nfs"
    options:
      server: "10.10.0.25" # 改成你自己的NFS服务器地址
      share: "export"
```

可以看到，这个PV定义的Volume类型是flexVolume。并且，我们指定了这个Volume的driver叫做k8s/nfs。而Volume的options字段，则是一个自定义字段。在我们这个例子里，options字段指定了NFS服务器的地址（server: "10.10.0.25"），以及NFS共享目录的名字（ share: "export"）。

像这样一个PV被创建后，一旦和某个PVC绑定起来，这个FlexVolume类型的Volume就会进入到我们前面讲解过的Volume处理流程。这个流程的名字，即“Attach阶段”和"Mount阶段"。而在具体的控制循环中，这两个操作实际上调用的，正是Kubernetes的pkg/volume目录下的存储插件（Volume Plugin）。在我们这个例子里，就是pkg/volume/flexvolume这个目录里的代码。

当然了，这个目录其实只是FlexVolume插件的入口，以"Mount阶段"为例，在FlexVolume目录里，它的处理过程非常简单，如下所示：

```
// SetUpAt creates new directory.
func (f *flexVolumeMounter) SetUpAt(dir string, fsGroup *int64) error {
  ...
  call := f.plugin.NewDriverCall(mountCmd)
  
  // Interface parameters
  call.Append(dir)
  
  extraOptions := make(map[string]string)
  
  // pod metadata
  extraOptions[optionKeyPodName] = f.podName
  extraOptions[optionKeyPodNamespace] = f.podNamespace
  
  ...
  
  call.AppendSpec(f.spec, f.plugin.host, extraOptions)
  
  _, err = call.Run()
  
  ...
  
  return nil
}
```

上面这个名叫SetUpAt()的方法，正式FlexVolume插件对“Mount阶段”的实现位置。而SetUpAt()实际上制作了一件事，那就是封装出了一行命令（即：NewDriverCall），由kubelet在“Mount阶段”去执行。
在我们这个例子中，kubelet要通过插件在宿主机上执行的命令，如下所示：

```
/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs mount <mount dir> <json param>
```

其中，/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs就是插件的可执行文件的路径。这个名叫nfs的文件，正是你要编写的插件的实现。它可以是一个二进制文件，也可以是一个脚本。总之，只要能在宿主机上被执行起来即可。而且这个路径里的k8s-nfs部分，正式这个插件在Kubernetes里的名字。它是从driver="k8s/nfs"字段解析出来的。这个driver字段的格式是：vendor/driver。比如，一家存储插件的提供商（vendor）的名字叫做k8s，提供的存储驱动（driver）是nfs，那么Kubernetse就会使用k8s-nfs来作为插件名。

所以说，当你编写完了FlexVolume的实现之后，一定要把它的可执行文件放在每个节点的插件目录下。

而紧跟在可执行文件后面的“mount”参数，定义的就是当前的操作。在FlexVolume里，这些操作参数的名字是固定的，比如init、mount、unmount、attach，以及dettach等等，分别对应不同的Volume处理操作。

而跟在mount参数后面的两个字段：<mount dir >和<json params>，则是FlexVolume必须提供给这条命令的两个执行参数。其中第一个执行参数<mount dir>，正式kubelet调用SetUpAt()方法传递来的dir的值。它代表的是当前正在处理的Volume在宿主机上的目录。在我们的例子里，这个路径如下所示：

```
/var/lib/kubelet/pods/<Pod ID>/volumes/k8s~nfs/pv-flex-nfs
```

其中pv-flex-nfs正式我们前面定义的PV的名字。

而第二个执行参数<json params>，则是一个Json Map格式的参数列表。我们在前面PV里定义的options字段的值，都会被追加在这个参数里。此外，在SetUpAt()方法里可以看到，这个参数列表里还包括了Pod的名字、Namespace等元数据（Metadata）。

在明白了存储插件的调用方式和参数列表之后，这个插件的可执行文件的实现部分就非常容器理解了。在这个例子中，我们直接编写了一个简单的shell脚本来作为插件的实现，它对“Mount阶段”的处理过程，如下所示：

```
domount() {
 MNTPATH=$1
 
 NFS_SERVER=$(echo $2 | jq -r '.server')
 SHARE=$(echo $2 | jq -r '.share')
 
 ...
 
 mkdir -p ${MNTPATH} &> /dev/null
 
 mount -t nfs ${NFS_SERVER}:/${SHARE} ${MNTPATH} &> /dev/null
 if [ $? -ne 0 ]; then
  err "{ \"status\": \"Failure\", \"message\": \"Failed to mount ${NFS_SERVER}:${SHARE} at ${MNTPATH}\"}"
  exit 1
 fi
 log '{"status": "Success"}'
 exit 0
}
```

可以额看到，当kubelet再宿主机上执行“nfs mount <mount dir> <json params>”的时候，这个名叫nfs的脚本，就可以直接从<mount dir> 参数里拿到Volume在宿主机上的目录，即：MNTPAHT=$1。而你在PV的options字段里定义的NFS的服务器地址（options.server）和共享目录名字（options.share），则可以从第二<json params>参数里解析出来。有了这三个参数之后，这个脚本最关键的一步，当然就是执行：mount -t nfs ${NFS_SERVER}:/${SHARE} ${MNTPATH} 。这样，一个NFS的数据卷就被挂载到了MNTPATH，也就Volume所在的宿主机目录上，一个持久化的Volume目录就处理完成了。

综上所述，在“Mount阶段”，kubelet的VolumeManagerReconcile控制循环里的一次“调谐”操作的执行流程，如下所示：

```
kubelet --> pkg/volume/flexvolume.SetUpAt() --> /usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs mount <mount dir> <json param>

```

不过，像这样的FlexVolume实现方式，虽然简单，但局限性却很大。比如，跟Kubernetes内置的NFS插件类似，这个NFS FlexVolume插件，也不能支持Dynamic Provisioning（即：为每个PVC自动创建PV和对应的Volume）。除非你再为它编写一个专门的External Provisioner。再比如，我的插件在执行mount操作的时候，可能会生成一些挂载信息。这些信息，在后面执行unmount操作的时候会被用到。可是，在上述FlexVolume的实现里，你没办法把这些信息保存在一个变量里，等到unmount的时候直接使用。这个愿你也很容器理解：FlexVolume每一次对插件可执行文件的调用，都是一次完全独立的操作。所以，我们只能把这些信息写在一个宿主机的临时文件里，等到unmount的时候再去读取。

这也是为什么，我们需要有Container Storage Interface（CSI）这样更完善、更编程友好的插件方式。

### 30.02 CSI

Kubernetes里通过存储插件管理容器持久化存储的原理，可以用如下所示的示意图来描述：

![image-20200601011808274](/Users/canghong/Library/Application Support/typora-user-images/image-20200601011808274.png)

可以看到，在上述体系下，无论是FlexVolume，还是Kubernetes内置的其他存储插件，它们实际上担任的角色，仅仅是Volume管理中的"Attach阶段"和“Mount阶段”的具体执行者。而像Dynamic Provisioning这样的功能，就不是存储插件的责任，而是Kubernetes本身存储管理功能的一部分。

相比之下，CSI插件体系的设计思想，就是把这个Provision阶段，以及Kubernetes里的一部分存储管理功能，从主干代码里剥离出来，做成了几个单独的组件。这些组件会通过Watch API监听Kubernetes里与存储相关的事件变化，比如PVC的创建，来执行具体的存储管理动作。而这些管理动作，比如“Attach”阶段和“Mount阶段”的具体操作，实际上就是通过调用CSI插件来完成的。这种设计思路，我可以用如下的一副示意图来表示：

![image-20200601015040939](/Users/canghong/Library/Application Support/typora-user-images/image-20200601015040939.png)

可以看到，这套存储插件体系多了三个独立的外部组件（External Components），即：Driver Registrar、External Provisioner、External Attacher，对应的正是从Kubernetes项目里剥离出来的那部分存储管理功能。需要注意的是，External Components虽然是外部组件，但依然由Kubernetes社区来开发和维护。而图中最右侧部分，就是需要我们编写代码来实现的CSI插件。一个CSI插件只有一个二进制文件，但它会议gRPC的方式对外提供三个服务（gRPC Service），分别叫做：CSI Identity、CSI Controller和CSI Node。

其中，Driver Registrar组件，负责将插件注册到kubelet里面（这可以类比，将可执行文件放在插件目录下）。而在具体实现上，Driver Registrar需要请求CSI插件的Identity服务来获取插件信息。

而External Provisioner组件，负责的正是Provision阶段。在具体实现上，External Provisioner监听（Watch）了APIServer里的PVC对象。当一个PVC被创建时，它就会调用CSI Controller的CreateVolume方法，为你创建对应的PV。此外，如果你使用的存储是公有云提供的磁盘的话。这一步就需要调用公有云的API来创建这个PV所描述的磁盘了。不过，由于CSI插件是独立于Kubernetes之外的，所以在CSI的API里不会直接使用Kubernetes定义的PV类型，而是会自己定义一个单独的Volume类型。为了方便叙述，我会把Kubernetes里的持久化卷类型叫做PV。把CSI里的持久化卷叫做CSI Volume。

最后一个External Attacher组件，负责的正是“Attach阶段”。在具体实现上，它监听了APIServer里的VolumeAttachment对象的变化。VolumeAttachment对象是Kubernetes确认一个Volume可以进入“Attach阶段”的重要标志。一旦出现了VolumeAttachment对象，External Attacher就会调用CSI Controller服务的ControllerPublish方法，完成它所对应的Volume的Attach阶段。而Volume的“Mount阶段”，并不属于External Components的职责。当kubelet的VolumeManagerReconciler控制循环检查到它需要执行Mount操作的时候，会通过pkg/volume/csi包，直接调用CSI Node服务完成Volume的“Mount阶段”。

在使用使用CSI插件的时候，我们会将这三个External Components作为sidecar容器和CSI插件放置在同一个Pod中。由于External Components对CSI插件的调用非常频繁，所以这种sidecar的部署方式非常高效。

接下来，讲解一下CSI插件里的三个服务：CSI Identity、CSI Controller 和CSI Node。

其中，CSI插件的CSI Identity服务，负责对外暴露这个插件本身的信息，如下所示：

```
service Identity {
  // return the version and name of the plugin
  rpc GetPluginInfo(GetPluginInfoRequest)
    returns (GetPluginInfoResponse) {}
  // reports whether the plugin has the ability of serving the Controller interface
  rpc GetPluginCapabilities(GetPluginCapabilitiesRequest)
    returns (GetPluginCapabilitiesResponse) {}
  // called by the CO just to check whether the plugin is running or not
  rpc Probe (ProbeRequest)
    returns (ProbeResponse) {}
}
```

而CSI Controller服务，定义的则是对CSI Volume（对应Kubernetes里的PV）的管理接口，比如：创建和删除CSI Volume，对CSI Volume进行Attach/Deattach（在CSI里，这个操作被叫做Publish/Unpublish），以及对CSI Volume进行Snapshot等，它们的接口定义如下所示：

```
service Controller {
  // provisions a volume
  rpc CreateVolume (CreateVolumeRequest)
    returns (CreateVolumeResponse) {}
    
  // deletes a previously provisioned volume
  rpc DeleteVolume (DeleteVolumeRequest)
    returns (DeleteVolumeResponse) {}
    
  // make a volume available on some required node
  rpc ControllerPublishVolume (ControllerPublishVolumeRequest)
    returns (ControllerPublishVolumeResponse) {}
    
  // make a volume un-available on some required node
  rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest)
    returns (ControllerUnpublishVolumeResponse) {}
    
  ...
  
  // make a snapshot
  rpc CreateSnapshot (CreateSnapshotRequest)
    returns (CreateSnapshotResponse) {}
    
  // Delete a given snapshot
  rpc DeleteSnapshot (DeleteSnapshotRequest)
    returns (DeleteSnapshotResponse) {}
    
  ...
}
```

不难发现，CSI Controller服务里定义的这些操作有个共同特点，那就是它们都无需在宿主机上进行，而是属于Kubernetes里Volume Controller的逻辑，也就是属于Master节点的一部分。

需要注意的是，正如我前面所提到的那样，CSI Controller服务的实际调用者，并不是Kubernetes（即：通过 pkg/volume/csi 发起CSI请求），而是External Provisioner 和 External Attacher。这两个 External Components，分别通过监听PVC和VolumeAttachement对象，来跟Kubernetes进行写作。

而CSI需要在宿主机上执行的操作，都定义在了CSI Node服务里，如下所示：

````

service Node {
  // temporarily mount the volume to a staging path
  rpc NodeStageVolume (NodeStageVolumeRequest)
    returns (NodeStageVolumeResponse) {}
    
  // unmount the volume from staging path
  rpc NodeUnstageVolume (NodeUnstageVolumeRequest)
    returns (NodeUnstageVolumeResponse) {}
    
  // mount the volume from staging to target path
  rpc NodePublishVolume (NodePublishVolumeRequest)
    returns (NodePublishVolumeResponse) {}
    
  // unmount the volume from staging path
  rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest)
    returns (NodeUnpublishVolumeResponse) {}
    
  // stats for the volume
  rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest)
    returns (NodeGetVolumeStatsResponse) {}
    
  ...
  
  // Similar to NodeGetId
  rpc NodeGetInfo (NodeGetInfoRequest)
    returns (NodeGetInfoResponse) {}
}
````

### 30.03 

可以看到，相比于 FlexVolume，CSI 的设计思想，把插件的职责从“两阶段处理”，扩展成了 Provision、Attach 和 Mount 三个阶段。其中，Provision 等价于“创建磁盘”，Attach 等价于“挂载磁盘到虚拟机”，Mount 等价于“将该磁盘格式化后，挂载在 Volume 的宿主机目录上”。

在有了 CSI 插件之后，Kubernetes 本身依然按照28章中所讲述的方式工作，唯一区别在于：
1: 当 AttachDetachController 需要进行“Attach”操作时（“Attach 阶段”），它实际上会执行到 pkg/volume/csi 目录中，创建一个 VolumeAttachment 对象，从而触发 External Attacher 调用 CSI Controller 服务的 ControllerPublishVolume 方法。
2: 当 VolumeManagerReconciler 需要进行“Mount”操作时（“Mount 阶段”），它实际上也会执行到 pkg/volume/csi 目录中，直接向 CSI Node 服务发起调用 NodePublishVolume 方法的请求。



## 31 容器存储实践：CSI插件编写指南

实践例子编写的CSI创建的功能，就是：让我们运行在DigitalOcean上的Kubernetes集群能够使用它的块存储服务，作为容器的持久化存储。

有了CSI插件之后，持久化存储的用法就非常简单了，你只需要创建一个如下所示的StorageClass对象即可：

```

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: do-block-storage
  namespace: kube-system
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: com.digitalocean.csi.dobs
```

有了这个StorageClass，External Provisoner就会为集群中新出现的PVC自动创建出PV，然后调用CSI插件创建出这个PV对应的Volume，这正是CSI体系中Dynamic Provisioning的实现方式。

不难看到，这个StorageClass里唯一引人注意的是provisioner: com.digitalocean.csi.dobs这个字段。显然，这个字段告诉了Kubernetes，请使用名叫com.digitalocean.csi.dobs的CSI插件来为我处理这个StorageClass相关的所有操作。

那么，Kubernetes又是如何知道一个CSI插件的名字的呢？那么就需要从CSI插件的第一个服务CSI Identity说起了。其实，一个CSI插件的代码结构非常简单，如下所示：

```
tree $GOPATH/src/github.com/digitalocean/csi-digitalocean/driver  
$GOPATH/src/github.com/digitalocean/csi-digitalocean/driver 
├── controller.go
├── driver.go
├── identity.go
├── mounter.go
└── node.go
```



### 31.01 CSI Identity

其中，CSI Identity 服务的实现，就定义在了driver目录下的identity.go文件里。当然，为了能够让Kubernetes访问到CSI Identity服务，我们需要先在driver.go文件里，定义一个标准的 gRPC Server，如下所示：

```

// Run starts the CSI plugin by communication over the given endpoint
func (d *Driver) Run() error {
 ...
 
 listener, err := net.Listen(u.Scheme, addr)
 ...
 
 d.srv = grpc.NewServer(grpc.UnaryInterceptor(errHandler))
 csi.RegisterIdentityServer(d.srv, d)
 csi.RegisterControllerServer(d.srv, d)
 csi.RegisterNodeServer(d.srv, d)
 
 d.ready = true // we're now ready to go!
 ...
 return d.srv.Serve(listener)
}
```

可以看到，只要把编写好的gRPC Server注册给CSI，它就可以响应来自External Components的CSI请求了。

CSI Identity服务中，最重要的接口是GetPluginInfo。它返回的就是这个插件的名字和版本号，如下所示

```

func (d *Driver) GetPluginInfo(ctx context.Context, req *csi.GetPluginInfoRequest) (*csi.GetPluginInfoResponse, error) {
 resp := &csi.GetPluginInfoResponse{
  Name:          driverName,
  VendorVersion: version,
 }
 ...
}
```

其中，driverName的值，正是"com.digitalocean.csi.dobs"。所以说，kubernetes正是通过GetPluginInfo的返回值，来找到你在StorageClass里声明要使用的CSI插件的。

### 31.02 CSI Controller

CSI插件的第二个服务是CSI Controller，这个服务主要实现的就是Volume管理流程中的“Provision阶段”和“Attach阶段”。"Provision阶段"对应的接口，是CreateVolume和DeleteVolume，它们的调用者是External Provisioner。以CreateVolume为例，它的主要逻辑如下所示：

```

func (d *Driver) CreateVolume(ctx context.Context, req *csi.CreateVolumeRequest) (*csi.CreateVolumeResponse, error) {
 ...
 
 volumeReq := &godo.VolumeCreateRequest{
  Region:        d.region,
  Name:          volumeName,
  Description:   createdByDO,
  SizeGigaBytes: size / GB,
 }
 
 ...
 
 vol, _, err := d.doClient.Storage.CreateVolume(ctx, volumeReq)
 
 ...
 
 resp := &csi.CreateVolumeResponse{
  Volume: &csi.Volume{
   Id:            vol.ID,
   CapacityBytes: size,
   AccessibleTopology: []*csi.Topology{
    {
     Segments: map[string]string{
      "region": d.region,
     },
    },
   },
  },
 }
 
 return resp, nil
}
```

可以看到，对于DigitalOcean这样的公有云来说，CreateVolume需要做的操作，就是调用DigitalOcean块存储服务的API，创建出一个存储卷（d.doClient.Storage.CreateVolume）。

而"Attach阶段"对应的接口是ControllerPublishVolume和ControllerUnpublishVolume，它们的调用者是External Attacher。以ControllerPublishVolume为例，它的逻辑如下所示：

```

func (d *Driver) ControllerPublishVolume(ctx context.Context, req *csi.ControllerPublishVolumeRequest) (*csi.ControllerPublishVolumeResponse, error) {
 ...
 
  dropletID, err := strconv.Atoi(req.NodeId)
  
  // check if volume exist before trying to attach it
  _, resp, err := d.doClient.Storage.GetVolume(ctx, req.VolumeId)
 
 ...
 
  // check if droplet exist before trying to attach the volume to the droplet
  _, resp, err = d.doClient.Droplets.Get(ctx, dropletID)
 
 ...
 
  action, resp, err := d.doClient.StorageActions.Attach(ctx, req.VolumeId, dropletID)

 ...
 
 if action != nil {
  ll.Info("waiting until volume is attached")
 if err := d.waitAction(ctx, req.VolumeId, action.ID); err != nil {
  return nil, err
  }
  }
  
  ll.Info("volume is attached")
 return &csi.ControllerPublishVolumeResponse{}, nil
}
```

可以看到，对于DigitalOcean来说，ControllerPublishVolume在“Attach阶段”需要做的工作，是调用DigitalOcean的API，将我们前面创建的存储卷，挂载到指定的虚拟机上（ d.doClient.StorageActions.Attach）。其中，存储卷由请求中的VolumeId来指定。而虚拟机，也就是将要运行Pod的宿主机，则由请求中的NodeId来指定。这些参数，都是External Attacher在发起请求时需要设置的。而External Attacher是如何获取这些参数的呢？External Attacher的工作原理，是监听（Watch）了一种名叫VolumeAttachment的API对象。这种API对象的主要字段如下所示： 

```

// VolumeAttachmentSpec is the specification of a VolumeAttachment request.
type VolumeAttachmentSpec struct {
 // Attacher indicates the name of the volume driver that MUST handle this
 // request. This is the name returned by GetPluginName().
 Attacher string
 
 // Source represents the volume that should be attached.
 Source VolumeAttachmentSource
 
 // The node that the volume should be attached to.
 NodeName string
}
```

而这个对象的生命周期，是由AttachDetachController负责管理的，这个Controller的控制循环会不断检查Pod对应的PV，在它所绑定的宿主机上的挂载情况，而从决定是否需要对这个PV 进行Attach（或者Dettach）操作。而这个Attach操作，就是创建出上面这样一个VolumeAttachment对象。可以看到，Attach操作所需要的PV的名字（Source）、宿主机的名字（NodeName）、存储插件的名字（Attacher），都是这个VolumeAttachment的一部分。

而当External Attacher监听到这样的一个对象出现之后，就可以立即使用VolumeAttachment里的这些字段，封装成一个gRPC请求调用CSI ControllerPublishVolume方法。

### 31.03 CSI Node

CSI Node服务对应的，是Volume管理流程里的“Mount”阶段。它的代码实现，在node.go文件里。

kubelet的VolumeManagerReconciler控制循环会调用CSI Node服务来完成Volume的“Mount”阶段。不过，在具体的实现中，这个“Mount阶段”的处理其实被细分成了NodeStageVolume和NodePublishVolume这两个接口。

对于磁盘和块设备来说，它们被Attach到宿主机之后，就成了宿主机上的一个待用存储设备。而到了“Mount”阶段，我们首先需要格式化这个设备，然后才能把他挂载到Volume对应的宿主机目录上。在kubelet的VolumeManagerReconciler控制循环中，这两部操作分别叫做MountDevice和SetUp。其中，MountDevice操作，就是直接调用CSI Node服务里的NodeStageVolume接口。顾名思义，这个接口的作用，就是格式化Volume在宿主机上对应的存储设备，然后挂载到一个临时目录（Staging目录）上。而SetUp操作则会调用CSI Node服务的NodePublishVolume接口。有了上述对设备的预处理后，它的实现就非常简单了，将Staing目录，绑定挂载到Volume对应的宿主机目录上。由于Staging目录，正是Volume对应的设备被格式化后挂载在宿主机上的位置，所以当它和Volume的宿主机目录绑定挂载之后，这个Volume宿主机目录的“持久化”处理也就完成了。

### 31.04 部署

我们部署CSI插件的常用原则是：

第一，通过DaemonSet在每个节点上都启动一个CSI插件，来为kubelet提供CSI Node服务。这是因为，CSI Node服务需要被kubelet直接调用，所以它要和kubelet“一对一”地部署起来。此外，除了CSI插件，我们还以sidecar的方式运行着driver-registrar这个外部组件。它的作用，是向kubelet注册这个CSI插件。这个注册过程使用的插件信息，则通过访问同一个Pod里的CSI插件容器的Identity服务获取到。
需要注意的是，由于CSI插件运行在一个容器里，那么CSI Node服务在“Mount阶段”执行的挂载操作，实际上是发生在这个容器的Mount Namespace里的。可是，我们真正希望执行挂载操作的对象，都是宿主机/var/lib/kubelet目录下的文件和目录。所以，在定义DaemonSet Pod的时候，我们需要把宿主机的/var/lib/kubelet以Volume的方式挂载进CSI插件容器的同名目录下，然后设置这个Volume的mountPropagation=Bidirectional，即开启双向挂载传播，从而将容器在合格目录下进行的挂载操作“传播”给宿主机，反之亦然。这个DaemonSet的完成yaml如下: 

```

kind: DaemonSet
apiVersion: apps/v1beta2
metadata:
  name: csi-do-node
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: csi-do-node
  template:
    metadata:
      labels:
        app: csi-do-node
        role: csi-do
    spec:
      serviceAccount: csi-do-node-sa
      hostNetwork: true
      containers:
        - name: driver-registrar
          image: quay.io/k8scsi/driver-registrar:v0.3.0
          ...
        - name: csi-do-plugin
          image: digitalocean/do-csi-plugin:v0.2.0
          args :
            - "--endpoint=$(CSI_ENDPOINT)"
            - "--token=$(DIGITALOCEAN_ACCESS_TOKEN)"
            - "--url=$(DIGITALOCEAN_API_URL)"
          env:
            - name: CSI_ENDPOINT
              value: unix:///csi/csi.sock
            - name: DIGITALOCEAN_API_URL
              value: https://api.digitalocean.com/
            - name: DIGITALOCEAN_ACCESS_TOKEN
              valueFrom:
                secretKeyRef:
                  name: digitalocean
                  key: access-token
          imagePullPolicy: "Always"
          securityContext:
            privileged: true
            capabilities:
              add: ["SYS_ADMIN"]
            allowPrivilegeEscalation: true
          volumeMounts:
            - name: plugin-dir
              mountPath: /csi
            - name: pods-mount-dir
              mountPath: /var/lib/kubelet
              mountPropagation: "Bidirectional"
            - name: device-dir
              mountPath: /dev
      volumes:
        - name: plugin-dir
          hostPath:
            path: /var/lib/kubelet/plugins/com.digitalocean.csi.dobs
            type: DirectoryOrCreate
        - name: pods-mount-dir
          hostPath:
            path: /var/lib/kubelet
            type: Directory
        - name: device-dir
          hostPath:
            path: /dev

```

第二，通过StatefulSet在任意一个节点上再启动一个CSI插件，为External Components提供CSI Controller服务。所以，作为CSI Controller服务的调用者，External Provisioner和External Attacher这两个外部组件，就需要以sidecar的方式和这次部署的CSI插件定义在同一个Pod里。至于我们为什么用StatefulSet而不是Deployment来运行这个CSI插件，是因为StatefulSet需要确保应用拓扑状态的稳定性，所以它对Pod的更新，是严格保证顺序的，即： 只有在前一个Pod停止并删除之后，它才会创建并启动下一个Pod。而像我们将StatefulSet的replicas设置为1的话，StatefulSet就会确保Pod被删除重建的时候，永远只有一个CSI插件的Pod运行在集群中。这对CSI插件的正确性来说，至关重要。这个StatefulSet的YAML文件如下：

```

kind: StatefulSet
apiVersion: apps/v1beta1
metadata:
  name: csi-do-controller
  namespace: kube-system
spec:
  serviceName: "csi-do"
  replicas: 1
  template:
    metadata:
      labels:
        app: csi-do-controller
        role: csi-do
    spec:
      serviceAccount: csi-do-controller-sa
      containers:
        - name: csi-provisioner
          image: quay.io/k8scsi/csi-provisioner:v0.3.0
          ...
        - name: csi-attacher
          image: quay.io/k8scsi/csi-attacher:v0.3.0
          ...
        - name: csi-do-plugin
          image: digitalocean/do-csi-plugin:v0.2.0
          args :
            - "--endpoint=$(CSI_ENDPOINT)"
            - "--token=$(DIGITALOCEAN_ACCESS_TOKEN)"
            - "--url=$(DIGITALOCEAN_API_URL)"
          env:
            - name: CSI_ENDPOINT
              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock
            - name: DIGITALOCEAN_API_URL
              value: https://api.digitalocean.com/
            - name: DIGITALOCEAN_ACCESS_TOKEN
              valueFrom:
                secretKeyRef:
                  name: digitalocean
                  key: access-token
          imagePullPolicy: "Always"
          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy/
      volumes:
        - name: socket-dir
          emptyDir: {}
```

接下来，我们就可以使用这个CSI插件了。

文章的开始部分，我们定义了一个StorageClass如下，现在这个插件 com.digitalocean.csi.dobs也已经存在了

```

kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: do-block-storage
  namespace: kube-system
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: com.digitalocean.csi.dobs
```

所以，接下来只需要定义一个声明使用这个StorageClass的PVC即可，如下所示： 

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: csi-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
  storageClassName: do-block-storage
```

当把上述这个PVC提交到Kubernetes后，就可以在Pod里声明使用这个csi-pvc来作为持久化存储了。

### 31.05 小结

以DigitalOcean 的 CSI 插件为例，

当用户创建了一个PVC之后，External Provisioner会监听到这个PVC的诞生，然后调用同一个Pod里的CSI插件的CSI Controller服务的CreateVolume方法，为你创建出对应的PV。这时候，运行在Kubernetes Master节点上的Volume Controller就会通过PersistentVolumeController控制循环，发现这对新创建出来的PV和PVC，并且看到它们声明的是同一个StorageClass。所以，它会把这一对PV和PVC绑定，使PVC进入Bound状态。然后，用户创建一个声明使用上述PVC的Pod，并且这个Pod被调度到了宿主机A上，这时，Volume Controller的AttachDetachController控制循环就会发现，上述PVC对应的Volume，需要被Attach到宿主机A上。所以，AttachDetachController就会创建一个VolumeAttachment对象，这个对象携带了宿主机A和待处理的Volume名字。External  Attacher监听到VolumeAttachment对象的诞生。于是，它就会使用这个对象里的宿主机和Volume名字，调用同一个Pod里的CSI插件的CSI Controller服务的ControllerPublishVolume，完成Attach阶段。上述过程完成后，运行在宿主机A的kubelet，就会通过VolumeManagerReconciler控制循环，发现当前宿主机上有一个Volume对应的存储设备（比如磁盘）已经被Attach到了某个设备目录下。于是kubelet就会调用同一宿主机上的CSI插件的CSI Node服务的NodeStageVolume和NodePublishVolume完成""




































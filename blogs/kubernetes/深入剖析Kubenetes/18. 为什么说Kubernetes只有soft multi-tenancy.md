# Kubernetes网络容器

## 36 为什么说Kubernetes只有soft multi-tenancy

在Kubernetes里，网络隔离能力的定义，是依靠一种专门的API对象来描述的，即：NetworkPolicy。一个完成的NetworkPolicy对象的示例，如下所示：

```

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

Kubernetes里的Pod默认都是“允许所有”（Accept All）的，即：Pod可以接受来自任何发送方的请求；也可以向任何接受方发送请求。如果要对这种情况作出限制，就必须通过NetworkPolicy对象来指定。在上面这个例子里，你首先会看到podSelector字段。它的作用就是定义这个NetworkPolicy的限制范围，比如：当前Namespace里携带了role=db标签的Pod。而如果你把podSelector字段留空：

```
spec:
 podSelector: {}
```

那么这个NetworkPolicy就会作用于当前Namespace下的所有Pod，而一旦Pod被选中，这个Pod既不允许被外界访问，也不允许访问外界。而NetworkPolicy定义的规则，其实就是白名单。上面这个NetworkPolicy，指定的规则如下所示：
1：该隔离规则只对default Namespace下的，携带了role=db标签的Pod有效。限制的请求类型包括ingree（流入）和egress（流出）
2：Kubernetes会拒绝任何访问被隔离Pod的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离Pod的6379端口。这些“白名单”对象包括：
   1）default Namespace里的，携带了role=frontend标签的Pod
   2）携带了Project=myproject标签的Namespace里的任何Pod
   3）任何源地址属于172.17.0.0/16网段，且不属于172.17.1.0/24
3：Kuberentes会拒绝被隔离Pod对外发起任何请求，除非请求的目的地址属于10.0.0.0/24网段，并且访问的是该网段地址的5978端口

注意NetworkPolicy规则的OR和AND的区别，举个例子：

```

  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    - podSelector:
        matchLabels:
          role: client
  ...
```

像这样定义namesapceSelector和podSelector是OR的关系

```

...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...
```

像这样定义的namespaceSelector和podSelector是AND的关系

此外，如果要使上面定义的NetworkPolicy在Kubernetes集群里真正产生作业，你的CNI网络插件就必须是支持Kubernetes的NetworkPolicy的。在具体实现上，凡是支持NetworkPolicy的CNI网络插件，都维护者一个NetworkPolicy Controller，通过控制循环的方式对NetworkPolicy对象的增删改查作出响应，然后在宿主机上完成iptables规则的配置工作。

那么，这些网络插件又是如何根据NetworkPolicy对Pod进行隔离的呢？接下来，我们以三层网络插件为例，分析这部分的原理。为了方便讲解，这一次编写一个比较简单的NetworkPolicy对象，如下所示：

```

apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
   - from:
     - namespaceSelector:
         matchLabels:
           project: myproject
     - podSelector:
         matchLabels:
           role: frontend
     ports:
       - protocol: tcp
         port: 6379
```

可以看到，我们指定的ingress“白名单”，是任何Namespace里，携带了project=myproject标签的Pod；以及default Namespace里，携带了role=frontend标签的Pod。允许被访问的端口是6379。而被隔离的对象，是所有携带了role=db标签的Pod。那么这个时候，Kubernetes网络插件就会使用这个NetworkPolicy的定义，在宿主机上生成iptables规则。为代码如下所示：

```

for dstIP := range 所有被networkpolicy.spec.podSelector选中的Pod的IP地址
  for srcIP := range 所有被ingress.from.podSelector选中的Pod的IP地址
    for port, protocol := range ingress.ports {
      iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT 
    }
  }
} 
```

这条规则的名字是KUBE-NWPLCY-CHAIN，含义是：当IP包的源地址是srcIP，目的地址是dstIP，协议是protocol，目的端口是port的时候允许通过。

可以看到，Kubernetes网络插件对Pod进行隔离，其实是靠在宿主机上生成NetworkPolicy对应的iptable规则来实现的。此外，在设置好上述“隔离”规则之后，网络插件还要想办法，将所有被隔离Pod的访问请求，都转发到上述KUBE-NWPLCY-CHAIN规则上去进行匹配。并且，如果匹配不通过，这个请求应该被“拒绝”。

在CNI网络插件中，上述需求可以通过设置两组iptables规则来实现。第一组规则，负责“拦截”对被隔离Pod的访问请求。生成这一组规则的为代码如下：

```

for pod := range 该Node上的所有Pod {
    if pod是networkpolicy.spec.podSelector选中的 {
        iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN
        iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN
        ...
    }
}
```

可以看到，这里的iptables规则使用了内置链FORWARD。实际上，iptables只是操作系统内核Netfilter子系统的“界面”。顾名思义，Netfilter子系统的作用，就是Linux内核里挡在“网卡”和“用户态进程”之间的一道防火墙。如下图所示：

![img](https://static001.geekbang.org/resource/image/4a/c2/4a012412dd694cb815ac9ee11ce511c2.png)

Netfiter中有很多“检查点”，在iptables中，这些“检查点”就被称为：链（Chain）。iptables表的作用，就是在某个具体的“检查点”按顺序执行不同的检查动作。

在理解了iptabels的工作原理后，我们再回到NetworkPolicy上来。这时候，前面由网络插件设置的、负责“拦截”进入Pod的请求的三条iptables规则，就很容易读懂了：

```
iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN
iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN
...
```

其中，第一条FORWARD链“拦截”的是一种特殊情况：它对应的是同一台宿主机上容器之间经过CNI网桥进行通信的流入数据包。其中，--physdev-is-bridged的意思就是，这个FORWARD链匹配的是，通过本机上的网桥设备，发往目的地址是podIP的IP包。而第二条FORWARD链“拦截”的则是普遍的情况，即：容器的跨主通信。这时候，流入容器的数据包都是经过路由转发的。不难看到，这些规则最后都跳转到了名叫KUBE-POD-SPECIFIC-FW-CHAIN的规则上。而这个KUBE-POD-SPECIFIC-FW-CHAIN的作用，就是允许作为“拒绝”和“允许”的判断。这部分的功能实现，可以简单描述为下面这样的iptables规则：

```
iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN
iptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable
```

可以看到，首先，在第一条规则里，我们会把IP包转交给前面定义的KUBE-NWPLCY-CHAIN规则进行匹配。如果匹配成功，IP包就会“允许”通过。如果匹配失败，IP包就会来到第二条规则上。可以看到，这是一条REJECT规则，通过这条规则，不满足定义的包将会被拒绝掉，从而实现了对容器的“隔离”。

### 36.01 小结

可以看到，NetworkPolicy实际上只是宿主机的一系列iptables规则。这跟传统IaaS里面的安全组（Security Group）是非常相似的。而基于上述讲述，你就会发现这样一个事实：

Kubernetes的网络模型以及大多数容器网络的实现，其实既不会保证容器之间二层网络的互通，也不会实现容器之间的二层网络隔离。这跟IaaS项目管理虚拟机的方式，是完全不同的。所以说，Kubernets从底层的设计和实现上，更倾向于假设你已经有了一套完整的物理基础设施。然后Kubernets负责在此基础上提供一种“弱多租户”（soft multi-tenancy）的能力


























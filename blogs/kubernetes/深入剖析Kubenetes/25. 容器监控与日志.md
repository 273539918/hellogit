

# Kubernetes容器监控与日志

## 48  Prometheus、Metrics Server与Kubernetes监控体系

Kubernetes项目的监控体系曾经非常繁杂，在社区中也有很多方案。但这套体系发展到今天，已经完全演变成了以Prometheus项目为核心的一套统一的方案。实际上，Prometheus项目是当年CNCF基金会起家时的“第二把交椅”。而这个项目发展到今天，已经全面接管了Kubernetes项目的整套监控体系。

比较有意思的是，Prometheus项目与Kubernetes项目一样，也是来自Google的Borg体系，它的原型系统，叫做BorgMon，是一个几乎与Borg同时诞生的内部监控系统。而Prometheus项目的发起原因也跟Kubernetes很类似，都是希望通过对用户更友好的方式，将Google内部系统的设计理念，传递给用户和开发者。

作为一个监控系统，Prometheus项目的作用和工作方式，其实可以用如下所示的一张官方示意图来解释

![img](https://static001.geekbang.org/resource/image/2a/d3/2ada1ece66fcc81d704c2ba46f9dd7d3.png)

可以看到，Prometheus项目工作的核心，是使用Pull（抓取）的方式去搜集被监控对象的Metrics数据（监控指标数据），然后，再把这些数据保存在一个TSDB（时间序列数据库，比如OpenTSDB、InfluxDB等）当中，以便后续可以按照时间进行检索。

有了这套核心监控机制，Prometheus剩下的组件就是用来配合这套机制运行。比如 Pushgateway，可以允许被监控对象以Push的方式向Prometheus推送Metric数据。而Alertmanager，则可以根据Metrics信息灵活地设置报警。当然，Prometheus最受欢迎的功能，还是通过Grafana对外暴露的、可以灵活配置的监控数据可视化界面。

有了Prometheus之后，我们就可以按照Metrics数据的来源，来对Kubernetes的监控体系做一个汇总了。

第一种Metrics，是宿主机的监控数据。这部分数据的提供，需要借助一个由Prometheus维护的Node Exporter工具。一般来说，Node Exporter会以DaemonSet的方式运行在宿主机上。其实，所谓的Exporter，就是代替被监控对象来对Prometheus暴露出可以被“抓取”的Metrics信息的一个辅助进程。

第二种Metrics，是来自于Kubernetes的API Server、kubelet等组件的 /metrics API。除了常规的CPU、内存的信息外，这部分信息还主要包括了各个组件的核心监控指标。比如，对于API Server来说，它就会在 /metrics API里，暴露出各个Controller的工作队列（Work Queue）的长度，请求的QPS和延迟数据等等。这些信息，是检查Kubernetes本身工作情况的主要依据。

第三种Metrics，是Kubernetes相关的监控数据。这部分数据，一般叫做Kubernetes核心监控数据（core metrics）。这其中包括了Pod、Node、容器、Service等主要Kubernetes核心概念的Metrics。其中，容器相关的Metrics主要来自于kubelet内置的cAdvisor服务。在kubelet启动后，cAdvisor服务也随之启动，而它能提供的信息，可以细化到每一个容器的CPU、文件系统、内存、网络等资源的使用情况。
需要注意的是，这里提到的Kubernetes核心监控数据，其实使用的是Kubernetes的一个非常重要的扩展能力，叫做Metrics Server。Metrics Server在Kubernetes社区的定位，其实是用来取代Heapster这个项目的。在Kubernetes项目发展的初期，Heapster是用户获取Kubernetes监控数据（比如Pod和Node的资源使用情况）的主要渠道。而后面提出来的Metrics Server，则把这些信息，通过标准的Kubernetes API暴露了出来。这样，Metrics信息就跟Heapster完成了解藕，允许Heapster项目慢慢退出舞台。

而有了Metrics Server之后，用户就可以通过标准的Kubernetes API来访问到这些监控数据了。比如，下面这个URL：

```
http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespaces/<namespace-name>/pods/<pod-name>
```

当你访问这个Metrics API时，它就会为你返回一个Pod的监控数据，而这些数据，其实是从kubelet的Summary API（即<kubelet_ip>:<kubelet_port>/stats/summary）采集而来的。Summary API返回的信息，既包括了cAdvisor的监控数据，也包括了kubelet本身汇总的信息。

需要指出的是，Metric Server并不是kube-apiserver的一部分，而是通过Aggregator这种插件机制，在独立部署的情况下同kube-apiserver一起统一对外服务的。

这里，Aggregator APIServer的工作原理，可以用如下所示的一副示意图来表示清楚：

![img](https://static001.geekbang.org/resource/image/0b/09/0b767b5224ad1906ddc4cce075618809.png)

可以看到，当Kubernetes的API Server开启了Aggregator模式之后，你再访问aips/metrics.k8s.io/v1beta1的时候，实际上访问到的是一个叫做kube-aggregator的代理。而kube-apiserver，正是这个代理的一个后端；而Metrics Server，正是另一个后端。

而且，在这个机制上，你还可以添加更多的后端给这个kube-aggregator。所以，kube-aggregator其实就是一个根据URL选择具体的API后端的代理服务器。通过这种方式，我们就可以很方便地扩展Kubernetes的API了。

在理解了Prometheus关心的三种监控数据源，以及Kubernetes的核心Metrics之后，作为用户，你其实要做的就是将Prometheus Operator在Kubernetes集群里部署起来。然后，按照本篇文章已开始介绍的架构，把上述Metrics源配置起来，让Prometheus自己去进行采集即可。

在具体的监控指标规划上，我建议遵循业界通用的USE原则和RED原则。
USE原则（主要用于资源监控指标）：
1、利用率（Utilization），资源被有效利用起来提供服务的平均时间占比
2、饱和度（Saturation），资源拥挤的程度，比如工作队列的长度
3、错误率（Error），错误的数量
RED原则（主要用于服务监控指标）：
1、每秒请求数量（Rate）;
2、每秒错误数量（Errors）;
3、服务响应时间（Duration）；



## 49 Custom metrics : 让Auto Scaling 不再“食之无味”

实际上，借助Promethus监控体系，Kubernetes就可以提供一种非常有用的能力，那就是Custom Metrics，自定义监控指标。

在过去的很多Paas项目中，其实有一种叫做Auto Scaling，即自动水平扩展的功能。只不过，这个功能往往只能依据某种指定的资源类型执行水平扩展，比如CPU或者Memory的使用值。

而在真实的场景中，用户需要进行Auto Scaling的依据往往是自定义的监控指标。比如，某个应用的等待队列长度，或者某种应用相关资源的使用情况。这些复杂多变的需求，在传统Paas项目和其他容器编排项目里，几乎是不可能轻松支持的。

而凭借强大的API扩展机制，Custom Metrics已经成为了Kubernetes的一项标准能力。并且，Kubernetes的自动扩展器组件Horizontal Pod Autoscaler（HPA），也可以直接使用Custom Metrics来执行用户指定的扩展策略，这里的整个过程都是非常灵活和可定制的。

不难想到，Kubernets里的Custom Metrics机制，也是借助Aggregator APIServer扩展机制来实现的。这里的具体原理是，当你把Custom Metrics APIServer启动之后，Kubernetes里就会出现一个叫做custom.metrics.k8s.io的API。而当你访问这个URL时，Aggregator就会把你的请求转发给Custom Metrics APIServer。 

而Custom Metrics APIServer的实现，其实就是一个Promethus项目的Adaptor。比如，现在我们要实现一个根据指定Pod收到的HTTP请求数量来进行Auto Scaling的Custom Metrics，这个Metrics就可以通过访问如下所示的自定义监控URL获取到：

```
https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/pods/sample-metrics-app/http_requests 
```

这里的工作原理是，当你访问这个URL的时候，Custom Metrics APIServer就会去Prometheus里查询名叫sample-metrics-app 这个Pod的http_requests指标的值，然后按照固定的格式返回给访问者。

当然，http_requests指标的值，就需要由Prometheus按照我在上一篇文章中讲到的核心监控体系，从目标Pod上采集。这里具体的做法有很多种，最普遍的做法，就是让Pod里的应用本身暴露出一个 /metric API，然后在这个API里返回自己收到的HTTP的请求的数量。所以说，接下来HPA只需要定时访问前面提到的自定义监控URL，然后根据这些值计算是否要执行Scaling即可。

接下来，通过一个具体的实例，讲解一下Custom Metrics具体的使用方法。GitHub仓库地址：https://github.com/resouer/kubeadm-workshop

首先，我们当然先部署Prometheus项目。这一步，使用Prometheus Operator来完成，如下所示：

```
$ kubectl apply -f demos/monitoring/prometheus-operator.yaml
clusterrole "prometheus-operator" created
serviceaccount "prometheus-operator" created
clusterrolebinding "prometheus-operator" created
deployment "prometheus-operator" created

$ kubectl apply -f demos/monitoring/sample-prometheus-instance.yaml
clusterrole "prometheus" created
serviceaccount "prometheus" created
clusterrolebinding "prometheus" created
prometheus "sample-metrics-prom" created
service "sample-metrics-prom" created
```

第二步，我们需要把Custom Metrics APIServer部署起来，如下所示：

```
$ kubectl apply -f demos/monitoring/custom-metrics.yaml
namespace "custom-metrics" created
serviceaccount "custom-metrics-apiserver" created
clusterrolebinding "custom-metrics:system:auth-delegator" created
rolebinding "custom-metrics-auth-reader" created
clusterrole "custom-metrics-read" created
clusterrolebinding "custom-metrics-read" created
deployment "custom-metrics-apiserver" created
service "api" created
apiservice "v1beta1.custom-metrics.metrics.k8s.io" created
clusterrole "custom-metrics-server-resources" created
clusterrolebinding "hpa-controller-custom-metrics" created
```

第三步，我们需要为Custom Metrics APIServer创建对应的ClusterRoleBinding，以便使用curl来直接访问Custom Metrics的API：

```
$ kubectl create clusterrolebinding allowall-cm --clusterrole custom-metrics-server-resources --user system:anonymous
clusterrolebinding "allowall-cm" created
```

第四步，我们就可以把待监控的应用和HPA部署起来了，如下所示 

```
$ kubectl apply -f demos/monitoring/sample-metrics-app.yaml
deployment "sample-metrics-app" created
service "sample-metrics-app" created
servicemonitor "sample-metrics-app" created
horizontalpodautoscaler "sample-metrics-app-hpa" created
ingress "sample-metrics-app" created
```

这里，我们需要关注一下HPA的配置，如下所示：

```
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2beta1
metadata:
  name: sample-metrics-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sample-metrics-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Object
    object:
      target:
        kind: Service
        name: sample-metrics-app
      metricName: http_requests
      targetValue: 100
```

可以看到，HPA的配置，就是你设置Auto Scaling规则的地方。

比如，scaleTargeRef字段，就指定了被监控的对象是名叫sample-metrics-app的Deployment，也就是我们上面部署的被监控应用。并且，它最小的实例数目是2，最大是10.

在metrics字段，我们指定了这个HPA进行Scale的依据，是名叫http_requests的Metrics。而获取这个Metrics的途径，则是访问名叫sample-metrics-app的Service。

有了这些字段里的定义，HPA就可以向如下所示的URL发起请求来获取Custom Metrics的值了：

```

https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests
```

需要注意的是，上述这个URL对应的被监控对象，是我们的应用对应的Service。这跟上述已开始举例用到的Pod对应的Custom Metrics URL是不一样的。当然，对于一个多实例的应用来说，通过Service来采集Pod的Custom Metrics其实才是合理的做法。

这时候，我们可以通过一个名叫hey的测试工具来为我们的应用增加一些访问压力，具体做法如下所示：

```
$ # Install hey
$ docker run -it -v /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey

$ export APP_ENDPOINT=$(kubectl get svc sample-metrics-app -o template --template {{.spec.clusterIP}}); echo ${APP_ENDPOINT}
$ hey -n 50000 -c 1000 http://${APP_ENDPOINT}
```

与此同时，如果你去访问应用Service的Custom Meteics URL，你就会看到这个URL已经可以为你返回应用收到的HTTP请求数量了，如下所示：

```
$ curl -sSLk https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests
{
  "kind": "MetricValueList",
  "apiVersion": "custom-metrics.metrics.k8s.io/v1beta1",
  "metadata": {
    "selfLink": "/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests"
  },
  "items": [
    {
      "describedObject": {
        "kind": "Service",
        "name": "sample-metrics-app",
        "apiVersion": "/__internal"
      },
      "metricName": "http_requests",
      "timestamp": "2018-11-30T20:56:34Z",
      "value": "501484m"
    }
  ]
}
```

这里需要注意的是，Custom Metrics API诶你返回的Value的格式。

在为被监控应用编写 /metrics API的返回值时，我们其实比较容器计算的，是该Pod收到的HTTP request的总数。所以，我们这个应用的代码其实是如下所示的样子：

```
if (request.url == "/metrics") {
    response.end("# HELP http_requests_total The amount of requests served by the server in total\n# TYPE http_requests_total counter\nhttp_requests_total " + totalrequests + "\n");
    return;
  }
```

可以看到，我们的应用在 /metrics对应的HTTP response里返回的，其实是http_requests_total的值。这，也就是Prometheus收集到的值。

而Custom Metrics APIServer在收到对http_requests指标的访问请求之后，它会从Prometheus里查询http_requests_total的值，然后把它折算成一个以时间为单位的请求率，最后把这个结果作为http_requests指标对应的值返回回去。

所以说，我们在对前面的Custom Metrics URL进行访问时，会看到值是501484m，这里的格式，其实就是milli-requests，相当于是在过去两分钟内，每秒有501个请求。这样，应用的开发者就无需关心如何计算每秒的请求个数了。而这样的“请求率”格式，是可以直接被HPA拿来使用的。

这时候，如果你同时查看Pod的个数的话，就会看到HPA开始增加Pod的数目了。

不过，在这里你看你会有一个疑问，Prometheus项目，又是如何知道采集哪些Pod的 /metrics API作为监控指标的来源呢？ 实际上，如果仔细观察一下我们前面创建应用的输出，你就会看到有一个类型是ServiceMonitor的对象也被创建了出来。它的YAML文件如下所示：

```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: sample-metrics-app
  labels:
    service-monitor: sample-metrics-app
spec:
  selector:
    matchLabels:
      app: sample-metrics-app
  endpoints:
  - port: web
```

这个ServiceMonitor对象，正是Prometheus Operator项目用来指定被监控Pod的一个配置文件。可以看到，我其实是通过Label Selector为Prometheus来指定被监控应用的。

## 50 让日志无处可逃：容器日志收集与管理

Kubernetes里面对容器日志的处理方式，都叫做cluster-level-logging，即：这个日志处理系统，与容器、Pod以及Node的生命周期都是完全无关的。这种设计当然是为了保证，无论是容器挂了、Pod被删除，甚至节点宕机的时候，应用的日志依然可以被正常获取到。

而对于一个容器来说，当应用把日志输出到stdout和stderr之后，容器项目在默认情况下就会把这些日志输出到宿主机上的一个JSON文件里。这样，你通过kubectl logs命令就可以看到这些容器的日志了。上述机制，就是我们今天要讲解的容器日志收集的基础假设。而如果你的应用是把文件输出到其他地方，比如直接输出到了容器里的某个文件里，或者输出到了远程存储里，那就属于特殊情况了。

Kuberentes项目本身，主要为你推荐了三种日志方案。

### 50.01 第一种收集方案

第一种，在Node上部署logging agent ，将日志文件转发到后端存储里保存起来。这个方案的架构图如下所示：

![img](https://static001.geekbang.org/resource/image/b5/43/b5515aed076aa6af63ace55b62d36243.jpg)

不难看到，这里的核心就在于logging agent，它会以DaemonSet的方式运行在节点上，然后将宿主机上的容器日志目录挂在进去，最后由logging-agent把日志转发出去。

举个例子，我们可以通过Fluentd项目作为宿主机上的logging-agent，然后把日志转发到远端的ElasticSearch里保存起来供将来进行检索。可以看到，在Node上部署logging agent最大的优点，在于一个节点值需要部署一个agent，并且不会对应用和Pod有任何侵入性。所以，这个方案，在社区里是最常用的一种。

但是也不难看到，这种方案不足之处就在于，它要求应用输出的日志，都必须是直接输出到容器的stdout和stderr里。

### 50.02 第二种方案

Kubernetes容器日志方案的第二种，就是对这种特殊情况的一个处理，即：当容器的日志只能输出到某些文件里的时候，我们就可以通过一个sidecar容器吧这些日志文件重新输出到sidecar的stdout和stderr上，这样就能够继续使用第一种方案了。如下图所示：

![img](https://static001.geekbang.org/resource/image/48/20/4863e3d7d1ef02a5a44e431369ac4120.jpg)



比如，现在我的应用Pod里只有一个容器，它会把日志输出到容器里的/var/log/1.log和 2.log这两个文件里。这个Pod的YAML文件如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
```

在这种情况下，你用kubectl logs命令是看不到应用的任何日志的。那么这个时候，我们就可以为这个Pod添加两个sidecar容器，分别将上述两个日志文件里的内容重新以stdout和stderr的方式输出出来，这个YAML文件的写法如下所示：

```

apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-1
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-log-2
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
```

这时候，你就可以通过kubectl logs命令查看这两个sidecar容器的日志，间接看到应用的日志内容了，如下所示：

```

$ kubectl logs counter count-log-1
0: Mon Jan 1 00:00:00 UTC 2001
1: Mon Jan 1 00:00:01 UTC 2001
2: Mon Jan 1 00:00:02 UTC 2001
...
$ kubectl logs counter count-log-2
Mon Jan 1 00:00:00 UTC 2001 INFO 0
Mon Jan 1 00:00:01 UTC 2001 INFO 1
Mon Jan 1 00:00:02 UTC 2001 INFO 2
...
```

由于sidecar跟主容器之间是共享Volume的，所以这里的sidecar方案的额外性能损耗并不高，也就是多占用一点CPU和内存罢了。但需要注意的是，这时候，宿主机上实际上会存在两份相同的日志文件：一份是应用自己写入的；另一份则是sidecar的stdout和stderr对应的JSON文件。这对磁盘是很大的浪费。所以说，不到万不得已或者应用容器完全不可能被修改，否则我还是建议你直接使用方案一，或者使用后面的方案三

### 50.03 第三种方案

第三种方案，就是通过一个sidecar容器，直接把应用的日志文件发送到远程存储里去。也就是相当于把方案一里的logging agent，放在了应用Pod里。这种方案的架构如下所示：

![img](https://static001.geekbang.org/resource/image/d4/c7/d464401baec60c11f96dfeea3ae3a9c7.jpg)

在这种方案里，你的应用还可以直接把日志输出到固定的文件里而不是stdout，你的logging-agent还可以使用fluentd，后端存储还可以是ElasticSearch。只不过，fluentd的输入源，变成了应用的日志文件。一般来说，我们会把fluentd的输入源配置保存在一个ConfigMap里，如下所示：

```

apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluentd.conf: |
    <source>
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag count.format1
    </source>
    
    <source>
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag count.format2
    </source>
    
    <match **>
      type google_cloud
    </match>
```

然后，我们在应用Pod的定义里，就可以声明一个Fluentd容器作为sidecar，专门负责将应用生成的1.log和 2.log转发到ElasticSearch当中。这个配置，如下所示：

```
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-agent
    image: k8s.gcr.io/fluentd-gcp:1.30
    env:
    - name: FLUENTD_ARGS
      value: -c /etc/fluentd-config/fluentd.conf
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: config-volume
      mountPath: /etc/fluentd-config
  volumes:
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config
```

可以看到，这个Fluentd容器使用的输入源，就是通过引用我们前面编写的ConfigMap来指定的。这里我用到了Projected Volume来把ConfigMap挂载到Pod里。

需要注意的是，这种方案虽然部署简单，并且对宿主机友好，但是这个sidecar容器很可能会消耗比较多的资源，甚至拖垮应用容器。并且，由于日志还是没有输出到stdout上，所以你通过kubectl logs是看不到任何日志输出的。

### 50.04 小结

综合对比上述三种方案，我比较建议你将应用日志输出到stdout和 stderr，然后通过在宿主机上部署logging-agent的方式来集中处理日志。

这种方案不仅管理简单，kubectl logs也可以用，而且可靠性高，并且宿主机本身，很可能就自带了rsyslogd等非常成熟的日志收集组件来供你使用。

除此之外，是有一种方式就是在编写应用的时候，就直接指定好日志的存储后端，如下所示:

![img](https://static001.geekbang.org/resource/image/13/99/13e8439d9945fea58c9672fc4ca30799.jpg)

在这种方案下，Kubernetes就完全不必操心容器日志收集了，这对于本身已经有非常完善的日志处理系统的公司来说，是一个非常好的选择。

最后需要指出的是，无论是哪种方案，你都必须要及时将这些日志文件从宿主机上清理掉，或者给日志目录专门挂载一些容量巨大的远程盘。否则，一旦宿主机磁盘分区被打满，整个系统就可能会陷入崩溃状态，这是非常麻烦的。

























 






































































# Kubernetes容器监控与日志

## 48  Prometheus、Metrics Server与Kubernetes监控体系

Kubernetes项目的监控体系曾经非常繁杂，在社区中也有很多方案。但这套体系发展到今天，已经完全演变成了以Prometheus项目为核心的一套统一的方案。实际上，Prometheus项目是当年CNCF基金会起家时的“第二把交椅”。而这个项目发展到今天，已经全面接管了Kubernetes项目的整套监控体系。

比较有意思的是，Prometheus项目与Kubernetes项目一样，也是来自Google的Borg体系，它的原型系统，叫做BorgMon，是一个几乎与Borg同时诞生的内部监控系统。而Prometheus项目的发起原因也跟Kubernetes很类似，都是希望通过对用户更友好的方式，将Google内部系统的设计理念，传递给用户和开发者。

作为一个监控系统，Prometheus项目的作用和工作方式，其实可以用如下所示的一张官方示意图来解释

![img](https://static001.geekbang.org/resource/image/2a/d3/2ada1ece66fcc81d704c2ba46f9dd7d3.png)

可以看到，Prometheus项目工作的核心，是使用Pull（抓取）的方式去搜集被监控对象的Metrics数据（监控指标数据），然后，再把这些数据保存在一个TSDB（时间序列数据库，比如OpenTSDB、InfluxDB等）当中，以便后续可以按照时间进行检索。

有了这套核心监控机制，Prometheus剩下的组件就是用来配合这套机制运行。比如 Pushgateway，可以允许被监控对象以Push的方式向Prometheus推送Metric数据。而Alertmanager，则可以根据Metrics信息灵活地设置报警。当然，Prometheus最受欢迎的功能，还是通过Grafana对外暴露的、可以灵活配置的监控数据可视化界面。

有了Prometheus之后，我们就可以按照Metrics数据的来源，来对Kubernetes的监控体系做一个汇总了。

第一种Metrics，是宿主机的监控数据。这部分数据的提供，需要借助一个由Prometheus维护的Node Exporter工具。一般来说，Node Exporter会以DaemonSet的方式运行在宿主机上。其实，所谓的Exporter，就是代替被监控对象来对Prometheus暴露出可以被“抓取”的Metrics信息的一个辅助进程。

第二种Metrics，是来自于Kubernetes的API Server、kubelet等组件的 /metrics API。除了常规的CPU、内存的信息外，这部分信息还主要包括了各个组件的核心监控指标。比如，对于API Server来说，它就会在 /metrics API里，暴露出各个Controller的工作队列（Work Queue）的长度，请求的QPS和延迟数据等等。这些信息，是检查Kubernetes本身工作情况的主要依据。

第三种Metrics，是Kubernetes相关的监控数据。这部分数据，一般叫做Kubernetes核心监控数据（core metrics）。这其中包括了Pod、Node、容器、Service等主要Kubernetes核心概念的Metrics。其中，容器相关的Metrics主要来自于kubelet内置的cAdvisor服务。在kubelet启动后，cAdvisor服务也随之启动，而它能提供的信息，可以细化到每一个容器的CPU、文件系统、内存、网络等资源的使用情况。
需要注意的是，这里提到的Kubernetes核心监控数据，其实使用的是Kubernetes的一个非常重要的扩展能力，叫做Metrics Server。Metrics Server在Kubernetes社区的定位，其实是用来取代Heapster这个项目的。在Kubernetes项目发展的初期，Heapster是用户获取Kubernetes监控数据（比如Pod和Node的资源使用情况）的主要渠道。而后面提出来的Metrics Server，则把这些信息，通过标准的Kubernetes API暴露了出来。这样，Metrics信息就跟Heapster完成了解藕，允许Heapster项目慢慢退出舞台。

而有了Metrics Server之后，用户就可以通过标准的Kubernetes API来访问到这些监控数据了。比如，下面这个URL：

```
http://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespaces/<namespace-name>/pods/<pod-name>
```

当你访问这个Metrics API时，它就会为你返回一个Pod的监控数据，而这些数据，其实是从kubelet的Summary API（即<kubelet_ip>:<kubelet_port>/stats/summary）采集而来的。Summary API返回的信息，既包括了cAdvisor的监控数据，也包括了kubelet本身汇总的信息。

需要指出的是，Metric Server并不是kube-apiserver的一部分，而是通过Aggregator这种插件机制，在独立部署的情况下同kube-apiserver一起统一对外服务的。

这里，Aggregator APIServer的工作原理，可以用如下所示的一副示意图来表示清楚：

![img](https://static001.geekbang.org/resource/image/0b/09/0b767b5224ad1906ddc4cce075618809.png)

可以看到，当Kubernetes的API Server开启了Aggregator模式之后，你再访问aips/metrics.k8s.io/v1beta1的时候，实际上访问到的是一个叫做kube-aggregator的代理。而kube-apiserver，正是这个代理的一个后端；而Metrics Server，正是另一个后端。

而且，在这个机制上，你还可以添加更多的后端给这个kube-aggregator。所以，kube-aggregator其实就是一个根据URL选择具体的API后端的代理服务器。通过这种方式，我们就可以很方便地扩展Kubernetes的API了。

在理解了Prometheus关心的三种监控数据源，以及Kubernetes的核心Metrics之后，作为用户，你其实要做的就是将Prometheus Operator在Kubernetes集群里部署起来。然后，按照本篇文章已开始介绍的架构，把上述Metrics源配置起来，让Prometheus自己去进行采集即可。

在具体的监控指标规划上，我建议遵循业界通用的USE原则和RED原则。
USE原则（主要用于资源监控指标）：
1、利用率（Utilization），资源被有效利用起来提供服务的平均时间占比
2、饱和度（Saturation），资源拥挤的程度，比如工作队列的长度
3、错误率（Error），错误的数量
RED原则（主要用于服务监控指标）：
1、每秒请求数量（Rate）;
2、每秒错误数量（Errors）;
3、服务响应时间（Duration）；



## 49 Custom metrics : 让Auto Scaling 不再“食之无味”

实际上，结束Promethus监控体系，Kubernetes就可以提供一种非常有用的能力，那就是Custom Metrics，自定义监控指标。

在过去的很多Paas项目中，其实有一种叫做Auto Scaling，即自动水平扩展的功能。只不过，这个功能往往只能依据某种制定的资源类型执行水平扩展，比如CPU或者Memory的使用值。

而在真实的场景中，用户需要进行Auto Scaling的依据往往是自定义的监控指标。比如，某个应用的等待队列长度，或者某种应用相关资源的使用情况。这些复杂多变的需求，在传统Paas项目和其他容器编排项目里，几乎是不可能轻松支持的。

而凭借强大的API扩展机制，Custom Metrics已经成为了Kubernetes的一项标准能力。并且，Kubernetes的自动扩展器组件Horizontal Pod Autoscaler（HPA），也可以直接使用Custom Metrics来执行用户指定的扩展策略，这里的整个过程都是非常灵活和可定制的。

不难想到，Kubernets里的Custom Metrics机制，也是借助Aggregator APIServer扩展机制来实现的。这里的具体原理是，当你把Custom Metrics APIServer启动之后，Kubernetes里就会出现一个叫做custom.metrics.k8s.io的API。而当你访问这个URL时，Aggregator就会把你的请求转发给Custom Metrics APIServer。 

而Custom Metrics APIServer的实现，其实就是一个Promethus项目的Adaptor。比如，现在我们要实现一个根据指定Pod收到的HTTP请求数量来进行Auto Scaling的Custom Metrics，这个Metrics就可以通过访问如下所示的自定义监控URL获取到：

```
https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/pods/sample-metrics-app/http_requests 
```

这里的工作原理是，当你访问这个URL的时候，Custom Metrics APIServer就会去Prometheus里查询名叫sample-metrics-app 这个Pod的http_requests指标的值，然后按照固定的格式返回给访问者。

当然，http_requests指标的值，就需要由Prometheus按照我在上一篇文章中讲到的核心监控体系，从目标Pod上采集。这里具体的做法有很多种，最普遍的做法，就是让Pod里的应用本身暴露出一个 /metric API，然后在这个API里返回自己收到的HTTP的请求的数量。所以说，接下来HPA只需要定时访问前面提到的自定义监控URL，然后根据这些值计算是否要执行Scaling即可。

接下来，通过一个具体的实例，讲解一下Custom Metrics具体的使用方法。GitHub仓库地址：https://github.com/resouer/kubeadm-workshop

首先，我们当然先部署Prometheus项目。这一步，使用Prometheus Operator来完成，如下所示：

```
$ kubectl apply -f demos/monitoring/prometheus-operator.yaml
clusterrole "prometheus-operator" created
serviceaccount "prometheus-operator" created
clusterrolebinding "prometheus-operator" created
deployment "prometheus-operator" created

$ kubectl apply -f demos/monitoring/sample-prometheus-instance.yaml
clusterrole "prometheus" created
serviceaccount "prometheus" created
clusterrolebinding "prometheus" created
prometheus "sample-metrics-prom" created
service "sample-metrics-prom" created
```

第二步，我们需要把Custom Metrics APIServer部署起来，如下所示：

```
$ kubectl apply -f demos/monitoring/custom-metrics.yaml
namespace "custom-metrics" created
serviceaccount "custom-metrics-apiserver" created
clusterrolebinding "custom-metrics:system:auth-delegator" created
rolebinding "custom-metrics-auth-reader" created
clusterrole "custom-metrics-read" created
clusterrolebinding "custom-metrics-read" created
deployment "custom-metrics-apiserver" created
service "api" created
apiservice "v1beta1.custom-metrics.metrics.k8s.io" created
clusterrole "custom-metrics-server-resources" created
clusterrolebinding "hpa-controller-custom-metrics" created
```

第三步，我们需要为Custom Metrics APIServer创建对应的ClusterRoleBinding，以便使用curl来直接访问Custom Metrics的API：

```
$ kubectl create clusterrolebinding allowall-cm --clusterrole custom-metrics-server-resources --user system:anonymous
clusterrolebinding "allowall-cm" created
```

第四步，我们就可以把待监控的应用和HPA部署起来了，如下所示 

```
$ kubectl apply -f demos/monitoring/sample-metrics-app.yaml
deployment "sample-metrics-app" created
service "sample-metrics-app" created
servicemonitor "sample-metrics-app" created
horizontalpodautoscaler "sample-metrics-app-hpa" created
ingress "sample-metrics-app" created
```

这里，我们需要关注一下HPA的配置，如下所示：

```
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2beta1
metadata:
  name: sample-metrics-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: sample-metrics-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Object
    object:
      target:
        kind: Service
        name: sample-metrics-app
      metricName: http_requests
      targetValue: 100
```

可以看到，HPA的配置，就是你设置Auto Scaling规则的地方。

比如，scaleTargeRef字段，就指定了被监控的对象是名叫sample-metrics-app的Deployment，也就是我们上面部署的被监控应用。并且，它最小的实例数目是2，最大是10.

在metrics字段，我们指定了这个HPA进行Scale的依据，是名叫http_requests的Metrics。而获取这个Metrics的途径，则是访问名叫sample-metrics-app的Service。

有了这些字段里的定义，HPA就可以向如下所示的URL发起请求来获取Custom Metrics的值了：

```

https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests
```

需要注意的是，上述这个URL对应的被监控对象，是我们的应用对应的Service。这跟上述已开始举例用到的Pod对应的Custom Metrics URL是不一样的。当然，对于一个多实例的应用来说，通过Service来采集Pod的Custom Metrics其实才是合理的做法。

这时候，我们可以通过一个名叫hey的测试工具来为我们的应用增加一些访问压力，具体做法如下所示：

```
$ # Install hey
$ docker run -it -v /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey

$ export APP_ENDPOINT=$(kubectl get svc sample-metrics-app -o template --template {{.spec.clusterIP}}); echo ${APP_ENDPOINT}
$ hey -n 50000 -c 1000 http://${APP_ENDPOINT}
```

与此同时，如果你去访问应用Service的Custom Meteics URL，你就会看到这个URL已经可以为你返回应用收到的HTTP请求数量了，如下所示：

```
$ curl -sSLk https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests
{
  "kind": "MetricValueList",
  "apiVersion": "custom-metrics.metrics.k8s.io/v1beta1",
  "metadata": {
    "selfLink": "/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests"
  },
  "items": [
    {
      "describedObject": {
        "kind": "Service",
        "name": "sample-metrics-app",
        "apiVersion": "/__internal"
      },
      "metricName": "http_requests",
      "timestamp": "2018-11-30T20:56:34Z",
      "value": "501484m"
    }
  ]
}
```

这里需要注意的是，Custom Metrics API诶你返回的Value的格式。

在为被监控应用编写 /metrics API的返回值时，我们其实比较容器计算的，是该Pod收到的HTTP request的总数。所以，我们这个应用的代码其实是如下所示的样子：

```
if (request.url == "/metrics") {
    response.end("# HELP http_requests_total The amount of requests served by the server in total\n# TYPE http_requests_total counter\nhttp_requests_total " + totalrequests + "\n");
    return;
  }
```

可以看到，我们的应用在 /metrics对应的HTTP response里返回的，其实是http_requests_total的值。这，也就是Prometheus收集到的值。

而Custom Metrics APIServer在收到对http_requests指标的访问请求之后，它会从Prometheus里查询http_requests_total的值，然后把它折算成一个以时间为单位的请求率，最后把这个结果作为http_requests指标对应的值返回回去。

所以说，我们在对前面的Custom Metrics URL进行访问时，会看到值是501484m，这里的格式，其实就是milli-requests，相当于是在过去两分钟内，每秒有501个请求。这样，应用的开发者就无需关心如何计算每秒的请求个数了。而这样的“请求率”格式，是可以直接被HPA拿来使用的。

这时候，如果你同时查看Pod的个数的话，就会看到HPA开始增加Pod的树木了。

不过，在这里你看你会有一个疑问，Prometheus项目，又是如何知道采集哪些Pod的 /metrics API作为监控指标的来源呢？ 实际上，如果仔细观察一下我们前面创建应用的输出，你就会看到有一个类型是ServiceMonitor的对象也被创建了出来。它的YAML文件如下所示：

```
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: sample-metrics-app
  labels:
    service-monitor: sample-metrics-app
spec:
  selector:
    matchLabels:
      app: sample-metrics-app
  endpoints:
  - port: web
```

这个ServiceMonitor对象，正是Prometheus Operator项目用来指定被监控Pod的一个配置文件。可以看到，我其实是通过Label Selector为Prometheus来指定被监控应用的。




































































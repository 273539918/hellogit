# The Google File System





## 0、概要

### 总结

google设计并实现的GFS分布式文件系统，能够运行在大量廉价硬件上，提供容错能力并具有较好的性能，在google已经大范围使用。这篇文章会对GFS进行介绍

### 内容

我们已经设计并实现了Google File System，这是一个可扩展的分布式文件系统，用于大型分布式数据密集型应用程序。它可以在廉价的普通硬件上运行并提供容错能力，并且可以为大量客户端提供较高的聚合性能。

在实现与以前的分布式文件系统相同的许多目标的同时，我们的设计是由对当前和预期的应用程序工作负载和技术环境的观察推动的，这反映出与某些早期文件系统假设的明显偏离。这导致我们重新审视传统选择，并探索根本不同的设计要点。

这个文件系统已成功满足我们的存储需求。 它广泛地在Google内部部署，作为存储平台，用于生成和处理我们的服务使用的数据以及需要大量数据集的研发工作。迄今为止，最大的群集在一千多台计算机上的数千个磁盘上提供了数百TB的存储，数百个客户端同时访问该群集。

在本文中，我们介绍了旨在支持分布式应用程序的文件系统接口扩展，讨论了我们设计的许多方面，并报告了来自微基准测试和实际使用情况的测量结果。



## 1. 介绍



### 总结

Google文件系统（GFS）的设计需要考虑：

 1）组件容错和自动恢复  
 2）文件庞大，需要重新考虑I/O操作和块大小 
 3）访问模式相对固定，"append"是性能优化和原子性保证的重点 

### 内容

我们已经设计并实现了Google文件系统（GFS），以满足Google数据处理快速增长的需求。 GFS与以前的分布式文件系统具有许多相同的目标，例如性能，可伸缩性，可靠性和可用性。 但是，其设计是由对我们当前和预期的应用程序工作负载和技术环境的主要观察驱动的，这反映出与某些早期文件系统设计假设存在明显差异。 我们重新审视了传统选择，并探索了设计中的根本不同点。

首先，组件故障是正常现象，而不是异常情况。 该文件系统由数百个甚至数千个由廉价PC存储机器组成，并且可被相当数量的客户端访问。组件的数量和质量实际上保证了某些组件在任意时刻的不可用以及某些组件无法从当前的故障中恢复，我们已经看到了由应用程序错误，操作系统错误，人为错误以及磁盘，内存，连接器，网络和电源故障引起的问题。 因此，持续监视，错误检测，容错和自动恢复必须是系统不可或缺的。

其次，按照传统标准，文件非常庞大。 GB大小文件是常见的。 每个文件通常包含许多应用程序对象，例如Web文档。我们无法定期处理TB级增长的数十亿个对象，即使文件系统可以支持它，也无法管理数十亿个KB大小的文件。因此，必须重新考虑设计参数，例如I / O操作和块大小。

第三，大多数文件是通过“附加”新数据而不是覆盖现有数据来进行改变的。文件内的随机写入实际上是不存在的。 写入后，仅读取文件，并且通常只能顺序读取。各种数据共享这些特征。 有些可能会构成大型存储库，数据分析程序会扫描这些存储库。 有些可能是通过运行应用程序连续生成的数据流。 有些可能是档案数据。 某些结果可能在一台机器上产生中间结果，而在另一台机器上同时产生或稍后产生。鉴于对大型文件的这种访问模式，“附加”成为性能优化和原子性保证的重点，而在客户端中缓存数据块却失去了吸引力。

第四，共同设计应用程序和文件系统API，提高灵活性，可以使整个系统受益。例如，我们放松了GFS的一致性模型，以极大地简化文件系统，而不会给应用程序带来繁重的负担。我们还引入了原子附加操作，以便多个客户端可以并发附加到文件，而无需在它们之间进行额外的同步。这些将在本文后面详细讨论。

当前部署了多个GFS群集以用于不同的目的。 最大的服务器拥有1000多个存储节点，超过300 TB的磁盘存储，并且连续不断地被不同计算机上的数百个客户端大量访问。



## 2、 设计概述

### 总结

 设计的系统有如下假设：
（1）系统由大量廉价PC组成，软件侧实现容错和故障恢复
（2）小文件为主（100MB左右），大文件较少
（3）主要是顺序读，少量随机读
（4）主要是append写，少量修改
（5）支持高并发的写入语义
（6）设计上高吞吐比低延迟更重要

GFS集群有Master、Checkserver、Client组成，均可运行在廉价PC上。文件会被分成固定大小的块(check)。 每个块创建时，master都会分配的不可变且全局唯一的64位块句柄来标识。checkerver将块作为Linux文件存储在本地磁盘上，并读取或写入块数据。为了可靠性，每个块都有多副本。默认情况下，存储三个副本，用户可以为文件名称空间的不同区域指定不同的复制级别。Master维护所有文件系统元数据。 这包括名称空间，访问控制信息，从文件到块的映射以及块的当前位置。 它还控制整个系统的活动，例如块管理，垃圾收集以及checkserver之间的块迁移。 Master定期通过HeartBeat与每个checkserver通信，以向其发出指令并收集其状态。

单Master极大地简化了设计，并且可以使用全局性的信息来做出块的放置和复制的决策。为了避免Master成为瓶颈，Client永远不会通过Master读取和写入文件数据。以读取为例：
（1）Client将应用程序想要读取的文件名和偏移量翻译成文件内的块索引
（2）Client向Master询问对应文件的块索引位置，Master将文件句柄和副本位置告知Client
（3）Client缓存Master返回的信息，并向副本位置发起读取数据的请求。缓存过期或者Client需要重新打开文件之前，Client都不需要再与Master交互

GFS默认块大小为64MB，设置较大的块大小有如下优势：
1）【master交互】减少client与master的交互，同一块上的信息只需要获取一次，因此块越大，client需要与master交互的次数就越少；
2）【网络开销】块越大，client在同一个块上执行的操作可能就越多，通过client与check server维持一个长TCP链接可以减少网络开销
3）【meta存储】master的meta信息更少，性能好

GFS设置较小的块有如下劣势：
1）块过大，增加了client访问该块的概率，可能使得某一个check server成为热点
2） 空间浪费，小文件存储也会占用一个块大小，导致浪费

Master存储三种主要类型的元数据：文件和块名称空间，从文件到块的映射以及每个块副本的位置。这三种元数据都会存储在master的内存中。此外文件和块名称空间，从文件到块的映射，还会被持久化到磁盘并同步到远程计算机；而每个块副本的位置不会被持久化存储，Master每次启动时通过主动询问CheckServer来获取这些信息。



### 内容

#### 2.1 假设条件

在设计满足我们需求的文件系统时，我们以既带来挑战又带来机遇的假设为指导。 我们之前提到了一些关键的观察，现在更详细地阐述了我们的假设。
（1）该系统由许多经常发生故障的廉价PC组成。 它必须不断地自我监控，并定期检测，容忍和及时从组件故障中恢复。
（2）系统存储少量的大文件。 我们期望有几百万个文件，每个文件的大小通常为100 MB或更大。 多GB文件是常见的情况，应进行有效管理。 必须支持小文件，但我们不需要对其进行优化。
（3）工作负载主要包括两种读取：大型流读取和小型随机读取。 在大型流读取中，单个操作通常读取数百KB，更常见的是1 MB或更多。 来自同一客户端的连续操作通常会读取文件的连续区域。 少量随机读取通常会以任意偏移量读取几个KB。 注重性能的应用程序经常对小读取进行批处理和排序，以稳定地通过文件而不是来回移动。
（4）工作负载还具有许多大的顺序写入，这些写入将数据追加到文件中。 典型的操作大小类似于读取的大小。 一旦写入，很少再修改文件。 支持对文件中任意位置的小写操作，但不一定要有效。
（5）系统必须为同时附加到同一文件的多个客户端有效地实现定义良好的语义。 我们的文件通常被用作生产者－消费者队列或用于多种合并。 每台计算机上运行一个的数百个生产者将同时添加到文件中。 具有最小同步开销的原子性是必不可少的。 该文件可能会在以后读取，或者消费者可能正在同时读取文件。
（6）高持续带宽比低延迟更重要。 我们的大多数目标应用程序都非常重视以高速率处理大量数据，而很少有对单个读取或写入有严格响应时间要求的应用程序。



#### 2.2 接口

GFS提供了一个熟悉的文件系统接口，尽管它没有实现诸如POSIX之类的标准API。 文件在目录中按层次结构组织，并由路径名标识。 我们支持创建，删除，打开，关闭，读取和写入文件的常规操作。

此外，GFS具有快照和append操作。 快照可以低成本创建文件或目录树的副本。 append允许多个客户端同时将数据附加到同一文件，同时保证每个客户端append的原子性。 这对于实现多路合并结果和生产者-消费者队列非常有用，许多客户端可以同时append这些合并结果和生产者-消费者队列而无需额外锁定。 我们发现这些类型的文件在构建大型分布式应用程序中具有不可估量的价值。 快照和append将分别在3.4和3.3节中进一步讨论。



#### 2.3 架构

一个GFS集群由一个主服务器和多个块服务器组成，并且可以由多个客户端访问，如图1所示。它们中的每一个通常都是运行用户级服务器进程的商用Linux计算机。 只要在机器资源允许的情况下，就可以在同一台机器上同时运行chunkserver和客户端，并且可以接受由于运行不稳定的应用程序代码而导致的较低可靠性。

文件分为固定大小的check。 每个块创建时，master都会分配的不可变且全局唯一的64位块句柄来标识。checkerver将check作为Linux文件存储在本地磁盘上，并读取或写入块数据。为了可靠性，每个块都复制多副本到多个块服务器上。默认情况下，存储三个副本，用户可以为文件名称空间的不同区域指定不同的复制级别。

Master维护所有文件系统元数据。 这包括名称空间，访问控制信息，从文件到块的映射以及块的当前位置。 它还控制整个系统的活动，例如块管理，垃圾收集以及checkserver之间的块迁移。 Master定期通过HeartBeat与每个checkserver通信，以向其发出指令并收集其状态。

链接到每个应用程序的GFS Client代码实现文件系统API，并与master和checkserver通信以代表该应用程序读取或写入数据。

client和checkserver均不缓存文件数据。 客户端和块服务器均不缓存文件数据。 客户端缓存几乎没有什么好处，因为大多数应用程序流经大文件或工作集太大而无法缓存。 没有它们，就消除了缓存一致性问题，从而简化了客户端和整个系统。 （但是，客户端确实缓存元数据。）块服务器不需要缓存文件数据，因为大块存储为本地文件，因此Linux的缓冲区缓存已经将经常访问的数据保存在内存中。



#### 2.4 单Master

单Master极大地简化了设计，并且可以使用全局性的信息来做出块的放置和复制的决策。但是，我们必须最小化它在读写中的参与，以免它成为瓶颈。Client永远不会通过Master读取和写入文件数据。 取而代之的是，Client询问Master应与哪些CheckServer联系。 它在有限的时间内缓存此信息，并直接与CheckServer交互以进行许多后续操作。

以图1为例：

首先，使用固定的块大小，客户端将应用程序指定的文件名和字节偏移量转换为文件内的块索引。

然后，它向主服务器发送一个包含文件名和块索引的请求。主服务器答复相应的块句柄和副本的位置。客户端使用文件名和块索引作为关键字来缓存此信息。

然后，客户端将请求发送到其中一个副本，最有可能是最接近的一个。该请求指定了块句柄和该块内的字节范围。在缓存信息过期或重新打开文件之前，对同一块的进一步读取不再需要客户端与主机之间的任何交互。实际上，客户端通常会在同一请求中请求多个块，而主服务器也可以在请求的块之后立即包含块的信息。

#### 2.5 块大小

块大小是关键设计参数之一。 我们选择了64 MB，它比典型的文件系统块大得多。 每个块副本都作为纯Linux文件存储在块服务器上，并且仅在需要时进行扩展。

较大的check size有几个重要的优点：1)它减少了client与master交互的需求，因为在同一块上进行读写只需要向主机发出一个初始请求即可获得块位置信息。 这部分减少是非常有意义的，因为应用程序通常按顺序读取和写入大文件。即使对于小的随机读取，客户端也可以轻松地缓存多TB工作集的所有块位置信息。2）其次，由于在较大的块上，客户端更有可能在给定的块上执行许多操作，因此它可以通过在延长的时间段内保持与块服务器的持久TCP连接来减少网络开销。 3）减少了master上meta信息的存储量

较大的check size有几个重要的缺点：1）块文件过大，增加了多个client访问该块的概率，可能会导致某一个check server成为热点  
2）存储空间的浪费，可能只要存储几kb，但是一次最小写一个块，就写成了64MB 

#### 2.6 **Metadata**

主机存储三种主要类型的元数据：文件和块名称空间，从文件到块的映射以及每个块副本的位置。所有元数据都保存在主机的内存中。 前两种类型（命名空间和文件到块的映射）还会将操作日志存储到master的本地磁盘上并同步到远程计算机。有了日志我们就能在Master异常时，快速恢复。Master不会持久化存储块副本的位置，它会在每一次启动时候去询问checkserver块的地址。



































